var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(n){const s=suggestions.classList.contains("d-none");if(s)return;const e=[...suggestions.querySelectorAll("a")];if(e.length===0)return;const t=e.indexOf(document.activeElement);if(n.key==="ArrowUp"){n.preventDefault();const s=t>0?t-1:0;e[s].focus()}else if(n.key==="ArrowDown"){n.preventDefault();const s=t+1<e.length?t+1:t;e[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/docs/welcome/",title:"Welcome to MLOps for ALL!",description:"Introduction to MLOps",content:"모두의 MLOps # 최근 MLOps와 관련된 주제의 세미나와 글들이 많아지고 모두 MLOps의 필요성에 대해 말하고 있습니다.\n그런데 MLOps란 무엇이며 이를 위해서 우리는 무엇을 공부해야 할까요?\n저희 모두의 MLOps는 MLOps를 공부하려고 하지만 어떻게 시작해야 하는지 모르는 분들을 위한 지침서를 작성하고자 이 프로젝트를 시작하였습니다.\nMLOps는 Machine Learning Operations의 약어입니다. Operations는 도메인에 따라서 또는 상황에 따라서 필요로 하는 것이 달라진다는 뜻을 내포하고 있습니다.\n특히 집필하는 2022년 기준으로 아직 표준이라고 불릴 수 있는 MLOps 도구가 존재하지 않습니다. 그래서 저희가 제시하는 방법을 실제 업무에 바로 적용하기에는 힘들 수도 있습니다.\n그래도 이 글을 통해서 많은 분이 MLOps란 무엇이며 각자의 환경에 맞게 어떤 것이 필요한지를 알 수 있는 첫 디딤돌이 되었으면 합니다.\n"}),e.add({id:1,href:"/docs/introduction/",title:"Introduction",description:"Introduction to MLOps",content:""}),e.add({id:2,href:"/docs/introduction/intro/",title:"1. What is MLOps?",description:"Introduction to MLOps",content:"Machine Learning Project # 2012년 Alexnet 이후 CV, NLP를 비롯하여 데이터가 존재하는 도메인이라면 어디서든 머신러닝과 딥러닝을 도입하고자 하였습니다.\n딥러닝과 머신러닝은 AI라는 단어로 묶이며 불렸고 많은 매체에서 AI의 필요성을 외쳤습니다. 그리고 무수히 많은 기업에서 머신러닝과 딥러닝을 이용한 수많은 프로젝트를 진행하였습니다. 하지만 그 결과는 어떻게 되었을까요?\n엘리먼트 AI의 음병찬 동북아 지역 총괄책임자는 \u0026ldquo;10개 기업에 AI 프로젝트를 시작한다면 그중 9개는 컨셉검증(POC)만 하다 끝난다\u0026rdquo;고 말했습니다.\n이처럼 많은 프로젝트에서 머신러닝과 딥러닝은 이 문제를 풀 수 있을 것 같다는 가능성만을 보여주고 사라졌습니다. 그리고 이 시기쯤에 AI에 다시 겨울이 다가오고 있다는 전망도 나오기 시작했습니다.\n왜 프로젝트 대부분이 컨셉검증(POC) 단계에서 끝났을까요?\n머신러닝과 딥러닝 코드만으로는 실제 서비스를 운영할 수 없기 때문입니다.\n실제 서비스 단계에서 머신러닝과 딥러닝의 코드가 차지하는 부분은 생각보다 크지 않기 때문에, 단순히 모델의 성능만이 아닌 다른 많은 부분을 고려해야 합니다.\n구글은 이런 문제를 2015년 Hidden Technical Debt in Machine Learning Systems에서 지적한 바 있습니다.\n하지만 이 논문이 나올 당시에는 아직 많은 머신러닝 엔지니어들이 딥러닝과 머신러닝의 가능성을 입증하기 바쁜 시기였기 때문에, 논문이 지적하는 바에 많은 주의를 기울이지는 않았습니다.\n그리고 몇 년이 지난 후 머신러닝과 딥러닝은 가능성을 입증해내어, 이제 사람들은 실제 서비스에 적용하고자 했습니다.\n하지만 곧 많은 사람이 실제 서비스는 쉽지 않다는 것을 깨달았습니다.\nDevops # MLOps는 이전에 없던 새로운 개념이 아니라 DevOps라고 불리는 개발 방법론에서 파생된 단어입니다. 그렇기에 DevOps를 이해한다면 MLOps를 이해하는 데 도움이 됩니다.\nDevOps # DevOps는 Development(개발)와 Operations(운영)의 합성어로 소프트웨어의 개발(Development)과 운영(Operations)의 합성어로서 소프트웨어 개발자와 정보기술 전문가 간의 소통, 협업 및 통합을 강조하는 개발 환경이나 문화를 말합니다. DevOps의 목적은 소프트웨어 개발 조직과 운영 조직간의 상호 의존적 대응이며 조직이 소프트웨어 제품과 서비스를 빠른 시간에 개발 및 배포하는 것을 목적으로 합니다.\nSilo Effect # 그럼 간단한 상황 설명을 통해 DevOps가 왜 필요한지 알아보도록 하겠습니다.\n서비스 초기에는 지원하는 기능이 많지 않으며 팀 또는 회사의 규모가 작습니다. 이때에는 개발팀과 운영팀의 구분이 없거나 작은 규모의 팀으로 구분되어 있습니다. 핵심은 규모가 작다는 것에 있습니다. 이때는 서로 소통할 수 있는 접점이 많고, 집중해야 하는 서비스가 적기 때문에 빠르게 서비스를 개선해 나갈 수 있습니다.\n하지만 서비스의 규모가 커질수록 개발팀과 운영팀은 분리되고 서로 소통할 수 있는 채널의 물리적인 한계가 오게 됩니다. 예를 들어서 다른 팀과 함께하는 미팅에 팀원 전체가 미팅을 하는 것이 아니라 각 팀의 팀장 혹은 소수의 시니어만 참석하여 미팅을 진행하게 됩니다. 이런 소통 채널의 한계는 필연적으로 소통의 부재로 이어지게 됩니다. 그러다 보면 개발팀은 새로운 기능들을 계속해서 개발하고 운영팀 입장에서는 개발팀에서 개발한 기능이 배포 시 장애를 일으키는 등 여러 문제가 생기게 됩니다.\n위와 같은 상황이 반복되면 조직 이기주의라고 불리는 사일로 현상이 생길 수 있습니다.\n 사일로(silo)는 곡식이나 사료를 저장하는 굴뚝 모양의 창고를 의미한다. 사일로는 독립적으로 존재하며 저장되는 물품이 서로 섞이지 않도록 철저히 관리할 수 있도록 도와준다.\n사일로 효과(Organizational Silos Effect)는 조직 부서 간에 서로 협력하지 않고 내부 이익만을 추구하는 현상을 의미한다. 조직 내에서 개별 부서끼리 서로 담을 쌓고 각자의 이익에만 몰두하는 부서 이기주의를 일컫는다.\n 사일로 현상은 서비스 품질의 저하로 이어지게 됩니다. 이러한 사일로 현상을 해결하기 위해 나온 것이 바로 DevOps입니다.\nCI/CD # Continuous Integration(CI) 와 Continuous Delivery (CD)는 개발팀과 운영팀의 장벽을 해제하기 위한 구체적인 방법입니다.\n이 방법을 통해서 개발팀에서는 운영팀의 환경을 이해하고 개발팀에서 개발 중인 기능이 정상적으로 배포까지 이어질 수 있는지 확인합니다. 운영팀은 검증된 기능 또는 개선된 제품을 더 자주 배포해 고객의 제품 경험을 상승시킵니다.\n앞에서 설명한 내용을 종합하자면 DevOps는 개발팀과 운영팀 간의 문제가 있었고 이를 해결하기 위한 방법론입니다.\nMLOps # 1) ML+Ops # MLOps는 Machine Learning 과 Operations의 합성어로 DevOps에서 Dev가 ML로 바뀌었습니다. 이제 앞에서 살펴본 DevOps를 통해 MLOps가 무엇인지 짐작해 볼 수 있습니다. “MLOps는 머신러닝팀과 운영팀의 문제를 해결하기 위한 방법입니다.” 이 말은 머신러닝팀과 운영팀 사이에 문제가 발생했다는 의미입니다. 그럼 왜 머신러닝팀과 운영팀에는 문제가 발생했을까요? 두 팀 간의 문제를 알아보기 위해서 추천시스템을 예시로 알아보겠습니다.\nRule Based # 처음 추천시스템을 만드는 경우 간단한 규칙을 기반으로 아이템을 추천합니다. 예를 들어서 1주일간 판매량이 가장 많은 순서대로 보여주는 식의 방식을 이용합니다. 이 방식으로 모델을 정한다면 특별한 이유가 없는 이상 모델의 수정이 필요 없습니다.\nMachine Learning # 서비스의 규모가 조금 커지고 로그 데이터가 많이 쌓인다면 이를 이용해 아이템 기반 혹은 유저 기반의 머신러닝 모델을 생성합니다. 이때 모델은 정해진 주기에 따라 모델을 재학습 후 재배포합니다.\nDeep Learning # 개인화 추천에 대한 요구가 더 커지고 더 좋은 성능을 내는 모델을 필요해질 경우 딥러닝을 이용한 모델을 개발하기 시작합니다. 이때 만드는 모델은 머신러닝과 같이 정해진 주기에 따라 모델을 재학습 후 재배포합니다.\n위에서 설명한 것을 x축을 모델의 복잡도, y축을 모델의 성능으로 두고 그래프로 표현한다면 다음과 같이 복잡도가 올라갈 때 모델의 성능이 올라가는 상승 관계를 갖습니다. 머신러닝에서 딥러닝으로 넘어갈 머신러닝 팀이 새로 생기게 됩니다.\n만약 관리해야할 모델이 적다면 서로 협업을 통해서 충분히 해결할 수 있지만 개발해야 할 모델이 많아진다면 DevOps의 경우와 같이 사일로 현상이 나타나게 됩니다.\nDevOps의 목표와 맞춰서 생각해보면 MLOps의 목표는 개발한 모델이 정상적으로 배포될 수 있는지 테스트하는 것입니다. 개발팀에서 개발한 기능이 정상적으로 배포될 수 있는지 확인하는 것이 DevOps의 목표였다면, MLOps의 목표는 머신러닝 팀에서 개발한 모델이 정상적으로 배포될 수 있는지 확인하는 것입니다.\n2) ML -\u0026gt; Ops # 하지만 최근 나오고 있는 MLOps 관련 제품과 설명을 보면 꼭 앞에서 설명한 목표만을 대상으로 하고 있지 않습니다. 어떤 경우에는 머신러닝 팀에서 만든 모델을 이용해 직접 운영을 할 수 있도록 도와주려고 합니다. 이러한 니즈는 최근 머신러닝 프로젝트가 진행되는 과정에서 알 수 있습니다.\n추천시스템의 경우 운영에서 간단한 모델부터 시작해 운영할 수 있었습니다. 하지만 자연어, 이미지와 같은 곳에서는 규칙 기반의 모델보다는 딥러닝을 이용해 주어진 태스크를 해결할 수 있는지 검증(POC)를 선행하는 경우가 많습니다. 검증이 끝난 프로젝트는 이제 서비스를 위한 운영 환경을 개발하기 시작합니다. 하지만 머신러닝 팀 내의 자체 역량으로는 이 문제를 해결하기 쉽지 않습니다. 이를 해결하기 위해서 MLOps가 필요한 경우도 있습니다.\n3) 결론 # 요약하자면 MLOps는 두 가지 목표가 있습니다. 앞에서 설명한 MLOps는 ML+Ops 로 두 팀의 생산성 향상을 위한 것이였습니다. 반면, 뒤에서 설명한 것은 ML-\u0026gt;Ops 로 머신러닝 팀에서 직접 운영을 할 수 있도록 도와주는 것을 말합니다.\n"}),e.add({id:3,href:"/docs/introduction/levels/",title:"2. Levels of MLOps",description:"Levels of MLOps",content:"이번 페이지에서는 구글에서 발표한 MLOps의 단계를 보며 MLOps의 핵심 기능은 무엇인지 알아 보겠습니다.\nHidden Technical Debt in ML System # 구글은 무려 2015년부터 MLOps의 필요성을 말했습니다. Hidden Technical Debt in Machine Learning Systems 은 그런 구글의 생각을 담은 논문입니다.\n이 논문의 핵심은 바로 머신러닝을 이용한 제품을 만드는데 있어서 머신러닝 코드는 전체 시스템을 구성하는데 있어서 아주 일부일 뿐이라는 것입니다.\n구글은 이 논문을 더 발전시켜서 MLOps라는 용어를 만들어 확장시켰습니다. 더 자세한 내용은 구글 클라우드 홈페이지에서 더 자세한 내용을 확인할 수 있습니다. 이번 포스트에서는 구글에서 말하는 MLOps란 어떤 것인지에 대해서 설명해보고자 합니다.\n구글에서는 MLOps의 발전 단계를 총 3(0~2)단계로 나누었습니다. 각 단계들에 대해 설명하기 앞서 이전 포스트에서 설명했던 개념 중 필요한 부분을 다시 한번 보겠습니다.\n머신러닝 모델을 운영하기 위해서는 모델을 개발하는 머신러닝 팀과 배포 및 운영을 담당하는 운영팀이 있습니다. 이 두 팀의 원할한 협업을 위해서 MLOps가 필요하게 되었습니다. 이전에는 간단히 Continuous Integration(CI)/Continuous Deployment(CD)를 통해서 할 수 있다고 하였는데, 어떻게 CI/CD를 하는지에 대해서 알아 보겠습니다.\n0단계: 수동 프로세스 # 0단계에서 두 팀은 “모델”을 통해 소통합니다. 머신 러닝팀은 쌓여있는 데이터로 모델을 학습시키고 학습된 모델을 운영팀에게 전달 합니다. 운영팀은 이렇게 전달받은 모델을 배포합니다.\n초기의 머신 러닝 모델들은 이 “모델” 중심의 소통을 통해 배포합니다. 그런데 이런 배포 방식은 여러 문제가 있습니다.\n예를 들어서 어떤 기능에서는 파이썬 3.7을 쓰고 어떤 기능에서는 파이썬 3.8을 쓴다면 다음과 같은 상황을 자주 목격할 수 있습니다.\n이러한 상황이 일어나는 이유는 머신러닝 모델의 특성에 있습니다. 학습된 머신러닝 모델이 동작하기 위해서는 3가지가 필요합니다.\n 파이썬 코드 학습된 가중치 환경 (패키지, 버전 등)  만약 이 3가지 중 한 가지라도 전달이 잘못 된다면 모델이 동작하지 않거나 예상하지 못한 예측을 할수 있습니다. 그런데 많은 경우 환경이 일치하지 않아서 동작하지 않는 경우가 많습니다. 머신러닝은 다양한 오픈소스를 사용하는데 오픈소스는 특성상 어떤 버전을 쓰는지에 따라서 같은 함수라도 결과가 다를 수 있습니다.\n이러한 문제는 서비스 초기에는 관리할 모델이 많지 않기 때문에 금방 해결할 수 있습니다. 하지만 관리하는 기능들이 많아지고 서로 소통에 어려움을 겪게 된다면 성능이 더 좋은 모델을 빠르게 배포할 수 없게 됩니다.\n1단계: ML 파이프라인 자동화 # Pipeline # 그래서 MLOps에서는 “파이프라인(Pipeline)”을 이용해 이러한 문제를 방지하고자 했습니다. MLOps의 파이프라인은 도커와 같은 컨테이너를 이용해 머신러닝 엔지니어가 모델 개발에 사용한 것과 동일한 환경으로 동작되는 것을 보장합니다. 이를 통해서 환경이 달라서 모델이 동작하지 않는 상황을 방지합니다.\n그런데 파이프라인은 범용적인 용어로 여러 다양한 태스크에서 사용됩니다. 머신러닝 엔지니어가 작성하는 파이프라인의 역할은 무엇일까요?\n머신러닝 엔지니어가 작성하는 파이프라인은 학습된 모델을 생산합니다. 그래서 파이프라인 대신 학습 파이프라인(Training Pipeline)이 더 정확하다고 볼 수 있습니다.\nContinuous Training # 그리고 Continuous Training(CT) 개념이 추가됩니다. 그렇다면 CT는 왜 필요할까요?\nAuto Retrain # Real World에서 데이터는 Data Shift라는 데이터의 분포가 계속해서 변하는 특징이 있습니다. 그래서 과거에 학습한 모델이 시간이 지남에 따라 모델의 성능이 저하되는 문제가 있습니다. 이 문제를 해결하는 가장 간단하고 효과적인 해결책은 바로 최근 데이터를 이용해 모델을 재학습하는 것입니다. 변화된 데이터 분포에 맞춰서 모델을 재학습하면 다시 준수한 성능을 낼 수 있습니다.\nAuto Deploy # 하지만 제조업과 같이 한 공장에서 여러 레시피를 처리하는 경우 무조건 재학습을 하는 것이 좋지 않을 수 도 있습니다. Blind Spot이 대표적인 예입니다.\n예를 들어서 자동차 생산 라인에서 모델 A에 대해서 모델을 만들고 이를 이용해 예측을 진행하고 있었습니다. 만약 전혀 다른 모델 B가 들어오면 이전에 보지 못한 데이터 패턴이기 때문에 모델 B에 대해서 새로운 모델을 학습합니다.\n이제 모델 B에 대해서 모델을 만들었기 때문에 모델은 예측을 진행할 것 입니다. 그런데 만약 데이터가 다시 모델 A로 바뀐다면 어떻게 할까요?\n만약 Retraining 규칙만 있다면 다시 모델 A에 대해서 새로운 모델을 학습하게 됩니다. 그런데 머신러닝 모델이 충분한 성능을 보이기 위해서는 충분한 양의 데이터가 모여야 합니다. Blind Spot이란 이렇게 데이터를 모으기 위해서 모델이 동작하지 않는 구간을 말합니다.\n이러한 Blind Spot을 해결하는 방법은 간단할 수 있습니다. 바로 모델 A에 대한 모델이 과거에 있었는지 확인하고 만약 있었다면 새로운 모델을 바로 학습하기 보다는 이 전 모델을 이용해 다시 예측을 하면 이런 Blind Spot을 해결할 수 있습니다. 이렇게 모델와 같은 메타 데이터를 이용해 모델을 자동으로 변환해주는 것을 Auto Deploy라고 합니다.\n정리하자면 CT를 위해서는 Auto Retraining의과 Auto Deploy 두 가지 기능이 필요합니다. 둘은 서로의 단점을 보완해 계속해서 모델의 성능을 유지할 수 있게 합니다.\n2단계: CI/CD 파이프라인의 자동화 # 2단계의 제목은 CI와 CD의 자동화 입니다. DevOps에서의 CI/CD의 대상은 소스 코드입니다. 그렇다면 MLOps는 어떤 것이 CI/CD의 대상일까요?\nMLOps의 CI/CD 대상 또한 소스 코드인 것은 맞지만 조금 더 엄밀히 정의하자면 학습 파이프라인이라고 볼 수 있습니다.\n그래서 모델을 학습하는데 있어서 영향이 있는 변화에 대해서 실제로 모델이 정상적으로 학습이 되는지 (CI), 학습된 모델이 정상적으로 동작하는지 (CD)를 확인해야 합니다. 그래서 학습을 하는 코드에 직접적인 수정이 있는 경우에는 CI/CD를 진행해야 합니다.\n코드 외에도 사용하는 패키지의 버전, 파이썬의 버전 변경도 CI/CD의 대상입니다. 많은 경우 머신 러닝은 오픈 소스를 이용합니다. 하지만 오픈 소스는 그 특성상 버전이 바뀌었을 때 함수의 내부 로직이 변하는 경우도 있습니다. 물론 어느 정도 버전이 올라 갈 때 이와 관련된 알림을 주지만 한 번에 버전이 크게 바뀐다면 이러한 변화를 모를 수도 있습니다.\n그래서 사용하는 패키지의 버전이 변하는 경우에도 CI/CD를 통해 정상적으로 모델이 학습, 동작하는지 확인을 해야 합니다.\n"}),e.add({id:4,href:"/docs/introduction/component/",title:"3. Components of MLOps",description:"Describe MLOps Components",content:"Practitioners guide to MLOps # 2021년 5월에 발표된 구글의 white paper : Practitioners guide to MLOps: A framework for continuous delivery and automation of machine learning에서는 MLOps의 핵심 기능들로 다음과 같은 것들을 언급하였습니다.\n 각 기능이 어떤 역할을 하는지 살펴보겠습니다.\n1. Experimentation # 실험(Experimentation)은 머신러닝 엔지니어들이 데이터를 분석하고, 프로토타입 모델을 만들며 학습 기능을 구현할 수 있도록 하는 다음과 같은 기능을 제공합니다.\n 깃(Git)과 같은 버전 컨트롤 도구와 통합된 노트북(Jupyter Notebook) 환경 제공 사용한 데이터, 하이퍼 파라미터, 평가 지표를 포함한 실험 추적 기능 제공 데이터와 모델에 대한 분석 및 시각화 기능 제공  2. Data Processing # 데이터 처리(Data Processing)는 머신러닝 모델 개발 단계, 지속적인 학습(Continuous Training) 단계, 그리고 API 배포(API Deployment) 단계에서 많은 양의 데이터를 사용할 수 있게 해 주는 다음과 같은 기능을 제공합니다.\n 다양한 데이터 소스와 서비스에 호환되는 데이터 커넥터(connector) 기능 제공 다양한 형태의 데이터와 호환되는 데이터 인코더(encoder) \u0026amp; 디코더(decoder) 기능 제공 다양한 형태의 데이터에 대한 데이터 변환과 피처 엔지니어링(feature engineering) 기능 제공 학습과 서빙을 위한 확장 가능한 배치, 스트림 데이터 처리 기능 제공  3. Model training # 모델 학습(Model training)은 모델 학습을 위한 알고리즘을 효율적으로 실행시켜주는 다음과 같은 기능을 제공합니다.\n ML 프레임워크의 실행을 위한 환경 제공 다수의 GPU / 분산 학습 사용을 위한 분산 학습 환경 제공 하이퍼 파라미터 튜닝과 최적화 기능 제공  4. Model evaluation # 모델 평가(Model evaluation)는 실험 환경과 상용 환경에서 동작하는 모델의 성능을 관찰할 수 있는 다음과 같은 기능을 제공합니다.\n 평가 데이터에 대한 모델 성능 평가 기능 서로 다른 지속 학습 실행 결과에 대한 예측 성능 추적 서로 다른 모델의 성능 비교와 시각화 해석할 수 있는 AI 기술을 이용한 모델 출력 해석 기능 제공  5. Model serving # 모델 서빙(Model serving)은 상용 환경에 모델을 배포하고 서빙하기 위한 다음과 같은 기능들을 제공합니다.\n 저 지연 추론과 고가용성 추론 기능 제공 다양한 ML 모델 서빙 프레임워크 지원(Tensorflow Serving, TorchServe, NVIDIA Triton, Scikit-learn, XGGoost. etc) 복잡한 형태의 추론 루틴 기능 제공, 예를 들어 전처리(preprocess) 또는 후처리(postprocess) 기능과 최종 결과를 위해 다수의 모델이 사용되는 경우를 말합니다. 순간적으로 치솟는 추론 요청을 처리하기 위한 오토 스케일링(autoscaling) 기능 제공 추론 요청과 추론 결과에 대한 로깅 기능 제공  6. Online experimentation # 온라인 실험(Online experimentation)은 새로운 모델이 생성되었을 때, 이 모델을 배포하면 어느 정도의 성능을 보일 것인지 검증하는 기능을 제공합니다. 이 기능은 새 모델을 배포하는 것까지 연동하기 위해 모델 저장소(Model Registry)와 연동되어야 합니다.\n 카나리(canary) \u0026amp; 섀도(shadow) 배포 기능 제공 A/B 테스트 기능 제공 멀티 암드 밴딧(Multi-armed bandit) 테스트 기능 제공  7. Model Monitoring # 모델 모니터링(Model Monitoring)은 상용 환경에 배포된 모델이 정상적으로 동작하고 있는지를 모니터링하는 기능을 제공합니다. 예를 들어 모델의 성능이 떨어져 업데이트가 필요한지에 대한 정보 등을 제공합니다.\n8. ML Pipeline # 머신러닝 파이프라인(ML Pipeline)은 상용 환경에서 복잡한 ML 학습과 추론 작업을 구성하고 제어하고 자동화하기 위한 다음과 같은 기능을 제공합니다.\n 다양한 이벤트를 소스를 통한 파이프라인 실행 기능 파이프라인 파라미터와 생성되는 산출물 관리를 위한 머신러닝 메타데이터 추적과 연동 기능 일반적인 머신러닝 작업을 위한 내장 컴포넌트 지원과 사용자가 직접 구현한 컴포넌트에 대한 지원 기능 서로 다른 실행 환경 제공 기능  9. Model Registry # 모델 저장소(Model Registry)는 머신러닝 모델의 생명 주기(Lifecycle)을 중앙 저장소에서 관리할 수 있게 해 주는 기능을 제공합니다.\n 학습된 모델 그리고 배포된 모델에 대한 등록, 추적, 버저닝 기능 제공 배포를 위해 필요한 데이터와 런타임 패키지들에 대한 정보 저장 기능  10. Dataset and Feature Repository #  데이터에 대한 공유, 검색, 재사용 그리고 버전 관리 기능 이벤트 스트리밍 및 온라인 추론 작업에 대한 실시간 처리 및 저 지연 서빙 기능 사진, 텍스트, 테이블 형태의 데이터와 같은 다양한 형태의 데이터 지원 기능  11. ML Metadata and Artifact Tracking # MLOps의 각 단계에서는 다양한 형태의 산출물들이 생성됩니다. ML 메타데이터는 이런 산출물들에 대한 정보를 의미합니다. ML 메타데이터와 산출물 관리는 산출물의 위치, 타입, 속성, 그리고 관련된 실험(experiment)에 대한 정보를 관리하기 위해 다음과 같은 기능들을 제공합니다.\n ML 산출물에 대한 히스토리 관리 기능 실험과 파이프라인 파라미터 설정에 대한 추적, 공유 기능 ML 산출물에 대한 저장, 접근, 시각화, 다운로드 기능 제공 다른 MLOps 기능과의 통합 기능 제공  "}),e.add({id:5,href:"/docs/introduction/why_kubernetes/",title:"4. Why Kubernetes?",description:"Reason for using k8s in MLOps",content:"MLOps \u0026amp; Kubernetes # 그렇다면 MLOps를 이야기할 때, 쿠버네티스(Kubernetes)라는 단어가 항상 함께 들리는 이유가 무엇일까요?\n성공적인 MLOps 시스템을 구축하기 위해서는 MLOps의 구성요소 에서 설명한 것처럼 다양한 구성 요소들이 필요하지만, 각각의 구성 요소들이 유기적으로 운영되기 위해서는 인프라 레벨에서 수많은 이슈를 해결해야 합니다.\n간단하게는 수많은 머신러닝 모델의 학습 요청을 차례대로 실행하는 것, 다른 작업 공간에서도 같은 실행 환경을 보장해야 하는 것, 배포된 서비스에 장애가 생겼을 때 빠르게 대응해야 하는 것 등의 이슈 등을 생각해볼 수 있습니다.\n여기서 컨테이너(Container)와 컨테이너 오케스트레이션 시스템(Container Orchestration System)의 필요성이 등장합니다.\n쿠버네티스와 같은 컨테이너 오케스트레이션 시스템을 도입하면 실행 환경의 격리와 관리를 효율적으로 수행할 수 있습니다. 컨테이너 오케스트레이션 시스템을 도입한다면, 머신러닝 모델을 개발하고 배포하는 과정에서 다수의 개발자가 소수의 클러스터를 공유하면서 \u0026lsquo;1번 클러스터 사용 중이신가요?\u0026rsquo;, \u0026lsquo;GPU 사용 중이던 제 프로세스 누가 죽였나요?\u0026rsquo;, \u0026lsquo;누가 클러스터에 x 패키지 업데이트했나요?\u0026rsquo; 와 같은 상황을 방지할 수 있습니다.\nContainer # 그렇다면 컨테이너란 무엇일까요? 마이크로소프트에서는 컨테이너를 다음과 같이 정의하고 있습니다.\n 컨테이너란 : 애플리케이션의 표준화된 이식 가능한 패키징\n 그런데 왜 머신러닝에서 컨테이너가 필요할까요? 머신러닝 모델들은 운영체제나 Python 실행 환경, 패키지 버전 등에 따라 다르게 동작할 수 있습니다.\n이를 방지하기 위해서 머신러닝에 사용된 소스 코드와 함께 종속적인 실행 환경 전체를 하나로 묶어서(패키징해서) 공유하고 실행하는 데 활용할 수 있는 기술이 컨테이너라이제이션(Containerization) 기술입니다. 이렇게 패키징된 형태를 컨테이너 이미지라고 부르며, 컨테이너 이미지를 공유함으로써 사용자들은 어떤 시스템에서든 같은 실행 결과를 보장할 수 있게 됩니다.\n즉, 단순히 Jupyter Notebook 파일이나, 모델의 소스 코드와 requirements.txt 파일을 공유하는 것이 아닌, 모든 실행 환경이 담긴 컨테이너 이미지를 공유한다면 \u0026ldquo;제 노트북에서는 잘 되는데요?\u0026rdquo; 와 같은 상황을 피할 수 있습니다.\n컨테이너를 처음 접하시는 분들이 흔히 하시는 오해 중 하나는 \u0026ldquo;컨테이너 == 도커\u0026ldquo;라고 받아들이는 것입니다.\n도커는 컨테이너와 같은 의미를 지니는 개념이 아니라, 컨테이너를 띄우거나, 컨테이너 이미지를 만들고 공유하는 것과 같이 컨테이너를 더욱더 쉽고 유연하게 사용할 수 있는 기능을 제공해주는 도구입니다. 정리하자면 컨테이너는 가상화 기술이고, 도커는 가상화 기술의 구현체라고 말할 수 있습니다.\n다만, 도커는 여러 컨테이너 가상화 도구 중에서 쉬운 사용성과 높은 효율성을 바탕으로 가장 빠르게 성장하여 대세가 되었기에 컨테이너하면 도커라는 이미지가 자동으로 떠오르게 되었습니다. 이렇게 컨테이너와 도커 생태계가 대세가 되기까지는 다양한 이유가 있지만, 기술적으로 자세한 이야기는 모두의 MLOps의 범위를 넘어서기 때문에 다루지는 않겠습니다.\n컨테이너 혹은 도커를 처음 들어보시는 분들에게는 모두의 MLOps의 내용이 다소 어렵게 느껴질 수 있으므로, 생활코딩, subicura 님의 개인 블로그 글 등의 자료를 먼저 살펴보는 것을 권장합니다.\nContainer Orchestration System # 그렇다면 컨테이너 오케스트레이션 시스템은 무엇일까요? 오케스트레이션이라는 단어에서 추측해 볼 수 있듯이, 수많은 컨테이너가 있을 때 컨테이너들이 서로 조화롭게 구동될 수 있도록 지휘하는 시스템에 비유할 수 있습니다.\n컨테이너 기반의 시스템에서 서비스는 컨테이너의 형태로 사용자들에게 제공됩니다. 이때 관리해야 할 컨테이너의 수가 적다면 운영 담당자 한 명이서도 충분히 모든 상황에 대응할 수 있습니다.\n하지만, 수백 개 이상의 컨테이너가 수 십 대 이상의 클러스터에서 구동되고 있고 장애를 일으키지 않고 항상 정상 동작해야 한다면, 모든 서비스의 정상 동작 여부를 담당자 한 명이 파악하고 이슈에 대응하는 것은 불가능에 가깝습니다.\n예를 들면, 모든 서비스가 정상적으로 동작하고 있는지를 계속해서 모니터링(Monitoring)해야 합니다.\n만약, 특정 서비스가 장애를 일으켰다면 여러 컨테이너의 로그를 확인해가며 문제를 파악해야 합니다.\n또한, 특정 클러스터나 특정 컨테이너에 작업이 몰리지 않도록 스케줄링(Scheduling)하고 로드 밸런싱(Load Balancing)하며, 스케일링(Scaling)하는 등의 수많은 작업을 담당해야 합니다. 이렇게 수많은 컨테이너의 상태를 지속해서 관리하고 운영하는 과정을 조금이나마 쉽게, 자동으로 할 수 있는 기능을 제공해주는 소프트웨어가 바로 컨테이너 오케스트레이션 시스템입니다.\n머신러닝에서는 어떻게 쓰일 수 있을까요?\n예를 들어서 GPU가 있어야 하는 딥러닝 학습 코드가 패키징된 컨테이너는 사용 가능한 GPU가 있는 클러스터에서 수행하고, 많은 메모리를 필요로 하는 데이터 전처리 코드가 패키징된 컨테이너는 메모리의 여유가 많은 클러스터에서 수행하고, 학습 중에 클러스터에 문제가 생기면 자동으로 같은 컨테이너를 다른 클러스터로 이동시키고 다시 학습을 진행하는 등의 작업을 사람이 일일이 수행하지 않고, 자동으로 관리하는 시스템을 개발한 뒤 맡기는 것입니다.\n집필을 하는 2022년을 기준으로 쿠버네티스는 컨테이너 오케스트레이션 시스템의 사실상의 표준(De facto standard)입니다.\nCNCF에서 2018년 발표한 Survey 에 따르면 다음 그림과 같이 이미 두각을 나타내고 있었으며, 2019년 발표한 Survey에 따르면 그중 78%가 상용 수준(Production Level)에서 사용하고 있다는 것을 알 수 있습니다.\n쿠버네티스 생태계가 이처럼 커지게 된 이유에는 여러 가지 이유가 있습니다. 하지만 도커와 마찬가지로 쿠버네티스 역시 머신러닝 기반의 서비스에서만 사용하는 기술이 아니기에, 자세히 다루기에는 상당히 많은 양의 기술적인 내용을 다루어야 하므로 이번 모두의 MLOps에서는 자세한 내용은 생략할 예정입니다.\n다만, 모두의 MLOps에서 앞으로 다룰 내용은 도커와 쿠버네티스에 대한 내용을 어느 정도 알고 계신 분들을 대상으로 작성하였습니다. 따라서 쿠버네티스에 대해 익숙하지 않으신 분들은 다음 쿠버네티스 공식 문서, subicura 님의 개인 블로그 글 등의 쉽고 자세한 자료들을 먼저 참고해주시는 것을 권장합니다.\n"}),e.add({id:6,href:"/docs/setup-kubernetes/",title:"Setup Kubernetes",description:"Setup kubernetes.",content:""}),e.add({id:7,href:"/docs/setup-kubernetes/intro/",title:"1. Introduction",description:"Setup Introduction",content:"MLOps 시스템 구축해보기 # MLOps를 공부하는 데 있어서 가장 큰 장벽은 MLOps 시스템을 구성해보고 사용해보기가 어렵다는 점입니다. AWS, GCP 등의 퍼블릭 클라우드 혹은 Weight \u0026amp; Bias, neptune.ai 등의 상용 툴을 사용해보기에는 과금에 대한 부담이 존재하고, 처음부터 모든 환경을 혼자서 구성하기에는 어디서부터 시작해야 할지 막막하게 느껴질 수밖에 없습니다.\n이런 이유들로 MLOps를 선뜻 시작해보지 못하시는 분들을 위해, 모두의 MLOps에서는 우분투가 설치되는 데스크톱 하나만 준비되어 있다면 MLOps 시스템을 밑바닥부터 구축하고 사용해 볼 수 있는 방법을 다룰 예정입니다.\n   우분투 데스크탑 환경을 준비할 수 없는 경우, 가상머신을 활용하여 환경을 구성하기  Windows 혹은 Intel Mac을 사용해 모두의 MLops 실습을 진행 중인 분들은 Virtual Box, VMware 등의 가상머신 소프트웨어를 이용하여 우분투 데스크탑 환경을 준비할 수 있습니다. 이 때, 권장 사양을 맞춰 가상 머신을 생성해주시기 바랍니다.\n또한, M1 Mac을 사용하시는 분들은 작성일(2022년 2월) 기준으로는 Virtual Box, VMware 는 이용할 수 없습니다. (M1 Apple Silicone Mac에 최적화된 macOS 앱 지원 확인하기)\n따라서, 클라우드 환경을 이용해 실습하는 것이 아니라면, UTM , Virtual machines for Mac을 설치하여 가상 머신을 이용해주세요.\n(앱스토어에서 구매하여 다운로드 받는 소프트웨어는 일종의 Donation 개념의 비용 지불입니다. 무료 버전과 자동 업데이트 정도의 차이가 있어, 무료버전을 사용해도 무방합니다.)\n해당 가상머신 소프트웨어는 Ubuntu 20.04.3 LTS 실습 운영체제를 지원하고 있어, M1 Mac에서 실습을 수행하는 것을 가능하게 합니다.\n 하지만 MLOps의 구성요소에서 설명하는 요소들을 모두 사용해볼 수는 없기에, 모두의 MLOps에서는 대표적인 오픈소스만을 설치한 뒤, 서로 연동하여 사용하는 부분을 주로 다룰 예정입니다.\n모두의 MLOps에서 설치하는 오픈소스가 표준을 의미하는 것은 아니며, 여러분의 상황에 맞게 적절한 툴을 취사선택하는 것을 권장합니다.\n구성 요소 # 이 글에서 만들어 볼 MLOps 시스템의 구성 요소들과 각 버전은 아래와 같은 환경에서 검증되었습니다.\n원활한 환경에서 테스트하기 위해 싱글 노드 클러스터 (혹은 클러스터) 와 클라이언트를 분리하여 설명해 드릴 예정입니다.\n클러스터 는 우분투가 설치되어 있는 데스크톱 하나를 의미합니다.\n클라이언트 는 노트북 혹은 클러스터가 설치되어 있는 데스크톱 외의 클라이언트로 사용할 수 있는 다른 데스크톱을 사용하는 것을 권장합니다.\n하지만 두 대의 머신을 준비할 수 없다면 데스크톱 하나를 동시에 클러스터와 클라이언트 용도로 사용하셔도 괜찮습니다.\n클러스터 # 1. Software # 아래는 클러스터에 설치해야 할 소프트웨어 목록입니다.\n   Software Version     Ubuntu 20.04.3 LTS   Docker (Server) 20.10.11   NVIDIA-Driver 470.86   Kubernetes v1.21.7   Kubeflow v1.4.0   MLFlow v1.21.0    2. Helm Chart # 아래는 Helm을 이용해 설치되어야 할 써드파티 소프트웨어 목록입니다.\n   Helm Chart Repo Name Version     datawire/ambassador 6.9.3   seldonio/seldon-core-operator 1.11.2    클라이언트 # 클라이언트는 MacOS (Intel CPU), Ubuntu 20.04 에서 검증되었습니다.\n   Software Version     kubectl v1.21.7   helm v3.7.1   kustomize v3.10.0    Minimum System Requirements # 모두의 MLOps를 설치할 클러스터는 다음과 같은 사양을 만족시키는 것을 권장합니다.\n이는 Kubernetes 및 Kubeflow 의 권장 사양에 의존합니다.\n CPU : 6 core RAM : 12GB DISK : 50GB GPU : NVIDIA GPU (Optional)  "}),e.add({id:8,href:"/docs/setup-kubernetes/kubernetes/",title:"2. Setup Kubernetes",description:"Setup Kubernetes",content:"Setup Kubernetes Cluster # 쿠버네티스를 처음 배우시는 분들에게 첫 진입 장벽은 쿠버네티스 실습 환경을 구축하는 것입니다.\n프로덕션 레벨의 쿠버네티스 클러스터를 구축할 수 있게 공식적으로 지원하는 도구는 kubeadm 이지만, 사용자들이 조금 더 쉽게 구축할 수 있도록 도와주는 kubespray, kops 등의 도구도 존재하며, 학습 목적을 위해서 컴팩트한 쿠버네티스 클러스터를 정말 쉽게 구축할 수 있도록 도와주는 k3s, minikube, microk8s, kind 등의 도구도 존재합니다.\n각각의 도구는 장단점이 다르기에 사용자마다 선호하는 도구가 다른 점을 고려하여, 본 글에서는 kubeadm, k3s, minikube의 3가지 도구를 활용하여 쿠버네티스 클러스터를 구축하는 방법을 다룹니다. 각 도구에 대한 자세한 비교는 다음 쿠버네티스 공식 문서를 확인해주시기를 바랍니다.\n모두의 MLOps에서 권장하는 툴은 k3s로 쿠버네티스 클러스터를 구축할 때 쉽게 할 수 있다는 장점이 있습니다.\n만약 쿠버네티스의 모든 기능을 사용하고 노드 구성까지 활용하고 싶다면 kubeadm을 권장해 드립니다.\nminikube는 저희가 설명하는 컴포넌트 외에도 다른 쿠버네티스를 add-on 형식으로 쉽게 설치할 수 있다는 장점이 있습니다.\n본 모두의 MLOps에서는 구축하게 될 MLOps 구성 요소들을 원활히 사용하기 위해, 각각의 도구를 활용해 쿠버네티스 클러스터를 구축할 때, 추가로 설정해 주어야 하는 부분이 추가되어 있습니다.\nUbuntu OS까지는 설치되어 있는 데스크탑을 k8s cluster로 구축한 뒤, 외부 클라이언트 노드에서 쿠버네티스 클러스터에 접근하는 것을 확인하는 것까지가 본 Setup Kubernetes단원의 범위입니다.\n자세한 구축 방법은 3가지 도구마다 다르기에 다음과 같은 흐름으로 구성되어 있습니다.\n3. Setup Prerequisite 4. Setup Kubernetes 4.1. with k3s 4.2. with minikube 4.3. with kubeadm 5. Setup Kubernetes Modules 그럼 이제 각각의 도구를 활용해 쿠버네티스 클러스터를 구축해보겠습니다. 반드시 모든 도구를 사용해 볼 필요는 없으며, 이 중 여러분이 익숙하신 도구를 활용해주시면 충분합니다.\n"}),e.add({id:9,href:"/docs/setup-kubernetes/install-prerequisite/",title:"3. Install Prerequisite",description:"Install docker",content:"이 페이지에서는 쿠버네티스를 설치하기에 앞서, 클러스터와 클라이언트에 설치 혹은 설정해두어야 하는 컴포넌트들에 대한 매뉴얼을 설명합니다.\nInstall apt packages # 추후 클라이언트와 클러스터의 원활한 통신을 위해서는 Port-Forwarding을 수행해야 할 일이 있습니다. Port-Forwarding을 위해서는 클러스터에 다음 패키지를 설치해 주어야 합니다.\nsudo apt-get update sudo apt-get install -y socat Install Docker #   도커 설치에 필요한 APT 패키지들을 설치합니다.\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y ca-certificates curl gnupg lsb-release   도커의 공식 GPG key를 추가합니다.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg   apt 패키지 매니저로 도커를 설치할 때, stable Repository에서 받아오도록 설정합니다.\necho \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null   현재 설치할 수 있는 도커 버전을 확인합니다.\nsudo apt-get update \u0026amp;\u0026amp; apt-cache madison docker-ce 출력되는 버전 중 5:20.10.11~3-0~ubuntu-focal 버전이 있는지 확인합니다.\napt-cache madison docker-ce | grep 5:20.10.11~3-0~ubuntu-focal 정상적으로 추가가 된 경우 다음과 같이 출력됩니다.\ndocker-ce | 5:20.10.11~3-0~ubuntu-focal | https://download.docker.com/linux/ubuntu focal/stable amd64 Packages   5:20.10.11~3-0~ubuntu-focal 버전의 도커를 설치합니다.\nsudo apt-get install -y containerd.io docker-ce=5:20.10.11~3-0~ubuntu-focal docker-ce-cli=5:20.10.11~3-0~ubuntu-focal   도커가 정상적으로 설치된 것을 확인합니다.\nsudo docker run hello-world 명령어 실행 후 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ sudo docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   docker 관련 command를 sudo 키워드 없이 사용할 수 있게 하도록 다음 명령어를 통해 권한을 추가합니다.\nsudo groupadd docker sudo usermod -aG docker $USER newgrp docker   sudo 키워드 없이 docker command를 사용할 수 있게 된 것을 확인하기 위해, 다시 한번 docker run을 실행합니다.\ndocker run hello-world 명령어 실행 후 다음과 같은 메시지가 보이면 정상적으로 권한이 추가된 것을 의미합니다.\nmlops@ubuntu:~$ docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   Turn off Swap Memory # kubelet 이 정상적으로 동작하게 하기 위해서는 클러스터 노드에서 swap이라고 불리는 가상메모리를 꺼 두어야 합니다. 다음 명령어를 통해 swap을 꺼 둡니다.\n(클러스터와 클라이언트를 같은 데스크톱에서 사용할 때 swap 메모리를 종료하면 속도의 저하가 있을 수 있습니다)\nsudo sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab sudo swapoff -a Install Kubectl # kubectl 은 쿠버네티스 클러스터에 API를 요청할 때 사용하는 클라이언트 툴입니다. 클라이언트 노드에 설치해두어야 합니다.\n  현재 폴더에 kubectl v1.21.7 버전을 다운받습니다.\ncurl -LO https://dl.k8s.io/release/v1.21.7/bin/linux/amd64/kubectl   kubectl 을 사용할 수 있도록 파일의 권한과 위치를 변경합니다.\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl   정상적으로 설치되었는지 확인합니다.\nkubectl version --client 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nClient Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.7\u0026#34;, GitCommit:\u0026#34;1f86634ff08f37e54e8bfcd86bc90b61c98f84d4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-11-17T14:41:19Z\u0026#34;, GoVersion:\u0026#34;go1.16.10\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;}   여러 개의 쿠버네티스 클러스터를 사용하는 경우, 여러 개의 kubeconfig 파일을 관리해야 하는 경우가 있습니다.\n여러 개의 kubeconfig 파일 혹은 여러 개의 kube-context를 효율적으로 관리하는 방법은 다음과 같은 문서를 참고하시기 바랍니다.\n https://dev.to/aabiseverywhere/configuring-multiple-kubeconfig-on-your-machine-59eo https://github.com/ahmetb/kubectx    References #  Install Docker Engine on Ubuntu 리눅스에 kubectl 설치 및 설정  "}),e.add({id:10,href:"/docs/setup-kubernetes/kubernetes-with-k3s/",title:"4.1. Install Kubernetes - K3s",description:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성 요소들을 클러스터에 설치합니다.\nInstall Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\nk3s 에서는 기본값으로 containerd를 백엔드로 이용해 설치합니다. 하지만 저희는 GPU를 사용하기 위해서 docker를 백엔드로 사용해야 하므로 --docker 옵션을 통해 백엔드를 docker로 설치하겠습니다.\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.21.7+k3s1 sh -s - server --disable traefik --disable servicelb --disable local-storage --docker k3s를 설치 후 k3s config를 확인합니다\nsudo cat /etc/rancher/k3s/k3s.",content:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성 요소들을 클러스터에 설치합니다.\nInstall Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\nk3s 에서는 기본값으로 containerd를 백엔드로 이용해 설치합니다. 하지만 저희는 GPU를 사용하기 위해서 docker를 백엔드로 사용해야 하므로 --docker 옵션을 통해 백엔드를 docker로 설치하겠습니다.\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.21.7+k3s1 sh -s - server --disable traefik --disable servicelb --disable local-storage --docker k3s를 설치 후 k3s config를 확인합니다\nsudo cat /etc/rancher/k3s/k3s.yaml 정상적으로 설치되면 다음과 같은 항목이 출력됩니다.\n(보안 문제와 관련된 키들은 \u0026lt;\u0026hellip;\u0026gt;로 가렸습니다.)\napiVersion: v1 clusters: - cluster: certificate-authority-data: \u0026lt;...\u0026gt; server: https://127.0.0.1:6443 name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: \u0026lt;...\u0026gt; client-key-data: \u0026lt;...\u0026gt; 2. 쿠버네티스 클러스터 셋업 # k3s config를 클러스터의 kubeconfig로 사용하기 위해서 복사합니다.\nmkdir .kube sudo cp /etc/rancher/k3s/k3s.yaml .kube/config 복사된 config 파일에 user가 접근할 수 있는 권한을 줍니다.\nsudo chown $USER:$USER .kube/config 3. 쿠버네티스 클라이언트 셋업 # 이제 클러스터에서 설정한 kubeconfig를 로컬로 이동합니다. 로컬에서는 경로를 ~/.kube/config로 설정합니다.\n처음 복사한 config 파일에는 server ip가 https://127.0.0.1:6443 으로 되어 있습니다.\n이 값을 클러스터의 ip에 맞게 수정합니다.\n(이번 페이지에서 사용하는 클러스터의 ip에 맞춰서 https://192.168.0.19:6443 으로 수정했습니다.)\napiVersion: v1 clusters: - cluster: certificate-authority-data: \u0026lt;...\u0026gt; server: https://192.168.0.19:6443 name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: \u0026lt;...\u0026gt; client-key-data: \u0026lt;...\u0026gt; 4. 쿠버네티스 기본 모듈 설치 # Setup Kubernetes Modules을 참고하여 다음 컴포넌트들을 설치해 주시기 바랍니다.\n helm kustomize CSI plugin [Optional] nvidia-docker, nvidia-device-plugin  5. 정상 설치 확인 # 최종적으로 node가 Ready 인지, OS, Docker, Kubernetes 버전을 확인합니다.\nkubectl get nodes -o wide 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ubuntu Ready control-plane,master 11m v1.21.7+k3s1 192.168.0.19 \u0026lt;none\u0026gt; Ubuntu 20.04.3 LTS 5.4.0-91-generic docker://20.10.11 6. References #  https://rancher.com/docs/k3s/latest/en/installation/install-options/  "}),e.add({id:11,href:"/docs/setup-kubernetes/kubernetes-with-minikube/",title:"4.2. Install Kubernetes - Minikube",description:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성 요소들을 클러스터에 설치합니다.\nInstall Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\nMinikube binary # Minikube를 사용하기 위해, v1.24.0 버전의 Minikube 바이너리를 설치합니다.\nwget https://github.com/kubernetes/minikube/releases/download/v1.24.0/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube 정상적으로 설치되었는지 확인합니다.\nminikube version 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ minikube version minikube version: v1.24.0 commit: 76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b 2. 쿠버네티스 클러스터 셋업 # 이제 Minikube를 활용해 쿠버네티스 클러스터를 클러스터에 구축합니다.",content:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성 요소들을 클러스터에 설치합니다.\nInstall Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\nMinikube binary # Minikube를 사용하기 위해, v1.24.0 버전의 Minikube 바이너리를 설치합니다.\nwget https://github.com/kubernetes/minikube/releases/download/v1.24.0/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube 정상적으로 설치되었는지 확인합니다.\nminikube version 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ minikube version minikube version: v1.24.0 commit: 76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b 2. 쿠버네티스 클러스터 셋업 # 이제 Minikube를 활용해 쿠버네티스 클러스터를 클러스터에 구축합니다. GPU 의 원활한 사용과 클러스터-클라이언트 간 통신을 간편하게 수행하기 위해, Minikube 는 driver=none 옵션을 활용하여 실행합니다. driver=none 옵션은 root user 로 실행해야 함에 주의 바랍니다.\nroot user로 전환합니다.\nsudo su minikube start를 수행하여 쿠버네티스 클러스터 구축을 진행합니다. Kubeflow의 원활한 사용을 위해, 쿠버네티스 버전은 v1.21.7로 지정하여 구축하며 --extra-config를 추가합니다.\nminikube start --driver=none \\ --kubernetes-version=v1.21.7 \\ --extra-config=apiserver.service-account-signing-key-file=/var/lib/minikube/certs/sa.key \\ --extra-config=apiserver.service-account-issuer=kubernetes.default.svc Disable default addons # Minikube를 설치하면 Default로 설치되는 addon이 존재합니다. 이 중 저희가 사용하지 않을 addon을 비활성화합니다.\nminikube addons disable storage-provisioner minikube addons disable default-storageclass 모든 addon이 비활성화된 것을 확인합니다.\nminikube addons list 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nroot@ubuntu:/home/mlops# minikube addons list |-----------------------------|----------|--------------|-----------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|-----------------------| | ambassador | minikube | disabled | unknown (third-party) | | auto-pause | minikube | disabled | google | | csi-hostpath-driver | minikube | disabled | kubernetes | | dashboard | minikube | disabled | kubernetes | | default-storageclass | minikube | disabled | kubernetes | | efk | minikube | disabled | unknown (third-party) | | freshpod | minikube | disabled | google | | gcp-auth | minikube | disabled | google | | gvisor | minikube | disabled | google | | helm-tiller | minikube | disabled | unknown (third-party) | | ingress | minikube | disabled | unknown (third-party) | | ingress-dns | minikube | disabled | unknown (third-party) | | istio | minikube | disabled | unknown (third-party) | | istio-provisioner | minikube | disabled | unknown (third-party) | | kubevirt | minikube | disabled | unknown (third-party) | | logviewer | minikube | disabled | google | | metallb | minikube | disabled | unknown (third-party) | | metrics-server | minikube | disabled | kubernetes | | nvidia-driver-installer | minikube | disabled | google | | nvidia-gpu-device-plugin | minikube | disabled | unknown (third-party) | | olm | minikube | disabled | unknown (third-party) | | pod-security-policy | minikube | disabled | unknown (third-party) | | portainer | minikube | disabled | portainer.io | | registry | minikube | disabled | google | | registry-aliases | minikube | disabled | unknown (third-party) | | registry-creds | minikube | disabled | unknown (third-party) | | storage-provisioner | minikube | disabled | kubernetes | | storage-provisioner-gluster | minikube | disabled | unknown (third-party) | | volumesnapshots | minikube | disabled | kubernetes | |-----------------------------|----------|--------------|-----------------------| 3. 쿠버네티스 클라이언트 셋업 # 이번에는 클라이언트에 쿠버네티스의 원활한 사용을 위한 도구를 설치합니다. 클라이언트와 클러스터 노드가 분리되지 않은 경우에는 root user로 모든 작업을 진행해야 함에 주의바랍니다.\n클라이언트와 클러스터 노드가 분리된 경우, 우선 kubernetes의 관리자 인증 정보를 클라이언트로 가져옵니다.\n 클러스터에서 config를 확인합니다.  # 클러스터 노드 minikube kubectl -- config view --flatten 다음과 같은 정보가 출력됩니다.  apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUd.... extensions: - extension: last-update: Mon, 06 Dec 2021 06:55:46 UTC provider: minikube.sigs.k8s.io version: v1.24.0 name: cluster_info server: https://192.168.0.62:8443 name: minikube contexts: - context: cluster: minikube extensions: - extension: last-update: Mon, 06 Dec 2021 06:55:46 UTC provider: minikube.sigs.k8s.io version: v1.24.0 name: context_info namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate-data: LS0tLS1CRUdJTi.... client-key-data: LS0tLS1CRUdJTiBSU0.... 클라이언트 노드에서 .kube 폴더를 생성합니다.  # 클라이언트 노드 mkdir -p /home/$USER/.kube 해당 파일에 2. 에서 출력된 정보를 붙여넣은 뒤 저장합니다.  vi /home/$USER/.kube/config 4. 쿠버네티스 기본 모듈 설치 # Setup Kubernetes Modules을 참고하여 다음 컴포넌트들을 설치해 주시기 바랍니다.\n helm kustomize CSI plugin [Optional] nvidia-docker, nvidia-device-plugin  5. 정상 설치 확인 # 최종적으로 node가 Ready 인지, OS, Docker, Kubernetes 버전을 확인합니다.\nkubectl get nodes -o wide 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ubuntu Ready control-plane,master 2d23h v1.21.7 192.168.0.75 \u0026lt;none\u0026gt; Ubuntu 20.04.3 LTS 5.4.0-91-generic docker://20.10.11 "}),e.add({id:12,href:"/docs/setup-kubernetes/kubernetes-with-kubeadm/",title:"4.3. Install Kubernetes - Kubeadm",description:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성 요소들을 클러스터에 설치합니다.\nInstall Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\n쿠버네티스를 위한 네트워크의 설정을 변경합니다.\nsudo modprobe br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 2. 쿠버네티스 클러스터 셋업 #  kubeadm : kubelet을 서비스에 등록하고, 클러스터 컴포넌트들 사이의 통신을 위한 인증서 발급 등 설치 과정 자동화 kubelet : container 리소스를 실행, 종료를 해 주는 컨테이너 핸들러 kubectl : 쿠버네티스 클러스터를 터미널에서 확인, 조작하기 위한 CLI 도구  다음 명령어를 통해 kubeadm, kubelet, kubectl을 설치합니다.",content:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성 요소들을 클러스터에 설치합니다.\nInstall Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\n쿠버네티스를 위한 네트워크의 설정을 변경합니다.\nsudo modprobe br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 2. 쿠버네티스 클러스터 셋업 #  kubeadm : kubelet을 서비스에 등록하고, 클러스터 컴포넌트들 사이의 통신을 위한 인증서 발급 등 설치 과정 자동화 kubelet : container 리소스를 실행, 종료를 해 주는 컨테이너 핸들러 kubectl : 쿠버네티스 클러스터를 터미널에서 확인, 조작하기 위한 CLI 도구  다음 명령어를 통해 kubeadm, kubelet, kubectl을 설치합니다. 실수로 이 컴포넌트들의 버전이 변경하면, 예기치 않은 장애를 낳을 수 있으므로 컴포넌트들이 변경되지 않도록 설정합니다.\nsudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl \u0026amp;\u0026amp; sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg \u0026amp;\u0026amp; echo \u0026#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list \u0026amp;\u0026amp; sudo apt-get update sudo apt-get install -y kubelet=1.21.7-00 kubeadm=1.21.7-00 kubectl=1.21.7-00 \u0026amp;\u0026amp; sudo apt-mark hold kubelet kubeadm kubectl kubeadm, kubelet, kubectl 이 잘 설치되었는지 확인합니다.\nmlops@ubuntu:~$ kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.7\u0026#34;, GitCommit:\u0026#34;1f86634ff08f37e54e8bfcd86bc90b61c98f84d4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-11-17T14:40:08Z\u0026#34;, GoVersion:\u0026#34;go1.16.10\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} mlops@ubuntu:~$ kubelet --version Kubernetes v1.21.7 mlops@ubuntu:~$ kubectl version --client Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.7\u0026#34;, GitCommit:\u0026#34;1f86634ff08f37e54e8bfcd86bc90b61c98f84d4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-11-17T14:41:19Z\u0026#34;, GoVersion:\u0026#34;go1.16.10\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 이제 kubeadm을 사용하여 쿠버네티스를 설치합니다.\nkubeadm config images list kubeadm config images pull sudo kubeadm init --pod-network-cidr=10.244.0.0/16 kubectl을 통해서 쿠버네티스 클러스터를 제어할 수 있도록 admin 인증서를 $HOME/.kube/config 경로에 복사합니다.\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config CNI를 설치합니다. 쿠버네티스 내부의 네트워크 설정을 전담하는 CNI는 여러 종류가 있으며, 모두의 MLOps에서는 flannel을 사용합니다.\nkubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.13.0/Documentation/kube-flannel.yml 쿠버네티스 노드의 종류에는 크게 마스터 노드와 워커 노드가 있습니다. 안정성을 위하여 마스터 노드에는 쿠버네티스 클러스터를 제어하는 작업만 실행되도록 하는 것이 일반적이지만, 이 매뉴얼에서는 싱글 클러스터를 가정하고 있으므로 마스터 노드에 모든 종류의 작업이 실행될 수 있도록 설정합니다.\nkubectl taint nodes --all node-role.kubernetes.io/master- 3. 쿠버네티스 클라이언트 셋업 # 클러스터에 생성된 kubeconfig 파일을 클라이언트에 복사하여 kubectl을 통해 클러스터를 제어할 수 있도록 합니다.\nmkdir -p $HOME/.kube scp -p {CLUSTER_USER_ID}@{CLUSTER_IP}:~/.kube/config ~/.kube/config 4. 쿠버네티스 기본 모듈 설치 # Setup Kubernetes Modules을 참고하여 다음 컴포넌트들을 설치해 주시기 바랍니다.\n helm kustomize CSI plugin [Optional] nvidia-docker, nvidia-device-plugin  5. 정상 설치 확인 # 다음 명령어를 통해 노드의 STATUS가 Ready 상태가 되었는지 확인합니다.\nkubectl get nodes Ready 가 되면 다음과 비슷한 결과가 출력됩니다.\nNAME STATUS ROLES AGE VERSION ubuntu Ready control-plane,master 2m55s v1.21.7 6. References #  kubeadm  "}),e.add({id:13,href:"/docs/setup-kubernetes/install-kubernetes-module/",title:"5. Install Kubernetes Modules",description:"Install Helm, Kustomize",content:"Setup Kubernetes Modules # 이번 페이지에서는 클러스터에서 사용할 모듈을 클라이언트 노드에서 설치하는 과정에 관해서 설명합니다.\n앞으로 소개되는 과정은 모두 클라이언트 노드에서 진행됩니다.\nHelm # Helm은 쿠버네티스 패키지와 관련된 자원을 한 번에 배포하고 관리할 수 있게 도와주는 패키지 매니징 도구 중 하나입니다.\n 현재 폴더에 Helm v3.7.1 버전을 내려받습니다.    For Linux amd64\nwget https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz   다른 OS는 공식 홈페이지를 참고하시어, 클라이언트 노드의 OS와 CPU에 맞는 바이너리의 다운 경로를 확인하시기 바랍니다.\n  helm을 사용할 수 있도록 압축을 풀고, 파일의 위치를 변경합니다.  tar -zxvf helm-v3.7.1-linux-amd64.tar.gz sudo mv linux-amd64/helm /usr/local/bin/helm 정상적으로 설치되었는지 확인합니다.  helm help 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nThe Kubernetes package manager Common actions for Helm: - helm search: search for charts - helm pull: download a chart to your local directory to view - helm install: upload the chart to Kubernetes - helm list: list releases of charts Environment variables: | Name | Description | |--------------------------|---------------------------------------------------------------------| | $HELM_CACHE_HOME | set an alternative location for storing cached files. | | $HELM_CONFIG_HOME | set an alternative location for storing Helm configuration. | | $HELM_DATA_HOME | set an alternative location for storing Helm data. | ... Kustomize # kustomize 또한 여러 쿠버네티스 리소스를 한 번에 배포하고 관리할 수 있게 도와주는 패키지 매니징 도구 중 하나입니다.\n 현재 폴더에 kustomize v3.10.0 버전의 바이너리를 다운받습니다.    For Linux amd64\nwget https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv3.10.0/kustomize_v3.10.0_linux_amd64.tar.gz   다른 OS는 kustomize/v3.10.0에서 확인 후 다운로드 받습니다.\n  kustomize 를 사용할 수 있도록 압축을 풀고, 파일의 위치를 변경합니다.  tar -zxvf kustomize_v3.10.0_linux_amd64.tar.gz sudo mv kustomize /usr/local/bin/kustomize 정상적으로 설치되었는지 확인합니다.  kustomize help 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nManages declarative configuration of Kubernetes. See https://sigs.k8s.io/kustomize Usage: kustomize [command] Available Commands: build Print configuration per contents of kustomization.yaml cfg Commands for reading and writing configuration. completion Generate shell completion script create Create a new kustomization in the current directory edit Edits a kustomization file fn Commands for running functions against configuration. ... CSI Plugin : Local Path Provisioner #  CSI Plugin은 kubernetes 내의 스토리지를 담당하는 모듈입니다. 단일 노드 클러스터에서 쉽게 사용할 수 있는 CSI Plugin인 Local Path Provisioner를 설치합니다.  kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.20/deploy/local-path-storage.yaml 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nnamespace/local-path-storage created serviceaccount/local-path-provisioner-service-account created clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created deployment.apps/local-path-provisioner created storageclass.storage.k8s.io/local-path created configmap/local-path-config created 또한, 다음과 같이 local-path-storage namespace 에 provisioner pod이 Running 인지 확인합니다.  kubectl -n local-path-storage get pod 정상적으로 수행되면 아래와 같이 출력됩니다.\nNAME READY STATUS RESTARTS AGE local-path-provisioner-d744ccf98-xfcbk 1/1 Running 0 7m 다음을 수행하여 default storage class로 변경합니다.  kubectl patch storageclass local-path -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; 정상적으로 수행되면 아래와 같이 출력됩니다.\nstorageclass.storage.k8s.io/local-path patched default storage class로 설정되었는지 확인합니다.  kubectl get sc 다음과 같이 NAME에 local-path (default) 인 storage class가 존재하는 것을 확인합니다.\nNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 2h "}),e.add({id:14,href:"/docs/setup-kubernetes/setup-nvidia-gpu/",title:"6. (Optional) Setup GPU",description:"Install nvidia docker, nvidia device plugin",content:"쿠버네티스 및 Kubeflow 등에서 GP 를 사용하기 위해서는 다음 작업이 필요합니다.\n1. Install NVIDIA Driver # nvidia-smi 수행 시 다음과 같은 화면이 출력된다면 이 단계는 생략해 주시기 바랍니다.\nmlops@ubuntu:~$ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.86 Driver Version: 470.86 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A | | 25% 32C P8 4W / 120W | 211MiB / 6078MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:02:00.0 Off | N/A | | 0% 34C P8 7W / 175W | 5MiB / 7982MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1644 G /usr/lib/xorg/Xorg 198MiB | | 0 N/A N/A 1893 G /usr/bin/gnome-shell 10MiB | | 1 N/A N/A 1644 G /usr/lib/xorg/Xorg 4MiB | +-----------------------------------------------------------------------------+ nvidia-smi의 출력 결과가 위와 같지 않다면 장착된 GPU에 맞는 nvidia driver를 설치해 주시기 바랍니다.\n만약 nvidia driver의 설치에 익숙하지 않다면 아래 명령어를 통해 설치하시기 바랍니다.\nsudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update \u0026amp;\u0026amp; sudo apt install -y ubuntu-drivers-common sudo ubuntu-drivers autoinstall sudo reboot 2. NVIDIA-Docker 설치 # NVIDIA-Docker를 설치합니다.\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\ sudo apt-key add - distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update sudo apt-get install -y nvidia-docker2 \u0026amp;\u0026amp; sudo systemctl restart docker 정상적으로 설치되었는지 확인하기 위해, GPU를 사용하는 도커 컨테이너를 실행해봅니다.\nsudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.86 Driver Version: 470.86 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A | | 25% 32C P8 4W / 120W | 211MiB / 6078MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:02:00.0 Off | N/A | | 0% 34C P8 6W / 175W | 5MiB / 7982MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ 3. NVIDIA-Docker를 Default Container Runtime으로 설정 # 쿠버네티스는 기본적으로 Docker-CE를 Default Container Runtime으로 사용합니다. 따라서, Docker Container 내에서 NVIDIA GPU를 사용하기 위해서는 NVIDIA-Docker 를 Container Runtime 으로 사용하여 pod를 생성할 수 있도록 Default Runtime을 수정해 주어야 합니다.\n /etc/docker/daemon.json 파일을 열어 다음과 같이 수정합니다.  sudo vi /etc/docker/daemon.json { \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } 파일이 변경된 것을 확인한 후, Docker를 재시작합니다.  sudo systemctl daemon-reload sudo service docker restart 변경 사항이 반영되었는지 확인합니다.  sudo docker info | grep nvidia 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ docker info | grep nvidia Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux nvidia runc Default Runtime: nvidia 4. Nvidia-Device-Plugin #  nvidia-device-plugin daemonset을 생성합니다.  kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.10.0/nvidia-device-plugin.yml nvidia-device-plugin pod이 RUNNING 상태로 생성되었는지 확인합니다.  kubectl get pod -n kube-system | grep nvidia 다음과 같은 결과가 출력되어야 합니다.\nkube-system nvidia-device-plugin-daemonset-nlqh2 1/1 Running 0 1h node 정보에 gpu가 사용가능하도록 설정되었는지 확인합니다.  kubectl get nodes \u0026#34;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\u0026#34; 다음과 같은 메시지가 보이면 정상적으로 설정된 것을 의미합니다.\n(모두의 MLOps 에서 실습을 진행한 클러스터는 2개의 GPU가 있어서 2가 출력됩니다. 본인의 클러스터의 GPU 개수와 맞는 숫자가 출력된다면 됩니다.)\nNAME GPU ubuntu 2 설정되지 않은 경우, GPU의 value가 \u0026lt;None\u0026gt; 으로 표시됩니다.\n"}),e.add({id:15,href:"/docs/setup-components/",title:"Setup Kubernetes Components",description:"",content:""}),e.add({id:16,href:"/docs/setup-components/install-components-kf/",title:"1. Kubeflow",description:"구성요소 설치 - Kubeflow",content:"설치 파일 준비 # Kubeflow v1.4.0 버전을 설치하기 위해서, 설치에 필요한 manifests 파일들을 준비합니다.\nkubeflow/manifests Repository 를 v1.4.0 태그로 깃 클론한 뒤, 해당 폴더로 이동합니다.\ngit clone -b v1.4.0 https://github.com/kubeflow/manifests.git cd manifests 각 구성 요소별 설치 # kubeflow/manifests Repository 에 각 구성 요소별 설치 커맨드가 적혀져 있지만, 설치하며 발생할 수 있는 이슈 혹은 정상적으로 설치되었는지 확인하는 방법이 적혀져 있지 않아 처음 설치하는 경우 어려움을 겪는 경우가 많습니다.\n따라서, 각 구성 요소별로 정상적으로 설치되었는지 확인하는 방법을 함께 작성합니다.\n또한, 본 문서에서는 모두의 MLOps 에서 다루지 않는 구성요소인 Knative, KFServing, MPI Operator 의 설치는 리소스의 효율적 사용을 위해 따로 설치하지 않습니다.\nCert-manager #  cert-manager 를 설치합니다.  kustomize build common/cert-manager/cert-manager/base | kubectl apply -f - 정상적으로 설치되면 다음과 같이 출력됩니다.\nnamespace/cert-manager created customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created serviceaccount/cert-manager created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager-webhook created role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created cert-manager namespace 의 3 개의 pod 가 모두 Running 이 될 때까지 기다립니다.\nkubectl get pod -n cert-manager 모두 Running 이 되면 다음과 비슷한 결과가 출력됩니다.\nNAME READY STATUS RESTARTS AGE cert-manager-7dd5854bb4-7nmpd 1/1 Running 0 2m10s cert-manager-cainjector-64c949654c-2scxr 1/1 Running 0 2m10s cert-manager-webhook-6b57b9b886-7q6g2 1/1 Running 0 2m10s kubeflow-issuer 를 설치합니다.  kustomize build common/cert-manager/kubeflow-issuer/base | kubectl apply -f - 정상적으로 설치되면 다음과 같이 출력됩니다.\nclusterissuer.cert-manager.io/kubeflow-self-signing-issuer created   cert-manager-webhook 이슈\ncert-manager-webhook deployment 가 Running 이 아닌 경우, 다음과 비슷한 에러가 발생하며 kubeflow-issuer가 설치되지 않을 수 있음에 주의하시기 바랍니다.\n해당 에러가 발생한 경우, cert-manager 의 3개의 pod 가 모두 Running 이 되는 것을 확인한 이후 다시 명령어를 수행하시기 바랍니다.\nError from server: error when retrieving current configuration of: Resource: \u0026#34;cert-manager.io/v1alpha2, Resource=clusterissuers\u0026#34;, GroupVersionKind: \u0026#34;cert-manager.io/v1alpha2, Kind=ClusterIssuer\u0026#34; Name: \u0026#34;kubeflow-self-signing-issuer\u0026#34;, Namespace: \u0026#34;\u0026#34; from server for: \u0026#34;STDIN\u0026#34;: conversion webhook for cert-manager.io/v1, Kind=ClusterIssuer failed: Post \u0026#34;https://cert-manager-webhook.cert-manager.svc:443/convert?timeout=30s\u0026#34;: dial tcp 10.101.177.157:443: connect: connection refused   Istio #  istio 관련 Custom Resource Definition(CRD) 를 설치합니다.  kustomize build common/istio-1-9/istio-crds/base | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\ncustomresourcedefinition.apiextensions.k8s.io/authorizationpolicies.security.istio.io created customresourcedefinition.apiextensions.k8s.io/destinationrules.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/envoyfilters.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/gateways.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/istiooperators.install.istio.io created customresourcedefinition.apiextensions.k8s.io/peerauthentications.security.istio.io created customresourcedefinition.apiextensions.k8s.io/requestauthentications.security.istio.io created customresourcedefinition.apiextensions.k8s.io/serviceentries.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/sidecars.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/virtualservices.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/workloadentries.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/workloadgroups.networking.istio.io created istio namespace 를 설치합니다.  kustomize build common/istio-1-9/istio-namespace/base | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nnamespace/istio-system created istio 를 설치합니다.  kustomize build common/istio-1-9/istio-install/base | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nserviceaccount/istio-ingressgateway-service-account created serviceaccount/istio-reader-service-account created serviceaccount/istiod-service-account created role.rbac.authorization.k8s.io/istio-ingressgateway-sds created role.rbac.authorization.k8s.io/istiod-istio-system created clusterrole.rbac.authorization.k8s.io/istio-reader-istio-system created clusterrole.rbac.authorization.k8s.io/istiod-istio-system created rolebinding.rbac.authorization.k8s.io/istio-ingressgateway-sds created rolebinding.rbac.authorization.k8s.io/istiod-istio-system created clusterrolebinding.rbac.authorization.k8s.io/istio-reader-istio-system created clusterrolebinding.rbac.authorization.k8s.io/istiod-istio-system created configmap/istio created configmap/istio-sidecar-injector created service/istio-ingressgateway created service/istiod created deployment.apps/istio-ingressgateway created deployment.apps/istiod created envoyfilter.networking.istio.io/metadata-exchange-1.8 created envoyfilter.networking.istio.io/metadata-exchange-1.9 created envoyfilter.networking.istio.io/stats-filter-1.8 created envoyfilter.networking.istio.io/stats-filter-1.9 created envoyfilter.networking.istio.io/tcp-metadata-exchange-1.8 created envoyfilter.networking.istio.io/tcp-metadata-exchange-1.9 created envoyfilter.networking.istio.io/tcp-stats-filter-1.8 created envoyfilter.networking.istio.io/tcp-stats-filter-1.9 created envoyfilter.networking.istio.io/x-forwarded-host created gateway.networking.istio.io/istio-ingressgateway created authorizationpolicy.security.istio.io/global-deny-all created authorizationpolicy.security.istio.io/istio-ingressgateway created mutatingwebhookconfiguration.admissionregistration.k8s.io/istio-sidecar-injector created validatingwebhookconfiguration.admissionregistration.k8s.io/istiod-istio-system created istio-system namespace 의 2 개의 pod 가 모두 Running 이 될 때까지 기다립니다.\nkubectl get po -n istio-system 모두 Running 이 되면 다음과 비슷한 결과가 출력됩니다.\nNAME READY STATUS RESTARTS AGE istio-ingressgateway-79b665c95-xm22l 1/1 Running 0 16s istiod-86457659bb-5h58w 1/1 Running 0 16s Dex # dex 를 설치합니다.\nkustomize build common/dex/overlays/istio | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nnamespace/auth created customresourcedefinition.apiextensions.k8s.io/authcodes.dex.coreos.com created serviceaccount/dex created clusterrole.rbac.authorization.k8s.io/dex created clusterrolebinding.rbac.authorization.k8s.io/dex created configmap/dex created secret/dex-oidc-client created service/dex created deployment.apps/dex created virtualservice.networking.istio.io/dex created auth namespace 의 1 개의 pod 가 모두 Running 이 될 때까지 기다립니다.\nkubectl get po -n auth 모두 Running 이 되면 다음과 비슷한 결과가 출력됩니다.\nNAME READY STATUS RESTARTS AGE dex-5ddf47d88d-458cs 1/1 Running 1 12s OIDC AuthService # OIDC AuthService 를 설치합니다.\nkustomize build common/oidc-authservice/base | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nconfigmap/oidc-authservice-parameters created secret/oidc-authservice-client created service/authservice created persistentvolumeclaim/authservice-pvc created statefulset.apps/authservice created envoyfilter.networking.istio.io/authn-filter created istio-system namespace 에 authservice-0 pod 가 Running 이 될 때까지 기다립니다.\nkubectl get po -n istio-system -w 모두 Running 이 되면 다음과 비슷한 결과가 출력됩니다.\nNAME READY STATUS RESTARTS AGE authservice-0 1/1 Running 0 14s istio-ingressgateway-79b665c95-xm22l 1/1 Running 0 2m37s istiod-86457659bb-5h58w 1/1 Running 0 2m37s Kubeflow Namespace # kubeflow namespace 를 생성합니다.\nkustomize build common/kubeflow-namespace/base | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nnamespace/kubeflow created kubeflow namespace 를 조회합니다.\nkubectl get ns kubeflow 정상적으로 생성되면 다음과 비슷한 결과가 출력됩니다.\nNAME STATUS AGE kubeflow Active 8s Kubeflow Roles # kubeflow-roles 를 설치합니다.\nkustomize build common/kubeflow-roles/base | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nclusterrole.rbac.authorization.k8s.io/kubeflow-admin created clusterrole.rbac.authorization.k8s.io/kubeflow-edit created clusterrole.rbac.authorization.k8s.io/kubeflow-kubernetes-admin created clusterrole.rbac.authorization.k8s.io/kubeflow-kubernetes-edit created clusterrole.rbac.authorization.k8s.io/kubeflow-kubernetes-view created clusterrole.rbac.authorization.k8s.io/kubeflow-view created 방금 생성한 kubeflow roles 를 조회합니다.\nkubectl get clusterrole | grep kubeflow 다음과 같이 총 6개의 clusterrole 이 출력됩니다.\nkubeflow-admin 2021-12-03T08:51:36Z kubeflow-edit 2021-12-03T08:51:36Z kubeflow-kubernetes-admin 2021-12-03T08:51:36Z kubeflow-kubernetes-edit 2021-12-03T08:51:36Z kubeflow-kubernetes-view 2021-12-03T08:51:36Z kubeflow-view 2021-12-03T08:51:36Z Kubeflow Istio Resources # kubeflow-istio-resources 를 설치합니다.\nkustomize build common/istio-1-9/kubeflow-istio-resources/base | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nclusterrole.rbac.authorization.k8s.io/kubeflow-istio-admin created clusterrole.rbac.authorization.k8s.io/kubeflow-istio-edit created clusterrole.rbac.authorization.k8s.io/kubeflow-istio-view created gateway.networking.istio.io/kubeflow-gateway created 방금 생성한 kubeflow roles 를 조회합니다.\nkubectl get clusterrole | grep kubeflow-istio 다음과 같이 총 3개의 clusterrole 이 출력됩니다.\nkubeflow-istio-admin 2021-12-03T08:53:17Z kubeflow-istio-edit 2021-12-03T08:53:17Z kubeflow-istio-view 2021-12-03T08:53:17Z Kubeflow namespace 에 gateway 가 정상적으로 설치되었는지 확인합니다.\nkubectl get gateway -n kubeflow 정상적으로 생성되면 다음과 비슷한 결과가 출력됩니다.\nNAME AGE kubeflow-gateway 31s Kubeflow Pipelines # kubeflow pipelines 를 설치합니다.\nkustomize build apps/pipeline/upstream/env/platform-agnostic-multi-user | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\ncustomresourcedefinition.apiextensions.k8s.io/clusterworkflowtemplates.argoproj.io created customresourcedefinition.apiextensions.k8s.io/cronworkflows.argoproj.io created customresourcedefinition.apiextensions.k8s.io/workfloweventbindings.argoproj.io created ...(생략) authorizationpolicy.security.istio.io/ml-pipeline-visualizationserver created authorizationpolicy.security.istio.io/mysql created authorizationpolicy.security.istio.io/service-cache-server created 위 명령어는 여러 resources 를 한 번에 설치하고 있지만, 설치 순서의 의존성이 있는 리소스가 존재합니다.\n따라서 때에 따라 다음과 비슷한 에러가 발생할 수 있습니다.\n\u0026#34;error: unable to recognize \u0026#34;STDIN\u0026#34;: no matches for kind \u0026#34;CompositeController\u0026#34; in version \u0026#34;metacontroller.k8s.io/v1alpha1\u0026#34;\u0026#34; 위와 비슷한 에러가 발생한다면, 10 초 정도 기다린 뒤 다시 위의 명령을 수행합니다.\nkustomize build apps/pipeline/upstream/env/platform-agnostic-multi-user | kubectl apply -f - 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow 다음과 같이 총 16개의 pod 가 모두 Running 이 될 때까지 기다립니다.\nNAME READY STATUS RESTARTS AGE cache-deployer-deployment-79fdf9c5c9-bjnbg 2/2 Running 1 5m3s cache-server-5bdf4f4457-48gbp 2/2 Running 0 5m3s kubeflow-pipelines-profile-controller-7b947f4748-8d26b 1/1 Running 0 5m3s metacontroller-0 1/1 Running 0 5m3s metadata-envoy-deployment-5b4856dd5-xtlkd 1/1 Running 0 5m3s metadata-grpc-deployment-6b5685488-kwvv7 2/2 Running 3 5m3s metadata-writer-548bd879bb-zjkcn 2/2 Running 1 5m3s minio-5b65df66c9-k5gzg 2/2 Running 0 5m3s ml-pipeline-8c4b99589-85jw6 2/2 Running 1 5m3s ml-pipeline-persistenceagent-d6bdc77bd-ssxrv 2/2 Running 0 5m3s ml-pipeline-scheduledworkflow-5db54d75c5-zk2cw 2/2 Running 0 5m2s ml-pipeline-ui-5bd8d6dc84-j7wqr 2/2 Running 0 5m2s ml-pipeline-viewer-crd-68fb5f4d58-mbcbg 2/2 Running 1 5m2s ml-pipeline-visualizationserver-8476b5c645-wljfm 2/2 Running 0 5m2s mysql-f7b9b7dd4-xfnw4 2/2 Running 0 5m2s workflow-controller-5cbbb49bd8-5zrwx 2/2 Running 1 5m2s 추가로 ml-pipeline UI가 정상적으로 접속되는지 확인합니다.\nkubectl port-forward svc/ml-pipeline-ui -n kubeflow 8888:80 웹 브라우저를 열어 http://localhost:8888/#/pipelines/ 경로에 접속합니다.\n다음과 같은 화면이 출력되는 것을 확인합니다.\n localhost 연결 거부 이슈  만약 다음과 같이 localhost에서 연결을 거부했습니다 라는 에러가 출력될 경우, 커맨드로 address 설정을 통해 접근하는 것이 가능합니다.\n보안상의 문제가 되지 않는다면, 아래와 같이 0.0.0.0 로 모든 주소의 bind를 열어주는 방향으로 ml-pipeline UI가 정상적으로 접속되는지 확인합니다.\nkubectl port-forward --address 0.0.0.0 svc/ml-pipeline-ui -n kubeflow 8888:80  위의 옵션으로 실행했음에도 여전히 연결 거부 이슈가 발생할 경우  방화벽 설정으로 접속해 모든 tcp 프로토콜의 포트에 대한 접속을 허가 또는 8888번 포트의 접속 허가를 추가해 접근 권한을 허가해줍니다.\n웹 브라우저를 열어 http://\u0026lt;당신의 가상 인스턴스 공인 ip 주소\u0026gt;:8888/#/pipelines/ 경로에 접속하면, ml-pipeline UI 화면이 출력되는 것을 확인할 수 있습니다.\n하단에서 진행되는 다른 포트의 경로에 접속할 때도 위의 절차와 동일하게 커맨드를 실행하고, 방화벽에 포트 번호를 추가해주면 실행하는 것이 가능합니다.\nKatib # Katib 를 설치합니다.\nkustomize build apps/katib/upstream/installs/katib-with-kubeflow | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\ncustomresourcedefinition.apiextensions.k8s.io/experiments.kubeflow.org created customresourcedefinition.apiextensions.k8s.io/suggestions.kubeflow.org created customresourcedefinition.apiextensions.k8s.io/trials.kubeflow.org created serviceaccount/katib-controller created serviceaccount/katib-ui created clusterrole.rbac.authorization.k8s.io/katib-controller created clusterrole.rbac.authorization.k8s.io/katib-ui created clusterrole.rbac.authorization.k8s.io/kubeflow-katib-admin created clusterrole.rbac.authorization.k8s.io/kubeflow-katib-edit created clusterrole.rbac.authorization.k8s.io/kubeflow-katib-view created clusterrolebinding.rbac.authorization.k8s.io/katib-controller created clusterrolebinding.rbac.authorization.k8s.io/katib-ui created configmap/katib-config created configmap/trial-templates created secret/katib-mysql-secrets created service/katib-controller created service/katib-db-manager created service/katib-mysql created service/katib-ui created persistentvolumeclaim/katib-mysql created deployment.apps/katib-controller created deployment.apps/katib-db-manager created deployment.apps/katib-mysql created deployment.apps/katib-ui created certificate.cert-manager.io/katib-webhook-cert created issuer.cert-manager.io/katib-selfsigned-issuer created virtualservice.networking.istio.io/katib-ui created mutatingwebhookconfiguration.admissionregistration.k8s.io/katib.kubeflow.org created validatingwebhookconfiguration.admissionregistration.k8s.io/katib.kubeflow.org created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep katib 다음과 같이 총 4 개의 pod 가 Running 이 될 때까지 기다립니다.\nkatib-controller-68c47fbf8b-b985z 1/1 Running 0 82s katib-db-manager-6c948b6b76-2d9gr 1/1 Running 0 82s katib-mysql-7894994f88-scs62 1/1 Running 0 82s katib-ui-64bb96d5bf-d89kp 1/1 Running 0 82s 추가로 katib UI가 정상적으로 접속되는지 확인합니다.\nkubectl port-forward svc/katib-ui -n kubeflow 8081:80 웹 브라우저를 열어 http://localhost:8081/katib/ 경로에 접속합니다.\n다음과 같은 화면이 출력되는 것을 확인합니다.\nCentral Dashboard # Dashboard 를 설치합니다.\nkustomize build apps/centraldashboard/upstream/overlays/istio | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nserviceaccount/centraldashboard created role.rbac.authorization.k8s.io/centraldashboard created clusterrole.rbac.authorization.k8s.io/centraldashboard created rolebinding.rbac.authorization.k8s.io/centraldashboard created clusterrolebinding.rbac.authorization.k8s.io/centraldashboard created configmap/centraldashboard-config created configmap/centraldashboard-parameters created service/centraldashboard created deployment.apps/centraldashboard created virtualservice.networking.istio.io/centraldashboard created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep centraldashboard kubeflow namespace 에 centraldashboard 관련 1 개의 pod 가 Running 이 될 때까지 기다립니다.\ncentraldashboard-8fc7d8cc-xl7ts 1/1 Running 0 52s 추가로 Central Dashboard UI가 정상적으로 접속되는지 확인합니다.\nkubectl port-forward svc/centraldashboard -n kubeflow 8082:80 웹 브라우저를 열어 http://localhost:8082/ 경로에 접속합니다.\n다음과 같은 화면이 출력되는 것을 확인합니다.\nAdmission Webhook # kustomize build apps/admission-webhook/upstream/overlays/cert-manager | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\ncustomresourcedefinition.apiextensions.k8s.io/poddefaults.kubeflow.org created serviceaccount/admission-webhook-service-account created clusterrole.rbac.authorization.k8s.io/admission-webhook-cluster-role created clusterrole.rbac.authorization.k8s.io/admission-webhook-kubeflow-poddefaults-admin created clusterrole.rbac.authorization.k8s.io/admission-webhook-kubeflow-poddefaults-edit created clusterrole.rbac.authorization.k8s.io/admission-webhook-kubeflow-poddefaults-view created clusterrolebinding.rbac.authorization.k8s.io/admission-webhook-cluster-role-binding created service/admission-webhook-service created deployment.apps/admission-webhook-deployment created certificate.cert-manager.io/admission-webhook-cert created issuer.cert-manager.io/admission-webhook-selfsigned-issuer created mutatingwebhookconfiguration.admissionregistration.k8s.io/admission-webhook-mutating-webhook-configuration created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep admission-webhook 1 개의 pod 가 Running 이 될 때까지 기다립니다.\nadmission-webhook-deployment-667bd68d94-2hhrx 1/1 Running 0 11s Notebooks \u0026amp; Jupyter Web App #  Notebook controller 를 설치합니다.  kustomize build apps/jupyter/notebook-controller/upstream/overlays/kubeflow | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\ncustomresourcedefinition.apiextensions.k8s.io/notebooks.kubeflow.org created serviceaccount/notebook-controller-service-account created role.rbac.authorization.k8s.io/notebook-controller-leader-election-role created clusterrole.rbac.authorization.k8s.io/notebook-controller-kubeflow-notebooks-admin created clusterrole.rbac.authorization.k8s.io/notebook-controller-kubeflow-notebooks-edit created clusterrole.rbac.authorization.k8s.io/notebook-controller-kubeflow-notebooks-view created clusterrole.rbac.authorization.k8s.io/notebook-controller-role created rolebinding.rbac.authorization.k8s.io/notebook-controller-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/notebook-controller-role-binding created configmap/notebook-controller-config-m44cmb547t created service/notebook-controller-service created deployment.apps/notebook-controller-deployment created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep notebook-controller 1 개의 pod 가 Running 이 될 때까지 기다립니다.\nnotebook-controller-deployment-75b4f7b578-w4d4l 1/1 Running 0 105s Jupyter Web App 을 설치합니다.  kustomize build apps/jupyter/jupyter-web-app/upstream/overlays/istio | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nserviceaccount/jupyter-web-app-service-account created role.rbac.authorization.k8s.io/jupyter-web-app-jupyter-notebook-role created clusterrole.rbac.authorization.k8s.io/jupyter-web-app-cluster-role created clusterrole.rbac.authorization.k8s.io/jupyter-web-app-kubeflow-notebook-ui-admin created clusterrole.rbac.authorization.k8s.io/jupyter-web-app-kubeflow-notebook-ui-edit created clusterrole.rbac.authorization.k8s.io/jupyter-web-app-kubeflow-notebook-ui-view created rolebinding.rbac.authorization.k8s.io/jupyter-web-app-jupyter-notebook-role-binding created clusterrolebinding.rbac.authorization.k8s.io/jupyter-web-app-cluster-role-binding created configmap/jupyter-web-app-config-76844k4cd7 created configmap/jupyter-web-app-logos created configmap/jupyter-web-app-parameters-chmg88cm48 created service/jupyter-web-app-service created deployment.apps/jupyter-web-app-deployment created virtualservice.networking.istio.io/jupyter-web-app-jupyter-web-app created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep jupyter-web-app 1개의 pod 가 Running 이 될 때까지 기다립니다.\njupyter-web-app-deployment-6f744fbc54-p27ts 1/1 Running 0 2m Profiles + KFAM # Profile Controller를 설치합니다.\nkustomize build apps/profiles/upstream/overlays/kubeflow | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\ncustomresourcedefinition.apiextensions.k8s.io/profiles.kubeflow.org created serviceaccount/profiles-controller-service-account created role.rbac.authorization.k8s.io/profiles-leader-election-role created rolebinding.rbac.authorization.k8s.io/profiles-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/profiles-cluster-role-binding created configmap/namespace-labels-data-48h7kd55mc created configmap/profiles-config-46c7tgh6fd created service/profiles-kfam created deployment.apps/profiles-deployment created virtualservice.networking.istio.io/profiles-kfam created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep profiles-deployment 1 개의 pod 가 Running 이 될 때까지 기다립니다.\nprofiles-deployment-89f7d88b-qsnrd 2/2 Running 0 42s Volumes Web App # Volumes Web App 을 설치합니다.\nkustomize build apps/volumes-web-app/upstream/overlays/istio | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nserviceaccount/volumes-web-app-service-account created clusterrole.rbac.authorization.k8s.io/volumes-web-app-cluster-role created clusterrole.rbac.authorization.k8s.io/volumes-web-app-kubeflow-volume-ui-admin created clusterrole.rbac.authorization.k8s.io/volumes-web-app-kubeflow-volume-ui-edit created clusterrole.rbac.authorization.k8s.io/volumes-web-app-kubeflow-volume-ui-view created clusterrolebinding.rbac.authorization.k8s.io/volumes-web-app-cluster-role-binding created configmap/volumes-web-app-parameters-4gg8cm2gmk created service/volumes-web-app-service created deployment.apps/volumes-web-app-deployment created virtualservice.networking.istio.io/volumes-web-app-volumes-web-app created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep volumes-web-app 1개의 pod가 Running 이 될 때까지 기다립니다.\nvolumes-web-app-deployment-8589d664cc-62svl 1/1 Running 0 27s Tensorboard \u0026amp; Tensorboard Web App #  Tensorboard Web App 를 설치합니다.  kustomize build apps/tensorboard/tensorboards-web-app/upstream/overlays/istio | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nserviceaccount/tensorboards-web-app-service-account created clusterrole.rbac.authorization.k8s.io/tensorboards-web-app-cluster-role created clusterrole.rbac.authorization.k8s.io/tensorboards-web-app-kubeflow-tensorboard-ui-admin created clusterrole.rbac.authorization.k8s.io/tensorboards-web-app-kubeflow-tensorboard-ui-edit created clusterrole.rbac.authorization.k8s.io/tensorboards-web-app-kubeflow-tensorboard-ui-view created clusterrolebinding.rbac.authorization.k8s.io/tensorboards-web-app-cluster-role-binding created configmap/tensorboards-web-app-parameters-g28fbd6cch created service/tensorboards-web-app-service created deployment.apps/tensorboards-web-app-deployment created virtualservice.networking.istio.io/tensorboards-web-app-tensorboards-web-app created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep tensorboards-web-app 1 개의 pod 가 Running 이 될 때까지 기다립니다.\ntensorboards-web-app-deployment-6ff79b7f44-qbzmw 1/1 Running 0 22s Tensorboard Controller 를 설치합니다.  kustomize build apps/tensorboard/tensorboard-controller/upstream/overlays/kubeflow | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\ncustomresourcedefinition.apiextensions.k8s.io/tensorboards.tensorboard.kubeflow.org created serviceaccount/tensorboard-controller created role.rbac.authorization.k8s.io/tensorboard-controller-leader-election-role created clusterrole.rbac.authorization.k8s.io/tensorboard-controller-manager-role created clusterrole.rbac.authorization.k8s.io/tensorboard-controller-proxy-role created rolebinding.rbac.authorization.k8s.io/tensorboard-controller-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/tensorboard-controller-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/tensorboard-controller-proxy-rolebinding created configmap/tensorboard-controller-config-bf88mm96c8 created service/tensorboard-controller-controller-manager-metrics-service created deployment.apps/tensorboard-controller-controller-manager created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep tensorboard-controller 1 개의 pod 가 Running 이 될 때까지 기다립니다.\ntensorboard-controller-controller-manager-954b7c544-vjpzj 3/3 Running 1 73s Training Operator # Training Operator 를 설치합니다.\nkustomize build apps/training-operator/upstream/overlays/kubeflow | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\ncustomresourcedefinition.apiextensions.k8s.io/mxjobs.kubeflow.org created customresourcedefinition.apiextensions.k8s.io/pytorchjobs.kubeflow.org created customresourcedefinition.apiextensions.k8s.io/tfjobs.kubeflow.org created customresourcedefinition.apiextensions.k8s.io/xgboostjobs.kubeflow.org created serviceaccount/training-operator created clusterrole.rbac.authorization.k8s.io/kubeflow-training-admin created clusterrole.rbac.authorization.k8s.io/kubeflow-training-edit created clusterrole.rbac.authorization.k8s.io/kubeflow-training-view created clusterrole.rbac.authorization.k8s.io/training-operator created clusterrolebinding.rbac.authorization.k8s.io/training-operator created service/training-operator created deployment.apps/training-operator created 정상적으로 설치되었는지 확인합니다.\nkubectl get po -n kubeflow | grep training-operator 1 개의 pod 가 Running 이 될 때까지 기다립니다.\ntraining-operator-7d98f9dd88-6887f 1/1 Running 0 28s User Namespace # Kubeflow 사용을 위해, 사용할 User의 Kubeflow Profile 을 생성합니다.\nkustomize build common/user-namespace/base | kubectl apply -f - 정상적으로 수행되면 다음과 같이 출력됩니다.\nconfigmap/default-install-config-9h2h2b6hbk created profile.kubeflow.org/kubeflow-user-example-com created kubeflow-user-example-com profile 이 생성된 것을 확인합니다.\nkubectl get profile kubeflow-user-example-com 37s 정상 설치 확인 # Kubeflow central dashboard에 web browser로 접속하기 위해 포트 포워딩합니다.\nkubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 Web Browser 를 열어 http://localhost:8080 으로 접속하여, 다음과 같은 화면이 출력되는 것을 확인합니다.\n다음 접속 정보를 입력하여 접속합니다.\n Email Address: user@example.com Password: 12341234  "}),e.add({id:17,href:"/docs/setup-components/install-components-mlflow/",title:"2. MLflow Tracking Server",description:"구성요소 설치 - MLflow",content:"Install MLflow Tracking Server # MLflow는 대표적인 오픈소스 ML 실험 관리 도구입니다. MLflow는 실험 관리 용도 외에도 ML Model 패키징, ML 모델 배포 관리, ML 모델 저장과 같은 기능도 제공하고 있습니다.\n모두의 MLOps에서는 MLflow를 실험 관리 용도로 사용합니다.\n그래서 MLflow에서 관리하는 데이터를 저장하고 UI를 제공하는 MLflow Tracking Server를 쿠버네티스 클러스터에 배포하여 사용할 예정입니다.\nBefore Install MLflow Tracking Server # PostgreSQL DB 설치 # MLflow Tracking Server가 Backend Store로 사용할 용도의 PostgreSQL DB를 쿠버네티스 클러스터에 배포합니다.\n먼저 mlflow-system이라는 namespace 를 생성합니다.\nkubectl create ns mlflow-system 다음과 같은 메시지가 출력되면 정상적으로 생성된 것을 의미합니다.\nnamespace/mlflow-system created postgresql DB를 mlflow-system namespace 에 생성합니다.\nkubectl -n mlflow-system apply -f https://raw.githubusercontent.com/mlops-for-all/helm-charts/b94b5fe4133f769c04b25068b98ccfa7a505aa60/mlflow/manifests/postgres.yaml 정상적으로 수행되면 다음과 같이 출력됩니다.\nservice/postgresql-mlflow-service created deployment.apps/postgresql-mlflow created persistentvolumeclaim/postgresql-mlflow-pvc created mlflow-system namespace 에 1개의 postgresql 관련 pod 가 Running 이 될 때까지 기다립니다.\nkubectl get pod -n mlflow-system | grep postgresql 다음과 비슷하게 출력되면 정상적으로 실행된 것입니다.\npostgresql-mlflow-7b9bc8c79f-srkh7 1/1 Running 0 38s Minio 설정 # MLflow Tracking Server가 Artifacts Store로 사용할 용도의 Minio는 이전 Kubeflow 설치 단계에서 설치한 Minio를 활용합니다.\n단, kubeflow 용도와 mlflow 용도를 분리하기 위해, mlflow 전용 버킷(bucket)을 생성하겠습니다.\nminio 에 접속하여 버킷을 생성하기 위해, 우선 minio-service 를 포트포워딩합니다.\nkubectl port-forward svc/minio-service -n kubeflow 9000:9000 웹 브라우저를 열어 localhost:9000으로 접속하면 다음과 같은 화면이 출력됩니다.\n다음과 같은 접속 정보를 입력하여 로그인합니다.\n Username: minio Password: minio123  우측 하단의 + 버튼을 클릭하여, Create Bucket를 클릭합니다.\nBucket Name에 mlflow를 입력하여 버킷을 생성합니다.\n정상적으로 생성되면 다음과 같이 왼쪽에 mlflow라는 이름의 버킷이 생성됩니다.\n Let\u0026rsquo;s Install MLflow Tracking Server # Helm Repository 추가 # helm repo add mlops-for-all https://mlops-for-all.github.io/helm-charts 다음과 같은 메시지가 출력되면 정상적으로 추가된 것을 의미합니다.\n\u0026#34;mlops-for-all\u0026#34; has been added to your repositories Helm Repository 업데이트 # helm repo update 다음과 같은 메시지가 출력되면 정상적으로 업데이트된 것을 의미합니다.\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;mlops-for-all\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ Helm Install # mlflow-server Helm Chart 0.2.0 버전을 설치합니다.\nhelm install mlflow-server mlops-for-all/mlflow-server \\ --namespace mlflow-system \\ --version 0.2.0  주의: 위의 helm chart는 MLflow 의 backend store 와 artifacts store 의 접속 정보를 kubeflow 설치 과정에서 생성한 minio와 위의 PostgreSQL DB 설치에서 생성한 postgresql 정보를 default로 하여 설치합니다.  별개로 생성한 DB 혹은 Object storage를 활용하고 싶은 경우, Helm Chart Repo를 참고하여 helm install 시 value를 따로 설정하여 설치하시기 바랍니다.    다음과 같은 메시지가 출력되어야 합니다.\nNAME: mlflow-server LAST DEPLOYED: Sat Dec 18 22:02:13 2021 NAMESPACE: mlflow-system STATUS: deployed REVISION: 1 TEST SUITE: None 정상적으로 설치되었는지 확인합니다.\nkubectl get pod -n mlflow-system | grep mlflow-server mlflow-system namespace 에 1 개의 mlflow-server 관련 pod 가 Running 이 될 때까지 기다립니다.\n다음과 비슷하게 출력되면 정상적으로 실행된 것입니다.\nmlflow-server-ffd66d858-6hm62 1/1 Running 0 74s 정상 설치 확인 # 그럼 이제 MLflow Server에 정상적으로 접속되는지 확인해보겠습니다.\n우선 클라이언트 노드에서 접속하기 위해, 포트포워딩을 수행합니다.\nkubectl port-forward svc/mlflow-server-service -n mlflow-system 5000:5000 웹 브라우저를 열어 localhost:5000으로 접속하면 다음과 같은 화면이 출력됩니다.\n"}),e.add({id:18,href:"/docs/setup-components/install-components-seldon/",title:"3. Seldon-Core",description:"구성요소 설치 - Seldon-Core",content:"Seldon-Core # Seldon-Core는 쿠버네티스 환경에 수많은 머신러닝 모델을 배포하고 관리할 수 있는 오픈소스 프레임워크 중 하나입니다.\n더 자세한 내용은 Seldon-Core 의 공식 제품 설명 페이지 와 깃헙 그리고 API Deployment 파트를 참고해주시기를 바랍니다.\nSelon-Core 설치 # Seldon-Core를 사용하기 위해서는 쿠버네티스의 인그레스(Ingress)를 담당하는 Ambassador 와 Istio 와 같은 모듈이 필요합니다.\nSeldon-Core 에서는 Ambassador 와 Istio 만을 공식적으로 지원하며, 모두의 MLOps에서는 Ambassador를 사용해 Seldon-core를 사용하므로 Ambassador를 설치하겠습니다.\nAmbassador - Helm Repository 추가 # helm repo add datawire https://www.getambassador.io 다음과 같은 메시지가 출력되면 정상적으로 추가된 것을 의미합니다.\n\u0026#34;datawire\u0026#34; has been added to your repositories Ambassador - Helm Repository 업데이트 # helm repo update 다음과 같은 메시지가 출력되면 정상적으로 업데이트된 것을 의미합니다.\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;datawire\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ Ambassador - Helm Install # ambassador Chart 6.9.3 버전을 설치합니다.\nhelm install ambassador datawire/ambassador \\ --namespace seldon-system \\ --create-namespace \\ --set image.repository=quay.io/datawire/ambassador \\ --set enableAES=false \\ --set crds.keep=false \\ --version 6.9.3 다음과 같은 메시지가 출력되어야 합니다.\n생략... W1206 17:01:36.026326 26635 warnings.go:70] rbac.authorization.k8s.io/v1beta1 Role is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 Role W1206 17:01:36.029764 26635 warnings.go:70] rbac.authorization.k8s.io/v1beta1 RoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 RoleBinding NAME: ambassador LAST DEPLOYED: Mon Dec 6 17:01:34 2021 NAMESPACE: seldon-system STATUS: deployed REVISION: 1 NOTES: ------------------------------------------------------------------------------- Congratulations! You\u0026#39;ve successfully installed Ambassador! ------------------------------------------------------------------------------- To get the IP address of Ambassador, run the following commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running \u0026#39;kubectl get svc -w --namespace seldon-system ambassador\u0026#39; On GKE/Azure: export SERVICE_IP=$(kubectl get svc --namespace seldon-system ambassador -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) On AWS: export SERVICE_IP=$(kubectl get svc --namespace seldon-system ambassador -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) echo http://$SERVICE_IP: For help, visit our Slack at http://a8r.io/Slack or view the documentation online at https://www.getambassador.io. seldon-system 에 4 개의 pod 가 Running 이 될 때까지 기다립니다.\nkubectl get pod -n seldon-system ambassador-7f596c8b57-4s9xh 1/1 Running 0 7m15s ambassador-7f596c8b57-dt6lr 1/1 Running 0 7m15s ambassador-7f596c8b57-h5l6f 1/1 Running 0 7m15s ambassador-agent-77bccdfcd5-d5jxj 1/1 Running 0 7m15s Seldon-Core - Helm Install # seldon-core-operator Chart 1.11.2 버전을 설치합니다.\nhelm install seldon-core seldon-core-operator \\ --repo https://storage.googleapis.com/seldon-charts \\ --namespace seldon-system \\ --set usageMetrics.enabled=true \\ --set ambassador.enabled=true \\ --version 1.11.2 다음과 같은 메시지가 출력되어야 합니다.\n생략... W1206 17:05:38.336391 28181 warnings.go:70] admissionregistration.k8s.io/v1beta1 ValidatingWebhookConfiguration is deprecated in v1.16+, unavailable in v1.22+; use admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration NAME: seldon-core LAST DEPLOYED: Mon Dec 6 17:05:34 2021 NAMESPACE: seldon-system STATUS: deployed REVISION: 1 TEST SUITE: None seldon-system namespace 에 1 개의 seldon-controller-manager pod 가 Running 이 될 때까지 기다립니다.\nkubectl get pod -n seldon-system | grep seldon-controller seldon-controller-manager-8457b8b5c7-r2frm 1/1 Running 0 2m22s References #  Example Model Servers with Seldon  "}),e.add({id:19,href:"/docs/setup-components/install-components-pg/",title:"4. Prometheus \u0026 Grafana",description:"구성요소 설치 - Prometheus \u0026 Grafana",content:"Prometheus \u0026amp; Grafana # 프로메테우스(Prometheus) 와 그라파나(Grafana) 는 모니터링을 위한 도구입니다.\n안정적인 서비스 운영을 위해서는 서비스와 서비스가 운영되고 있는 인프라의 상태를 지속해서 관찰하고, 관찰한 메트릭을 바탕으로 문제가 생길 때 빠르게 대응해야 합니다.\n이러한 모니터링을 효율적으로 수행하기 위한 많은 도구 중 모두의 MLOps에서는 오픈소스인 프로메테우스와 그라파나를 사용할 예정입니다.\n더 자세한 내용은 Prometheus 공식 문서, Grafana 공식 문서를 확인해주시기를 바랍니다.\n프로메테우스는 다양한 대상으로부터 Metric을 수집하는 도구이며, 그라파나는 모인 데이터를 시각화하는 것을 도와주는 도구입니다. 서로 간의 종속성은 없지만 상호 보완적으로 사용할 수 있어 함께 사용되는 경우가 많습니다.\n이번 페이지에서는 쿠버네티스 클러스터에 프로메테우스와 그라파나를 설치한 뒤, Seldon-Core 로 생성한 SeldonDeployment 로 API 요청을 보내, 정상적으로 Metrics 이 수집되는지 확인해보겠습니다.\n본 글에서는 seldonio/seldon-core-analytics Helm Chart 1.12.0 버전을 활용해 쿠버네티스 클러스터에 프로메테우스와 그라파나를 설치하고, Seldon-Core 에서 생성한 SeldonDeployment의 Metrics 을 효율적으로 확인하기 위한 대시보드도 함께 설치합니다.\nHelm Repository 추가 # helm repo add seldonio https://storage.googleapis.com/seldon-charts 다음과 같은 메시지가 출력되면 정상적으로 추가된 것을 의미합니다.\n\u0026#34;seldonio\u0026#34; has been added to your repositories Helm Repository 업데이트 # helm repo update 다음과 같은 메시지가 출력되면 정상적으로 업데이트된 것을 의미합니다.\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;seldonio\u0026#34; chart repository ...Successfully got an update from the \u0026#34;datawire\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ Helm Install # seldon-core-analytics Helm Chart 1.12.0 버전을 설치합니다.\nhelm install seldon-core-analytics seldonio/seldon-core-analytics \\ --namespace seldon-system \\ --version 1.12.0 다음과 같은 메시지가 출력되어야 합니다.\n생략... NAME: seldon-core-analytics LAST DEPLOYED: Tue Dec 14 18:29:38 2021 NAMESPACE: seldon-system STATUS: deployed REVISION: 1 정상적으로 설치되었는지 확인합니다.\nkubectl get pod -n seldon-system | grep seldon-core-analytics seldon-system namespace 에 6개의 seldon-core-analytics 관련 pod 가 Running 이 될 때까지 기다립니다.\nseldon-core-analytics-grafana-657c956c88-ng8wn 2/2 Running 0 114s seldon-core-analytics-kube-state-metrics-94bb6cb9-svs82 1/1 Running 0 114s seldon-core-analytics-prometheus-alertmanager-64cf7b8f5-nxbl8 2/2 Running 0 114s seldon-core-analytics-prometheus-node-exporter-5rrj5 1/1 Running 0 114s seldon-core-analytics-prometheus-pushgateway-8476474cff-sr4n6 1/1 Running 0 114s seldon-core-analytics-prometheus-seldon-685c664894-7cr45 2/2 Running 0 114s 정상 설치 확인 # 그럼 이제 그라파나에 정상적으로 접속되는지 확인해보겠습니다.\n우선 클라이언트 노드에서 접속하기 위해, 포트포워딩을 수행합니다.\nkubectl port-forward svc/seldon-core-analytics-grafana -n seldon-system 8090:80 웹 브라우저를 열어 localhost:8090으로 접속하면 다음과 같은 화면이 출력됩니다.\n다음과 같은 접속정보를 입력하여 접속합니다.\n Email or username : admin Password : password  로그인하면 다음과 같은 화면이 출력됩니다.\n좌측의 대시보드 아이콘을 클릭하여, Manage 버튼을 클릭합니다.\n기본적인 그라파나 대시보드가 포함되어있는 것을 확인할 수 있습니다. 이 중 Prediction Analytics 대시보드를 클릭합니다.\nSeldon Core API Dashboard 가 보이고, 다음과 같이 출력되는 것을 확인할 수 있습니다.\nReferences #  Seldon-Core-Analytics Helm Chart  "}),e.add({id:20,href:"/docs/kubeflow-dashboard-guide/",title:"Kubeflow UI Guide",description:"How to use Kubeflow Dashboard.",content:""}),e.add({id:21,href:"/docs/kubeflow-dashboard-guide/intro/",title:"1. Central Dashboard",description:"Kubeflow 설치를 완료하면, 다음 커맨드를 통해 대시보드에 접속할 수 있습니다.\nkubectl port-forward --address 0.0.0.0 svc/istio-ingressgateway -n istio-system 8080:80 Central Dashboard는 Kubeflow에서 제공하는 모든 기능을 통합하여 제공하는 UI입니다. Central Dashboard에서 제공하는 기능은 크게 왼쪽의 탭을 기준으로 구분할 수 있습니다.\n Home Notebooks Tensorboards Volumes Models Experiments(AutoML) Experiments(KFP) Pipelines Runs Recurring Runs Artifacts Executions  그럼 이제 기능별 간단한 사용법을 알아보겠습니다.",content:"Kubeflow 설치를 완료하면, 다음 커맨드를 통해 대시보드에 접속할 수 있습니다.\nkubectl port-forward --address 0.0.0.0 svc/istio-ingressgateway -n istio-system 8080:80 Central Dashboard는 Kubeflow에서 제공하는 모든 기능을 통합하여 제공하는 UI입니다. Central Dashboard에서 제공하는 기능은 크게 왼쪽의 탭을 기준으로 구분할 수 있습니다.\n Home Notebooks Tensorboards Volumes Models Experiments(AutoML) Experiments(KFP) Pipelines Runs Recurring Runs Artifacts Executions  그럼 이제 기능별 간단한 사용법을 알아보겠습니다.\n"}),e.add({id:22,href:"/docs/kubeflow-dashboard-guide/notebooks/",title:"2. Notebooks",description:"노트북 서버(Notebook Server) 생성하기 # 다음 Central Dashboard의 왼쪽 탭의 Notebooks를 클릭해보겠습니다.\n다음과 같은 화면을 볼 수 있습니다.\nNotebooks 탭은 JupyterHub와 비슷하게 유저별로 jupyter notebook 및 code server 환경(이하 노트북 서버)을 독립적으로 생성하고 접속할 수 있는 페이지입니다.\n오른쪽 위의 + NEW NOTEBOOK 버튼을 클릭합니다.\n아래와 같은 화면이 나타나면, 이제 생성할 노트북 서버의 스펙(Spec)을 명시하여 생성합니다.\n 각 스펙에 대한 자세한 내용은 아래와 같습니다.  name:  노트북 서버를 구분할 수 있는 이름으로 생성합니다.",content:"노트북 서버(Notebook Server) 생성하기 # 다음 Central Dashboard의 왼쪽 탭의 Notebooks를 클릭해보겠습니다.\n다음과 같은 화면을 볼 수 있습니다.\nNotebooks 탭은 JupyterHub와 비슷하게 유저별로 jupyter notebook 및 code server 환경(이하 노트북 서버)을 독립적으로 생성하고 접속할 수 있는 페이지입니다.\n오른쪽 위의 + NEW NOTEBOOK 버튼을 클릭합니다.\n아래와 같은 화면이 나타나면, 이제 생성할 노트북 서버의 스펙(Spec)을 명시하여 생성합니다.\n 각 스펙에 대한 자세한 내용은 아래와 같습니다.  name:  노트북 서버를 구분할 수 있는 이름으로 생성합니다.   namespace :  따로 변경할 수 없습니다. (현재 로그인한 user 계정의 namespace이 자동으로 지정되어 있습니다.)   Image:  sklearn, pytorch, tensorflow 등의 파이썬 패키지가 미리 설치된 jupyter lab 이미지 중 사용할 이미지를 선택합니다.  노트북 서버 내에서 GPU를 사용하여 tensorflow-cuda, pytorch-cuda 등의 이미지를 사용하는 경우, 하단의 GPUs 부분을 확인하시기 바랍니다.   추가적인 패키지나 소스코드 등을 포함한 커스텀(Custom) 노트북 서버를 사용하고 싶은 경우에는 커스텀 이미지(Custom Image)를 만들고 배포 후 사용할 수도 있습니다.   CPU / RAM  필요한 자원 사용량을 입력합니다.  cpu : core 단위  가상 core 개수 단위를 의미하며, int 형식이 아닌 1.5, 2.7 등의 float 형식도 입력할 수 있습니다.   memory : Gi 단위     GPUs  주피터 노트북에 할당할 GPU 개수를 입력합니다.  None  GPU 자원이 필요하지 않은 상황   1, 2, 4  GPU 1, 2, 4 개 할당     GPU Vendor  앞의 (Optional) Setup GPU 를 따라 nvidia gpu plugin을 설치하였다면 NVIDIA를 선택합니다.     Workspace Volume  노트북 서버 내에서 필요한 만큼의 디스크 용량을 입력합니다. Type 과 Name 은 변경하지 않고, 디스크 용량을 늘리고 싶거나 AccessMode 를 변경하고 싶을 때에만 변경해서 사용하시면 됩니다.  \u0026ldquo;Don\u0026rsquo;t use Persistent Storage for User\u0026rsquo;s home\u0026rdquo; 체크박스는 노트북 서버의 작업 내용을 저장하지 않아도 상관없을 때에만 클릭합니다. 일반적으로는 누르지 않는 것을 권장합니다. 기존에 미리 생성해두었던 PVC를 사용하고 싶을 때에는, Type을 \u0026ldquo;Existing\u0026rdquo; 으로 입력하여 해당 PVC의 이름을 입력하여 사용하시면 됩니다.     Data Volumes  추가적인 스토리지 자원이 필요하다면 \u0026quot;+ ADD VOLUME\u0026quot; 버튼을 클릭하여 생성할 수 있습니다.   Configurations, Affinity/Tolerations, Miscellaneous Settings  일반적으로는 필요하지 않으므로 모두의 MLOps에서는 자세한 설명을 생략합니다.     모두 정상적으로 입력하였다면 하단의 LAUNCH 버튼이 활성화되며, 버튼을 클릭하면 노트북 서버 생성이 시작됩니다.\n생성 후 아래와 같이 Status 가 초록색 체크 표시 아이콘으로 변하며, CONNECT 버튼이 활성화됩니다.\n 노트북 서버 접속하기 # CONNECT 버튼을 클릭하면 브라우저에 새 창이 열리며, 다음과 같은 화면이 보입니다.\nLauncher의 Notebook, Console, Terminal 아이콘을 클릭하여 사용할 수 있습니다.\n생성된 Notebook 화면\n생성된 Terminal 화면\n 노트북 서버 중단하기 # 노트북 서버를 오랜 시간 사용하지 않는 경우, 쿠버네티스 클러스터의 효율적인 리소스 사용을 위해서 노트북 서버를 중단(Stop)할 수 있습니다. 단, 이 경우 노트북 서버 생성 시 Workspace Volume 또는 Data Volume으로 지정해놓은 경로 외에 저장된 데이터는 모두 초기화되는 것에 주의하시기 바랍니다.\n노트북 서버 생성 당시 경로를 변경하지 않았다면, 디폴트(Default) Workspace Volume의 경로는 노트북 서버 내의 /home/jovyan 이므로, /home/jovyan 의 하위 경로 이외의 경로에 저장된 데이터는 모두 사라집니다.\n다음과 같이 STOP 버튼을 클릭하면 노트북 서버가 중단됩니다.\n중단이 완료되면 다음과 같이 CONNECT 버튼이 비활성화되며, PLAY 버튼을 클릭하면 다시 정상적으로 사용할 수 있습니다.\n"}),e.add({id:23,href:"/docs/kubeflow-dashboard-guide/tensorboards/",title:"3. Tensorboards",description:"다음으로는 Central Dashboard의 왼쪽 탭의 Tensorboards를 클릭해보겠습니다.\n다음과 같은 화면을 볼 수 있습니다.\nTensorboards 탭은 Tensorflow, PyTorch 등의 프레임워크에서 제공하는 Tensorboard 유틸이 생성한 ML 학습 관련 데이터를 시각화하는 텐서보드 서버(Tensorboard Server)를 쿠버네티스 클러스터에 생성하는 기능을 제공합니다.\n이렇게 생성한 텐서보드 서버는, 일반적인 원격 텐서보드 서버의 사용법과 같이 사용할 수도 있으며, Kubeflow 파이프라인 런에서 바로 텐서보드 서버에 데이터를 저장하는 용도로 활용할 수 있습니다.\nKubeflow 파이프라인 런의 결과를 시각화하는 방법에는 다양한 방식이 있으며, 모두의 MLOps에서는 더 일반적으로 활용할 수 있도록 Kubeflow 컴포넌트의 Visualization 기능과 MLflow의 시각화 기능을 활용할 예정이므로, Tensorboards 페이지에 대한 자세한 설명은 생략하겠습니다.",content:"다음으로는 Central Dashboard의 왼쪽 탭의 Tensorboards를 클릭해보겠습니다.\n다음과 같은 화면을 볼 수 있습니다.\nTensorboards 탭은 Tensorflow, PyTorch 등의 프레임워크에서 제공하는 Tensorboard 유틸이 생성한 ML 학습 관련 데이터를 시각화하는 텐서보드 서버(Tensorboard Server)를 쿠버네티스 클러스터에 생성하는 기능을 제공합니다.\n이렇게 생성한 텐서보드 서버는, 일반적인 원격 텐서보드 서버의 사용법과 같이 사용할 수도 있으며, Kubeflow 파이프라인 런에서 바로 텐서보드 서버에 데이터를 저장하는 용도로 활용할 수 있습니다.\nKubeflow 파이프라인 런의 결과를 시각화하는 방법에는 다양한 방식이 있으며, 모두의 MLOps에서는 더 일반적으로 활용할 수 있도록 Kubeflow 컴포넌트의 Visualization 기능과 MLflow의 시각화 기능을 활용할 예정이므로, Tensorboards 페이지에 대한 자세한 설명은 생략하겠습니다.\n"}),e.add({id:24,href:"/docs/kubeflow-dashboard-guide/volumes/",title:"4. Volumes",description:"Volumes # 다음으로는 Central Dashboard의 왼쪽 탭의 Volumes를 클릭해보겠습니다.\n다음과 같은 화면을 볼 수 있습니다.\nVolumes 탭은 Kubernetes의 볼륨(Volume), 정확히는 퍼시스턴트 볼륨 클레임(Persistent Volume Claim, 이하 pvc) 중 현재 user의 namespace에 속한 pvc를 관리하는 기능을 제공합니다.\n위 스크린샷을 보면, 1. Notebooks 페이지에서 생성한 Volume의 정보를 확인할 수 있습니다. 해당 Volume의 Storage Class는 쿠버네티스 클러스터 설치 당시 설치한 Default Storage Class인 local-path로 설정되어있음을 확인할 수 있습니다.\n이외에도 user namespace에 새로운 볼륨을 생성하거나, 조회하거나, 삭제하고 싶은 경우에 Volumes 페이지를 활용할 수 있습니다.",content:"Volumes # 다음으로는 Central Dashboard의 왼쪽 탭의 Volumes를 클릭해보겠습니다.\n다음과 같은 화면을 볼 수 있습니다.\nVolumes 탭은 Kubernetes의 볼륨(Volume), 정확히는 퍼시스턴트 볼륨 클레임(Persistent Volume Claim, 이하 pvc) 중 현재 user의 namespace에 속한 pvc를 관리하는 기능을 제공합니다.\n위 스크린샷을 보면, 1. Notebooks 페이지에서 생성한 Volume의 정보를 확인할 수 있습니다. 해당 Volume의 Storage Class는 쿠버네티스 클러스터 설치 당시 설치한 Default Storage Class인 local-path로 설정되어있음을 확인할 수 있습니다.\n이외에도 user namespace에 새로운 볼륨을 생성하거나, 조회하거나, 삭제하고 싶은 경우에 Volumes 페이지를 활용할 수 있습니다.\n 볼륨 생성하기 # 오른쪽 위의 + NEW VOLUME 버튼을 클릭하면 다음과 같은 화면을 볼 수 있습니다.\nname, size, storage class, access mode를 지정하여 생성할 수 있습니다.\n원하는 리소스 스펙을 지정하여 생성하면 다음과 같이 볼륨의 Status가 Pending으로 조회됩니다. Status 아이콘에 마우스 커서를 가져다 대면 *해당 볼륨은 mount하여 사용하는 first consumer가 나타날 때 실제로 생성을 진행한다(This volume will be bound when its first consumer is created.)*는 메시지를 확인할 수 있습니다.\n이는 실습을 진행하는 StorageClass인 local-path의 볼륨 생성 정책에 해당하며, 문제 상황이 아닙니다.\n해당 페이지에서 Status가 Pending 으로 보이더라도 해당 볼륨을 사용하길 원하는 노트북 서버 혹은 파드(Pod)에서는 해당 볼륨의 이름을 지정하여 사용할 수 있으며, 그때 실제로 볼륨 생성이 진행됩니다.\n"}),e.add({id:25,href:"/docs/kubeflow-dashboard-guide/experiments/",title:"5. Experiments(AutoML)",description:"다음으로는 Central Dashboard의 왼쪽 탭의 Experiments(AutoML)을 클릭해보겠습니다.\nExperiments(AutoML) 페이지는 Kubeflow에서 Hyperparameter Tuning과 Neural Architecture Search를 통한 AutoML을 담당하는 Katib를 관리할 수 있는 페이지입니다.\nKatib와 Experiments(AutoML)에 대한 사용법은 모두의 MLOps v1.0에서는 다루지 않으며, v2.0에 추가될 예정입니다.",content:"다음으로는 Central Dashboard의 왼쪽 탭의 Experiments(AutoML)을 클릭해보겠습니다.\nExperiments(AutoML) 페이지는 Kubeflow에서 Hyperparameter Tuning과 Neural Architecture Search를 통한 AutoML을 담당하는 Katib를 관리할 수 있는 페이지입니다.\nKatib와 Experiments(AutoML)에 대한 사용법은 모두의 MLOps v1.0에서는 다루지 않으며, v2.0에 추가될 예정입니다.\n"}),e.add({id:26,href:"/docs/kubeflow-dashboard-guide/experiments-and-others/",title:"6. Kubeflow Pipeline 관련",description:"Central Dashboard의 왼쪽 탭의 Experiments(KFP), Pipelines, Runs, Recurring Runs, Artifacts, Executions 페이지들에서는 Kubeflow Pipeline과 Pipeline의 실행 그리고 Pipeline Run의 결과를 관리합니다.\nKubeflow Pipeline이 모두의 MLOps에서 Kubeflow를 사용하는 주된 이유이며, Kubeflow Pipeline을 만드는 방법, 실행하는 방법, 결과를 확인하는 방법 등 자세한 내용은 3.Kubeflow에서 다룹니다.",content:"Central Dashboard의 왼쪽 탭의 Experiments(KFP), Pipelines, Runs, Recurring Runs, Artifacts, Executions 페이지들에서는 Kubeflow Pipeline과 Pipeline의 실행 그리고 Pipeline Run의 결과를 관리합니다.\nKubeflow Pipeline이 모두의 MLOps에서 Kubeflow를 사용하는 주된 이유이며, Kubeflow Pipeline을 만드는 방법, 실행하는 방법, 결과를 확인하는 방법 등 자세한 내용은 3.Kubeflow에서 다룹니다.\n"}),e.add({id:27,href:"/docs/kubeflow/",title:"Kubeflow",description:"How to use Kubeflow.",content:""}),e.add({id:28,href:"/docs/kubeflow/kubeflow-intro/",title:"1. Kubeflow Introduction",description:"Kubeflow를 사용하기 위해서는 컴포넌트(Component)와 파이프라인(Pipeline)을 작성해야 합니다.\n모두의 MLOps에서 설명하는 방식은 Kubeflow Pipeline 공식 홈페이지에서 설명하는 방식과는 다소 차이가 있습니다. 여기에서는 Kubeflow Pipeline을 워크플로(Workflow)가 아닌 앞서 설명한 MLOps를 구성하는 요소 중 하나의 컴포넌트로 사용하기 때문입니다.\n그럼 이제 컴포넌트와 파이프라인은 무엇이며 어떻게 작성할 수 있는지 알아보도록 하겠습니다.",content:"Kubeflow를 사용하기 위해서는 컴포넌트(Component)와 파이프라인(Pipeline)을 작성해야 합니다.\n모두의 MLOps에서 설명하는 방식은 Kubeflow Pipeline 공식 홈페이지에서 설명하는 방식과는 다소 차이가 있습니다. 여기에서는 Kubeflow Pipeline을 워크플로(Workflow)가 아닌 앞서 설명한 MLOps를 구성하는 요소 중 하나의 컴포넌트로 사용하기 때문입니다.\n그럼 이제 컴포넌트와 파이프라인은 무엇이며 어떻게 작성할 수 있는지 알아보도록 하겠습니다.\n"}),e.add({id:29,href:"/docs/kubeflow/kubeflow-concepts/",title:"2. Kubeflow Concepts",description:"Component # 컴포넌트(Component)는 컴포넌트 콘텐츠(Component contents)와 컴포넌트 래퍼(Component wrapper)로 구성되어 있습니다. 하나의 컴포넌트는 컴포넌트 래퍼를 통해 kubeflow에 전달되며 전달된 컴포넌트는 정의된 컴포넌트 콘텐츠를 실행(execute)하고 아티팩트(artifacts)들을 생산합니다.\nComponent Contents # 컴포넌트 콘텐츠를 구성하는 것은 총 3가지가 있습니다.\n Environemnt Python code w\\ Config Generates Artifacts  예시와 함께 각 구성 요소가 어떤 것인지 알아보도록 하겠습니다. 다음과 같이 데이터를 불러와 SVC(Support Vector Classifier)를 학습한 후 SVC 모델을 저장하는 과정을 적은 파이썬 코드가 있습니다.",content:"Component # 컴포넌트(Component)는 컴포넌트 콘텐츠(Component contents)와 컴포넌트 래퍼(Component wrapper)로 구성되어 있습니다. 하나의 컴포넌트는 컴포넌트 래퍼를 통해 kubeflow에 전달되며 전달된 컴포넌트는 정의된 컴포넌트 콘텐츠를 실행(execute)하고 아티팩트(artifacts)들을 생산합니다.\nComponent Contents # 컴포넌트 콘텐츠를 구성하는 것은 총 3가지가 있습니다.\n Environemnt Python code w\\ Config Generates Artifacts  예시와 함께 각 구성 요소가 어떤 것인지 알아보도록 하겠습니다. 다음과 같이 데이터를 불러와 SVC(Support Vector Classifier)를 학습한 후 SVC 모델을 저장하는 과정을 적은 파이썬 코드가 있습니다.\nimport dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target= pd.read_csv(train_target_path) clf= SVC( kernel=kernel ) clf.fit(train_data) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) 위의 파이썬 코드는 다음과 같이 컴포넌트 콘텐츠로 나눌 수 있습니다.\nEnvironment는 파이썬 코드에서 사용하는 패키지들을 import하는 부분입니다.\n다음으로 Python Code w\\ Config 에서는 주어진 Config를 이용해 실제로 학습을 수행합니다.\n마지막으로 아티팩트를 저장하는 과정이 있습니다.\nComponent Wrapper # 컴포넌트 래퍼는 컴포넌트 콘텐츠에 필요한 Config를 전달하고 실행시키는 작업을 합니다.\nKubeflow에서는 컴포넌트 래퍼를 위의 train_svc_from_csv와 같이 함수의 형태로 정의합니다. 컴포넌트 래퍼가 콘텐츠를 감싸면 다음과 같이 됩니다.\nArtifacts # 위의 설명에서 컴포넌트는 아티팩트(Artifacts)를 생성한다고 했습니다. 아티팩트란 evaluation result, log 등 어떤 형태로든 파일로 생성되는 것을 통틀어서 칭하는 용어입니다. 그중 우리가 관심을 두는 유의미한 것들은 다음과 같은 것들이 있습니다.\n Model Data Metric etc  Model # 저희는 모델을 다음과 같이 정의 했습니다.\n 모델이란 파이썬 코드와 학습된 Weights와 Network 구조 그리고 이를 실행시키기 위한 환경이 모두 포함된 형태\n Data # 데이터는 전 처리된 피처, 모델의 예측 값 등을 포함합니다.\nMetric # Metric은 동적 지표와 정적 지표 두 가지로 나누었습니다.\n 동적 지표란 train loss와 같이 학습이 진행되는 중 에폭(Epoch)마다 계속해서 변화하는 값을 의미합니다. 정적 지표란 학습이 끝난 후 최종적으로 모델을 평가하는 정확도 등을 의미합니다.  Pipeline # 파이프라인은 컴포넌트의 집합과 컴포넌트를 실행시키는 순서도로 구성되어 있습니다. 이 때, 순서도는 방향 순환이 없는 그래프로 이루어져 있으며, 간단한 조건문을 포함할 수 있습니다.\nPipeline Config # 앞서 컴포넌트를 실행시키기 위해서는 Config가 필요하다고 설명했습니다. 파이프라인을 구성하는 컴포넌트의 Config 들을 모아 둔 것이 파이프라인 Config입니다.\nRun # 파이프라인이 필요로 하는 파이프라인 Config가 주어져야지만 파이프라인을 실행할 수 있습니다.\nKubeflow에서는 실행된 파이프라인을 Run 이라고 부릅니다.\n파이프라인이 실행되면 각 컴포넌트가 아티팩트들을 생성합니다. Kubeflow pipeline에서는 Run 하나당 고유한 ID 를 생성하고, Run에서 생성되는 모든 아티팩트들을 저장합니다.\n그러면 이제 직접 컴포넌트와 파이프라인을 작성하는 방법에 대해서 알아보도록 하겠습니다.\n"}),e.add({id:30,href:"/docs/kubeflow/basic-requirements/",title:"3. Install Requirements",description:"실습을 위해 권장하는 파이썬 버전은 python\u0026gt;=3.7입니다. 파이썬 환경에 익숙하지 않은 분들은 다음 Appendix 1. 파이썬 가상환경을 참고하여 클라이언트 노드에 설치해주신 뒤 패키지 설치를 진행해주시기를 바랍니다.\n실습을 진행하기에서 필요한 패키지들과 버전은 다음과 같습니다.\n  requirements.txt\nkfp==1.8.9 scikit-learn==1.0.1 mlflow==1.21.0 pandas==1.3.4 dill==0.3.4   앞에서 만든 파이썬 가상환경을 활성화합니다.\npyenv activate demo 패키지 설치를 진행합니다.\npip3 install -U pip pip3 install kfp==1.8.9 scikit-learn==1.0.1 mlflow==1.21.0 pandas==1.3.4 dill==0.3.4 ",content:"실습을 위해 권장하는 파이썬 버전은 python\u0026gt;=3.7입니다. 파이썬 환경에 익숙하지 않은 분들은 다음 Appendix 1. 파이썬 가상환경을 참고하여 클라이언트 노드에 설치해주신 뒤 패키지 설치를 진행해주시기를 바랍니다.\n실습을 진행하기에서 필요한 패키지들과 버전은 다음과 같습니다.\n  requirements.txt\nkfp==1.8.9 scikit-learn==1.0.1 mlflow==1.21.0 pandas==1.3.4 dill==0.3.4   앞에서 만든 파이썬 가상환경을 활성화합니다.\npyenv activate demo 패키지 설치를 진행합니다.\npip3 install -U pip pip3 install kfp==1.8.9 scikit-learn==1.0.1 mlflow==1.21.0 pandas==1.3.4 dill==0.3.4 "}),e.add({id:31,href:"/docs/kubeflow/basic-component/",title:"4. Component - Write",description:"Component # 컴포넌트(Component)를 작성하기 위해서는 다음과 같은 내용을 작성해야 합니다.\n 컴포넌트 콘텐츠(Component Contents) 작성 컴포넌트 래퍼(Component Wrapper) 작성  이제 각 과정에 대해서 알아보도록 하겠습니다.\nComponent Contents # 컴포넌트 콘텐츠는 우리가 흔히 작성하는 파이썬 코드와 다르지 않습니다.\n예를 들어서 숫자를 입력으로 받고 입력받은 숫자를 출력한 뒤 반환하는 컴포넌트를 작성해 보겠습니다.\n파이썬 코드로 작성하면 다음과 같이 작성할 수 있습니다.\nprint(number) 그런데 이 코드를 실행하면 에러가 나고 동작하지 않는데 그 이유는 출력해야 할 number가 정의되어 있지 않기 때문입니다.",content:"Component # 컴포넌트(Component)를 작성하기 위해서는 다음과 같은 내용을 작성해야 합니다.\n 컴포넌트 콘텐츠(Component Contents) 작성 컴포넌트 래퍼(Component Wrapper) 작성  이제 각 과정에 대해서 알아보도록 하겠습니다.\nComponent Contents # 컴포넌트 콘텐츠는 우리가 흔히 작성하는 파이썬 코드와 다르지 않습니다.\n예를 들어서 숫자를 입력으로 받고 입력받은 숫자를 출력한 뒤 반환하는 컴포넌트를 작성해 보겠습니다.\n파이썬 코드로 작성하면 다음과 같이 작성할 수 있습니다.\nprint(number) 그런데 이 코드를 실행하면 에러가 나고 동작하지 않는데 그 이유는 출력해야 할 number가 정의되어 있지 않기 때문입니다.\nKubeflow Concepts에서 number 와 같이 컴포넌트 콘텐츠에서 필요한 값들은 Config로 정의한다고 했습니다. 컴포넌트 콘텐츠를 실행시키기 위해 필요한 Config들은 컴포넌트 래퍼에서 전달이 되어야 합니다.\nComponent Wrapper # Define a standalone Python function # 이제 필요한 Config를 전달할 수 있도록 컴포넌트 래퍼를 만들어야 합니다.\n별도의 Config 없이 컴포넌트 래퍼로 감쌀 경우 다음과 같이 됩니다.\ndef print_and_return_number(): print(number) return number 이제 콘텐츠에서 필요한 Config를 래퍼의 argument로 추가합니다. 다만, argument 만을 적는 것이 아니라 argument의 타입 힌트도 작성해야 합니다. Kubeflow에서는 파이프라인을 Kubeflow 포맷으로 변환할 때, 컴포넌트 간의 연결에서 정해진 입력과 출력의 타입이 일치하는지 체크합니다. 만약 컴포넌트가 필요로 하는 입력과 다른 컴포넌트로부터 전달받은 출력의 포맷이 일치하지 않을 경우 파이프라인 생성을 할 수 없습니다.\n이제 다음과 같이 argument와 그 타입, 그리고 반환하는 타입을 적어서 컴포넌트 래퍼를 완성합니다.\ndef print_and_return_number(number: int) -\u0026gt; int: print(number) return number Kubeflow에서 반환 값으로 사용할 수 있는 타입은 json에서 표현할 수 있는 타입들만 사용할 수 있습니다. 대표적으로 사용되며 권장하는 타입들은 다음과 같습니다.\n int float str  만약 단일 값이 아닌 여러 값을 반환하려면 collections.namedtuple 을 이용해야 합니다.\n자세한 내용은 Kubeflow 공식 문서를 참고 하시길 바랍니다.\n예를 들어서 입력받은 숫자를 2로 나눈 몫과 나머지를 반환하는 컴포넌트는 다음과 같이 작성해야 합니다.\nfrom typing import NamedTuple def divide_and_return_number( number: int, ) -\u0026gt; NamedTuple(\u0026#34;DivideOutputs\u0026#34;, [(\u0026#34;quotient\u0026#34;, int), (\u0026#34;remainder\u0026#34;, int)]): from collections import namedtuple quotient, remainder = divmod(number, 2) print(\u0026#34;quotient is\u0026#34;, quotient) print(\u0026#34;remainder is\u0026#34;, remainder) divide_outputs = namedtuple( \u0026#34;DivideOutputs\u0026#34;, [ \u0026#34;quotient\u0026#34;, \u0026#34;remainder\u0026#34;, ], ) return divide_outputs(quotient, remainder) Convert to Kubeflow Format # 이제 작성한 컴포넌트를 kubeflow에서 사용할 수 있는 포맷으로 변환해야 합니다. 변환은 kfp.components.create_component_from_func 를 통해서 할 수 있습니다.\n이렇게 변환된 형태는 파이썬에서 함수로 import 하여서 파이프라인에서 사용할 수 있습니다.\nfrom kfp.components import create_component_from_func @create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number Share component with yaml file # 만약 파이썬 코드로 공유를 할 수 없는 경우 YAML 파일로 컴포넌트를 공유해서 사용할 수 있습니다. 이를 위해서는 우선 컴포넌트를 YAML 파일로 변환한 뒤 kfp.components.load_component_from_file 을 통해 파이프라인에서 사용할 수 있습니다.\n우선 작성한 컴포넌트를 YAML 파일로 변환하는 과정에 대해서 설명합니다.\nfrom kfp.components import create_component_from_func @create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number if __name__ == \u0026#34;__main__\u0026#34;: print_and_return_number.component_spec.save(\u0026#34;print_and_return_number.yaml\u0026#34;) 작성한 파이썬 코드를 실행하면 print_and_return_number.yaml 파일이 생성됩니다. 파일을 확인하면 다음과 같습니다.\nname: Print and return number inputs: - {name: number, type: Integer} outputs: - {name: Output, type: Integer} implementation: container: image: python:3.7 command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def print_and_return_number(number): print(number) return number def _serialize_int(int_value: int) -\u0026gt; str: if isinstance(int_value, str): return int_value if not isinstance(int_value, int): raise TypeError(\u0026#39;Value \u0026#34;{}\u0026#34; has type \u0026#34;{}\u0026#34; instead of int.\u0026#39;.format(str(int_value), str(type(int_value)))) return str(int_value) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Print and return number\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--number\u0026#34;, dest=\u0026#34;number\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;----output-paths\u0026#34;, dest=\u0026#34;_output_paths\u0026#34;, type=str, nargs=1) _parsed_args = vars(_parser.parse_args()) _output_files = _parsed_args.pop(\u0026#34;_output_paths\u0026#34;, []) _outputs = print_and_return_number(**_parsed_args) _outputs = [_outputs] _output_serializers = [ _serialize_int, ] import os for idx, output_file in enumerate(_output_files): try: os.makedirs(os.path.dirname(output_file)) except OSError: pass with open(output_file, \u0026#39;w\u0026#39;) as f: f.write(_output_serializers[idx](_outputs[idx])) args: - --number - {inputValue: number} - \u0026#39;----output-paths\u0026#39; - {outputPath: Output} 이제 생성된 파일을 공유해서 파이프라인에서 다음과 같이 사용할 수 있습니다.\nfrom kfp.components import load_component_from_file print_and_return_number = load_component_from_file(\u0026#34;print_and_return_number.yaml\u0026#34;) How Kubeflow executes component # Kubeflow에서 컴포넌트가 실행되는 순서는 다음과 같습니다.\n docker pull \u0026lt;image\u0026gt;: 정의된 컴포넌트의 실행 환경 정보가 담긴 이미지를 pull run command: pull 한 이미지에서 컴포넌트 콘텐츠를 실행합니다.  print_and_return_number.yaml 를 예시로 들자면 @create_component_from_func 의 default image 는 python:3.7 이므로 해당 이미지를 기준으로 컴포넌트 콘텐츠를 실행하게 됩니다.\n docker pull python:3.7 print(number)  References: #  Getting Started With Python function based components  "}),e.add({id:32,href:"/docs/kubeflow/basic-pipeline/",title:"5. Pipeline - Write",description:"Pipeline # 컴포넌트는 독립적으로 실행되지 않고 파이프라인의 구성요소로써 실행됩니다. 그러므로 컴포넌트를 실행해 보려면 파이프라인을 작성해야 합니다. 그리고 파이프라인을 작성하기 위해서는 컴포넌트의 집합과 컴포넌트의 실행 순서가 필요합니다.\n이번 페이지에서는 숫자를 입력받고 출력하는 컴포넌트와 두 개의 컴포넌트로부터 숫자를 받아서 합을 출력하는 컴포넌트가 있는 파이프라인을 만들어 보도록 하겠습니다.\nComponent Set # 우선 파이프라인에서 사용할 컴포넌트들을 작성합니다.\n print_and_return_number  입력받은 숫자를 출력하고 반환하는 컴포넌트입니다.\n컴포넌트가 입력받은 값을 반환하기 때문에 int를 return의 타입 힌트로 입력합니다.",content:"Pipeline # 컴포넌트는 독립적으로 실행되지 않고 파이프라인의 구성요소로써 실행됩니다. 그러므로 컴포넌트를 실행해 보려면 파이프라인을 작성해야 합니다. 그리고 파이프라인을 작성하기 위해서는 컴포넌트의 집합과 컴포넌트의 실행 순서가 필요합니다.\n이번 페이지에서는 숫자를 입력받고 출력하는 컴포넌트와 두 개의 컴포넌트로부터 숫자를 받아서 합을 출력하는 컴포넌트가 있는 파이프라인을 만들어 보도록 하겠습니다.\nComponent Set # 우선 파이프라인에서 사용할 컴포넌트들을 작성합니다.\n print_and_return_number  입력받은 숫자를 출력하고 반환하는 컴포넌트입니다.\n컴포넌트가 입력받은 값을 반환하기 때문에 int를 return의 타입 힌트로 입력합니다.\n@create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number sum_and_print_numbers  입력받은 두 개의 숫자의 합을 출력하는 컴포넌트입니다.\n이 컴포넌트 역시 두 숫자의 합을 반환하기 때문에 int를 return의 타입 힌트로 입력합니다.\n@create_component_from_func def sum_and_print_numbers(number_1: int, number_2: int) -\u0026gt; int: sum_num = number_1 + number_2 print(sum_num) return sum_num Component Order # Define Order # 필요한 컴포넌트의 집합을 만들었으면, 다음으로는 이들의 순서를 정의해야 합니다.\n이번 페이지에서 만들 파이프라인의 순서를 그림으로 표현하면 다음과 같이 됩니다.\nSingle Output # 이제 이 순서를 코드로 옮겨보겠습니다.\n우선 위의 그림에서 print_and_return_number_1 과 print_and_return_number_2 를 작성하면 다음과 같이 됩니다.\ndef example_pipeline(): number_1_result = print_and_return_number(number_1) number_2_result = print_and_return_number(number_2) 컴포넌트를 실행하고 그 반환 값을 각각 number_1_result 와 number_2_result 에 저장합니다.\n저장된 number_1_result 의 반환 값은 number_1_resulst.output 를 통해 사용할 수 있습니다.\nMulti Output # 위의 예시에서 컴포넌트는 단일 값만을 반환하기 때문에 output을 이용해 바로 사용할 수 있습니다.\n만약, 여러 개의 반환 값이 있다면 outputs에 저장이 되며 dict 타입이기에 key를 이용해 원하는 반환 값을 사용할 수 있습니다. 예를 들어서 앞에서 작성한 여러 개를 반환하는 컴포넌트 의 경우를 보겠습니다. divde_and_return_number 의 return 값은 quotient 와 remainder 가 있습니다. 이 두 값을 print_and_return_number 에 전달하는 예시를 보면 다음과 같습니다.\ndef multi_pipeline(): divided_result = divde_and_return_number(number) num_1_result = print_and_return_number(divided_result.outputs[\u0026#34;quotient\u0026#34;]) num_2_result = print_and_return_number(divided_result.outputs[\u0026#34;remainder\u0026#34;]) divde_and_return_number의 결과를 divided_result에 저장하고 각각 divided_result.outputs[\u0026quot;quotient\u0026quot;], divided_result.outputs[\u0026quot;remainder\u0026quot;]로 값을 가져올 수 있습니다.\nWrite to python code # 이제 다시 본론으로 돌아와서 이 두 값의 결과를 sum_and_print_numbers 에 전달합니다.\ndef example_pipeline(): number_1_result = print_and_return_number(number_1) number_2_result = print_and_return_number(number_2) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ) 다음으로 각 컴포넌트에 필요한 Config들을 모아서 파이프라인 Config로 정의 합니다.\ndef example_pipeline(number_1: int, number_2:int): number_1_result = print_and_return_number(number_1) number_2_result = print_and_return_number(number_2) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ) Convert to Kubeflow Format # 마지막으로 kubeflow에서 사용할 수 있는 형식으로 변환합니다. 변환은 kfp.dsl.pipeline 함수를 이용해 할 수 있습니다.\nfrom kfp.dsl import pipeline @pipeline(name=\u0026#34;example_pipeline\u0026#34;) def example_pipeline(number_1: int, number_2: int): number_1_result = print_and_return_number(number_1) number_2_result = print_and_return_number(number_2) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ) Kubeflow에서 파이프라인을 실행하기 위해서는 yaml 형식으로만 가능하기 때문에 생성한 파이프라인을 정해진 yaml 형식으로 컴파일(Compile) 해 주어야 합니다. 컴파일은 다음 명령어를 이용해 생성할 수 있습니다.\nif __name__ == \u0026#34;__main__\u0026#34;: import kfp kfp.compiler.Compiler().compile(example_pipeline, \u0026#34;example_pipeline.yaml\u0026#34;) Conclusion # 앞서 설명한 내용을 한 파이썬 코드로 모으면 다음과 같이 됩니다.\nimport kfp from kfp.components import create_component_from_func from kfp.dsl import pipeline @create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number @create_component_from_func def sum_and_print_numbers(number_1: int, number_2: int): print(number_1 + number_2) @pipeline(name=\u0026#34;example_pipeline\u0026#34;) def example_pipeline(number_1: int, number_2: int): number_1_result = print_and_return_number(number_1) number_2_result = print_and_return_number(number_2) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(example_pipeline, \u0026#34;example_pipeline.yaml\u0026#34;) 컴파일된 결과를 보면 다음과 같습니다.\n example_pipeline.yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: example-pipeline- annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline_compilation_time: \u0026#39;2021-12-05T13:38:51.566777\u0026#39;, pipelines.kubeflow.org/pipeline_spec: \u0026#39;{\u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;number_1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;number_2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;example_pipeline\u0026#34;}\u0026#39;} labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3} spec: entrypoint: example-pipeline templates: - name: example-pipeline inputs: parameters: - {name: number_1} - {name: number_2} dag: tasks: - name: print-and-return-number template: print-and-return-number arguments: parameters: - {name: number_1, value: \u0026#39;{{inputs.parameters.number_1}}\u0026#39;} - name: print-and-return-number-2 template: print-and-return-number-2 arguments: parameters: - {name: number_2, value: \u0026#39;{{inputs.parameters.number_2}}\u0026#39;} - name: sum-and-print-numbers template: sum-and-print-numbers dependencies: [print-and-return-number, print-and-return-number-2] arguments: parameters: - {name: print-and-return-number-2-Output, value: \u0026#39;{{tasks.print-and-return-number-2.outputs.parameters.print-and-return-number-2-Output}}\u0026#39;} - {name: print-and-return-number-Output, value: \u0026#39;{{tasks.print-and-return-number.outputs.parameters.print-and-return-number-Output}}\u0026#39;} - name: print-and-return-number container: args: [--number, \u0026#39;{{inputs.parameters.number_1}}\u0026#39;, \u0026#39;----output-paths\u0026#39;, /tmp/outputs/Output/data] command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def print_and_return_number(number): print(number) return number def _serialize_int(int_value: int) -\u0026gt; str: if isinstance(int_value, str): return int_value if not isinstance(int_value, int): raise TypeError(\u0026#39;Value \u0026#34;{}\u0026#34; has type \u0026#34;{}\u0026#34; instead of int.\u0026#39;.format(str(int_value), str(type(int_value)))) return str(int_value) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Print and return number\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--number\u0026#34;, dest=\u0026#34;number\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;----output-paths\u0026#34;, dest=\u0026#34;_output_paths\u0026#34;, type=str, nargs=1) _parsed_args = vars(_parser.parse_args()) _output_files = _parsed_args.pop(\u0026#34;_output_paths\u0026#34;, []) _outputs = print_and_return_number(**_parsed_args) _outputs = [_outputs] _output_serializers = [ _serialize_int, ] import os for idx, output_file in enumerate(_output_files): try: os.makedirs(os.path.dirname(output_file)) except OSError: pass with open(output_file, \u0026#39;w\u0026#39;) as f: f.write(_output_serializers[idx](_outputs[idx])) image: python:3.7 inputs: parameters: - {name: number_1} outputs: parameters: - name: print-and-return-number-Output valueFrom: {path: /tmp/outputs/Output/data} artifacts: - {name: print-and-return-number-Output, path: /tmp/outputs/Output/data} metadata: labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp} annotations: {pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--number\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;number\u0026#34;}, \u0026#34;----output-paths\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;Output\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def print_and_return_number(number):\\n print(number)\\n return number\\n\\ndef _serialize_int(int_value: int) -\u0026gt; str:\\n if isinstance(int_value, str):\\n return int_value\\n if not isinstance(int_value, int):\\n raise TypeError(\u0026#39;\u0026#39;Value \\\u0026#34;{}\\\u0026#34; has type \\\u0026#34;{}\\\u0026#34; instead of int.\u0026#39;\u0026#39;.format(str(int_value), str(type(int_value))))\\n return str(int_value)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Print and return number\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--number\\\u0026#34;, dest=\\\u0026#34;number\\\u0026#34;, type=int, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;----output-paths\\\u0026#34;, dest=\\\u0026#34;_output_paths\\\u0026#34;, type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\\\u0026#34;_output_paths\\\u0026#34;, [])\\n\\n_outputs = print_and_return_number(**_parsed_args)\\n\\n_outputs = [_outputs]\\n\\n_output_serializers = [\\n _serialize_int,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n try:\\n os.makedirs(os.path.dirname(output_file))\\n except OSError:\\n pass\\n with open(output_file, \u0026#39;\u0026#39;w\u0026#39;\u0026#39;) as f:\\n f.write(_output_serializers[idx](_outputs[idx]))\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;Print and return number\u0026#34;, \u0026#34;outputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;Output\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}]}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;, pipelines.kubeflow.org/arguments.parameters: \u0026#39;{\u0026#34;number\u0026#34;: \u0026#34;{{inputs.parameters.number_1}}\u0026#34;}\u0026#39;} - name: print-and-return-number-2 container: args: [--number, \u0026#39;{{inputs.parameters.number_2}}\u0026#39;, \u0026#39;----output-paths\u0026#39;, /tmp/outputs/Output/data] command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def print_and_return_number(number): print(number) return number def _serialize_int(int_value: int) -\u0026gt; str: if isinstance(int_value, str): return int_value if not isinstance(int_value, int): raise TypeError(\u0026#39;Value \u0026#34;{}\u0026#34; has type \u0026#34;{}\u0026#34; instead of int.\u0026#39;.format(str(int_value), str(type(int_value)))) return str(int_value) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Print and return number\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--number\u0026#34;, dest=\u0026#34;number\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;----output-paths\u0026#34;, dest=\u0026#34;_output_paths\u0026#34;, type=str, nargs=1) _parsed_args = vars(_parser.parse_args()) _output_files = _parsed_args.pop(\u0026#34;_output_paths\u0026#34;, []) _outputs = print_and_return_number(**_parsed_args) _outputs = [_outputs] _output_serializers = [ _serialize_int, ] import os for idx, output_file in enumerate(_output_files): try: os.makedirs(os.path.dirname(output_file)) except OSError: pass with open(output_file, \u0026#39;w\u0026#39;) as f: f.write(_output_serializers[idx](_outputs[idx])) image: python:3.7 inputs: parameters: - {name: number_2} outputs: parameters: - name: print-and-return-number-2-Output valueFrom: {path: /tmp/outputs/Output/data} artifacts: - {name: print-and-return-number-2-Output, path: /tmp/outputs/Output/data} metadata: labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp} annotations: {pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--number\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;number\u0026#34;}, \u0026#34;----output-paths\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;Output\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def print_and_return_number(number):\\n print(number)\\n return number\\n\\ndef _serialize_int(int_value: int) -\u0026gt; str:\\n if isinstance(int_value, str):\\n return int_value\\n if not isinstance(int_value, int):\\n raise TypeError(\u0026#39;\u0026#39;Value \\\u0026#34;{}\\\u0026#34; has type \\\u0026#34;{}\\\u0026#34; instead of int.\u0026#39;\u0026#39;.format(str(int_value), str(type(int_value))))\\n return str(int_value)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Print and return number\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--number\\\u0026#34;, dest=\\\u0026#34;number\\\u0026#34;, type=int, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;----output-paths\\\u0026#34;, dest=\\\u0026#34;_output_paths\\\u0026#34;, type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\\\u0026#34;_output_paths\\\u0026#34;, [])\\n\\n_outputs = print_and_return_number(**_parsed_args)\\n\\n_outputs = [_outputs]\\n\\n_output_serializers = [\\n _serialize_int,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n try:\\n os.makedirs(os.path.dirname(output_file))\\n except OSError:\\n pass\\n with open(output_file, \u0026#39;\u0026#39;w\u0026#39;\u0026#39;) as f:\\n f.write(_output_serializers[idx](_outputs[idx]))\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;Print and return number\u0026#34;, \u0026#34;outputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;Output\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}]}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;, pipelines.kubeflow.org/arguments.parameters: \u0026#39;{\u0026#34;number\u0026#34;: \u0026#34;{{inputs.parameters.number_2}}\u0026#34;}\u0026#39;} - name: sum-and-print-numbers container: args: [--number-1, \u0026#39;{{inputs.parameters.print-and-return-number-Output}}\u0026#39;, --number-2, \u0026#39;{{inputs.parameters.print-and-return-number-2-Output}}\u0026#39;] command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def sum_and_print_numbers(number_1, number_2): print(number_1 + number_2) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Sum and print numbers\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--number-1\u0026#34;, dest=\u0026#34;number_1\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--number-2\u0026#34;, dest=\u0026#34;number_2\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = sum_and_print_numbers(**_parsed_args) image: python:3.7 inputs: parameters: - {name: print-and-return-number-2-Output} - {name: print-and-return-number-Output} metadata: labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp} annotations: {pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--number-1\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;number_1\u0026#34;}, \u0026#34;--number-2\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;number_2\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def sum_and_print_numbers(number_1, number_2):\\n print(number_1 + number_2)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Sum and print numbers\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--number-1\\\u0026#34;, dest=\\\u0026#34;number_1\\\u0026#34;, type=int, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--number-2\\\u0026#34;, dest=\\\u0026#34;number_2\\\u0026#34;, type=int, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = sum_and_print_numbers(**_parsed_args)\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;number_1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;number_2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;Sum and print numbers\u0026#34;}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;, pipelines.kubeflow.org/arguments.parameters: \u0026#39;{\u0026#34;number_1\u0026#34;: \u0026#34;{{inputs.parameters.print-and-return-number-Output}}\u0026#34;, \u0026#34;number_2\u0026#34;: \u0026#34;{{inputs.parameters.print-and-return-number-2-Output}}\u0026#34;}\u0026#39;} arguments: parameters: - {name: number_1} - {name: number_2} serviceAccountName: pipeline-runner  "}),e.add({id:33,href:"/docs/kubeflow/basic-pipeline-upload/",title:"6. Pipeline - Upload",description:"Upload Pipeline # 이제 우리가 만든 파이프라인을 직접 kubeflow에서 업로드 해 보겠습니다.\n파이프라인 업로드는 kubeflow 대시보드 UI를 통해 진행할 수 있습니다. Install Kubeflow 에서 사용한 방법을 이용해 포트포워딩합니다.\nkubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 http://localhost:8080에 접속해 대시보드를 열어줍니다.\n1. Pipelines 탭 선택 # 2. Upload Pipeline 선택 # 3. Choose file 선택 # 4. 생성된 yaml파일 업로드 # 5. Create # Upload Pipeline Version # 업로드된 파이프라인은 업로드를 통해서 버전을 관리할 수 있습니다.",content:"Upload Pipeline # 이제 우리가 만든 파이프라인을 직접 kubeflow에서 업로드 해 보겠습니다.\n파이프라인 업로드는 kubeflow 대시보드 UI를 통해 진행할 수 있습니다. Install Kubeflow 에서 사용한 방법을 이용해 포트포워딩합니다.\nkubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 http://localhost:8080에 접속해 대시보드를 열어줍니다.\n1. Pipelines 탭 선택 # 2. Upload Pipeline 선택 # 3. Choose file 선택 # 4. 생성된 yaml파일 업로드 # 5. Create # Upload Pipeline Version # 업로드된 파이프라인은 업로드를 통해서 버전을 관리할 수 있습니다. 다만 깃헙과 같은 코드 차원의 버전 관리가 아닌 같은 이름의 파이프라인을 모아서 보여주는 역할을 합니다. 위의 예시에서 파이프라인을 업로드한 경우 다음과 같이 example_pipeline이 생성된 것을 확인할 수 있습니다.\n클릭하면 다음과 같은 화면이 나옵니다.\nUpload Version을 클릭하면 다음과 같이 파이프라인을 업로드할 수 있는 화면이 생성됩니다.\n파이프라인을 업로드 합니다.\n업로드된 경우 다음과 같이 파이프라인 버전을 확인할 수 있습니다.\n"}),e.add({id:34,href:"/docs/kubeflow/basic-run/",title:"7. Pipeline - Run",description:"Run Pipeline # 이제 업로드한 파이프라인을 실행시켜 보겠습니다.\nBefore Run # 1. Create Experiment # Experiment란 Kubeflow 에서 실행되는 Run을 논리적으로 관리하는 단위입니다.\nKubeflow에서 namespace를 처음 들어오면 생성되어 있는 Experiment가 없습니다. 따라서 파이프라인을 실행하기 전에 미리 Experiment를 생성해두어야 합니다. Experiment이 있다면 Run Pipeline으로 넘어가도 무방합니다.\nExperiment는 Create Experiment 버튼을 통해 생성할 수 있습니다.\n2. Name 입력 # Experiment로 사용할 이름을 입력합니다.\nRun Pipeline # 1. Create Run 선택 # 2. Experiment 선택 # 3.",content:"Run Pipeline # 이제 업로드한 파이프라인을 실행시켜 보겠습니다.\nBefore Run # 1. Create Experiment # Experiment란 Kubeflow 에서 실행되는 Run을 논리적으로 관리하는 단위입니다.\nKubeflow에서 namespace를 처음 들어오면 생성되어 있는 Experiment가 없습니다. 따라서 파이프라인을 실행하기 전에 미리 Experiment를 생성해두어야 합니다. Experiment이 있다면 Run Pipeline으로 넘어가도 무방합니다.\nExperiment는 Create Experiment 버튼을 통해 생성할 수 있습니다.\n2. Name 입력 # Experiment로 사용할 이름을 입력합니다.\nRun Pipeline # 1. Create Run 선택 # 2. Experiment 선택 # 3. Pipeline Config 입력 # 파이프라인을 생성할 때 입력한 Config 값들을 채워 넣습니다. 업로드한 파이프라인은 number_1과 number_2를 입력해야 합니다.\n4. Start # 입력 후 Start 버튼을 누르면 파이프라인이 실행됩니다.\nRun Result # 실행된 파이프라인들은 Runs 탭에서 확인할 수 있습니다. Run을 클릭하면 실행된 파이프라인과 관련된 자세한 내용을 확인해 볼 수 있습니다.\n클릭하면 다음과 같은 화면이 나옵니다. 아직 실행되지 않은 컴포넌트는 회색 표시로 나옵니다.\n컴포넌트가 실행이 완료되면 초록색 체크 표시가 나옵니다.\n가장 마지막 컴포넌트를 보면 입력한 Config인 3과 5의 합인 8이 출력된 것을 확인할 수 있습니다.\n"}),e.add({id:35,href:"/docs/kubeflow/advanced-component/",title:"8. Component - InputPath/OutputPath",description:"Complex Outputs # 이번 페이지에서는 Kubeflow Concepts 예시로 나왔던 코드를 컴포넌트로 작성해 보겠습니다.\nComponent Contents # 아래 코드는 Kubeflow Concepts에서 사용했던 컴포넌트 콘텐츠입니다.\nimport dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) Component Wrapper # Define a standalone Python function # 컴포넌트 래퍼에 필요한 Config들과 함께 작성하면 다음과 같이 됩니다.\ndef train_from_csv( train_data_path: str, train_target_path: str, model_path: str, kernel: str, ): import dill import pandas as pd from sklearn.",content:"Complex Outputs # 이번 페이지에서는 Kubeflow Concepts 예시로 나왔던 코드를 컴포넌트로 작성해 보겠습니다.\nComponent Contents # 아래 코드는 Kubeflow Concepts에서 사용했던 컴포넌트 콘텐츠입니다.\nimport dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) Component Wrapper # Define a standalone Python function # 컴포넌트 래퍼에 필요한 Config들과 함께 작성하면 다음과 같이 됩니다.\ndef train_from_csv( train_data_path: str, train_target_path: str, model_path: str, kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) Basic Usage Component에서 설명할 때 입력과 출력에 대한 타입 힌트를 적어야 한다고 설명 했었습니다. 그런데 만약 json에서 사용할 수 있는 기본 타입이 아닌 dataframe, model와 같이 복잡한 객체들은 어떻게 할까요?\n파이썬에서 함수간에 값을 전달할 때, 객체를 반환해도 그 값이 호스트의 메모리에 저장되어 있으므로 다음 함수에서도 같은 객체를 사용할 수 있습니다. 하지만 kubeflow에서 컴포넌트들은 각각 컨테이너 위에서 서로 독립적으로 실행됩니다. 즉, 같은 메모리를 공유하고 있지 않기 때문에, 보통의 파이썬 함수에서 사용하는 방식과 같이 객체를 전달할 수 없습니다. 컴포넌트 간에 넘겨 줄 수 있는 정보는 json 으로만 가능합니다. 따라서 Model이나 DataFrame과 같이 json 형식으로 변환할 수 없는 타입의 객체는 다른 방법을 통해야 합니다.\nKubeflow에서는 이를 해결하기 위해 json-serializable 하지 않은 타입의 객체는 메모리 대신 파일에 데이터를 저장한 뒤, 그 파일을 이용해 정보를 전달합니다. 저장된 파일의 경로는 str이기 때문에 컴포넌트 간에 전달할 수 있기 때문입니다. 그런데 kubeflow에서는 minio를 이용해 파일을 저장하는데 유저는 실행을 하기 전에는 각 파일의 경로를 알 수 없습니다. 이를 위해서 kubeflow에서는 입력과 출력의 경로와 관련된 매직을 제공하는데 바로 InputPath와 OutputPath 입니다.\nInputPath는 단어 그대로 입력 경로를 OutputPath 는 단어 그대로 출력 경로를 의미합니다.\n예를 들어서 데이터를 생성하고 반환하는 컴포넌트에서는 data_path: OutputPath()를 argument로 만듭니다. 그리고 데이터를 받는 컴포넌트에서는 data_path: InputPath()을 argument로 생성합니다.\n이렇게 만든 후 파이프라인에서 서로 연결을 하면 kubeflow에서 필요한 경로를 자동으로 생성후 입력해 주기 때문에 더 이상 유저는 경로를 신경쓰지 않고 컴포넌트간의 관계만 신경쓰면 됩니다.\n이제 이 내용을 바탕으로 다시 컴포넌트 래퍼를 작성하면 다음과 같이 됩니다.\nfrom kfp.components import InputPath, OutputPath def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) InputPath나 OutputPath는 string을 입력할 수 있습니다. 이 string은 입력 또는 출력하려고 하는 파일의 포맷입니다.\n그렇다고 꼭 이 포맷으로 파일 형태로 저장이 강제되는 것은 아닙니다.\n다만 파이프라인을 컴파일할 때 최소한의 타입 체크를 위한 도우미 역할을 합니다.\n만약 파일 포맷이 고정되지 않는다면 입력하지 않으면 됩니다 (타입 힌트 에서 Any 와 같은 역할을 합니다).\nConvert to Kubeflow Format # 작성한 컴포넌트를 kubeflow에서 사용할 수 있는 포맷으로 변환합니다.\nfrom kfp.components import InputPath, OutputPath, create_component_from_func @create_component_from_func def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) Rule to use InputPath/OutputPath # InputPath나 OutputPath argument는 파이프라인으로 작성할 때 지켜야하는 규칙이 있습니다.\nLoad Data Component # 위에서 작성한 컴포넌트를 실행하기 위해서는 데이터가 필요하므로 데이터를 생성하는 컴포넌트를 작성합니다.\nfrom functools import partial from kfp.components import InputPath, OutputPath, create_component_from_func @create_component_from_func def load_iris_data( data_path: OutputPath(\u0026#34;csv\u0026#34;), target_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.DataFrame(iris[\u0026#34;data\u0026#34;], columns=iris[\u0026#34;feature_names\u0026#34;]) target = pd.DataFrame(iris[\u0026#34;target\u0026#34;], columns=[\u0026#34;target\u0026#34;]) data.to_csv(data_path, index=False) target.to_csv(target_path, index=False) Write Pipeline # 이제 파이프라인을 작성해 보도록 하겠습니다.\nfrom kfp.dsl import pipeline @pipeline(name=\u0026#34;complex_pipeline\u0026#34;) def complex_pipeline(kernel: str): iris_data = load_iris_data() model = train_from_csv( train_data=iris_data.outputs[\u0026#34;data\u0026#34;], train_target=iris_data.outputs[\u0026#34;target\u0026#34;], kernel=kernel, ) 한 가지 이상한 점을 확인하셨나요?\n바로 입력과 출력에서 받는 argument중 경로와 관련된 것들에 _path 접미사가 모두 사라졌습니다.\niris_data.outputs[\u0026quot;data_path\u0026quot;] 가 아닌 iris_data.outputs[\u0026quot;data\u0026quot;] 으로 접근하는 것을 확인할 수 있습니다.\n이는 kubeflow에서 정한 법칙으로 InputPath 와 OutputPath 으로 생성된 경로들은 파이프라인에서 접근할 때는 _path 접미사를 생략하여 접근합니다.\n다만 방금 작성한 파이프라인을 업로드할 경우 실행이 되지 않습니다. 이유는 다음 페이지에서 설명합니다.\n"}),e.add({id:36,href:"/docs/kubeflow/advanced-environment/",title:"9. Component - Environment",description:"Component Environment # 앞서 8. Component - InputPath/OutputPath에서 작성한 파이프라인을 실행하면 실패하게 됩니다. 왜 실패하는지 알아보고 정상적으로 실행될 수 있도록 수정합니다.\nConvert to Kubeflow Format # 앞에서 작성한 컴포넌트를 yaml파일로 변환하도록 하겠습니다.\nfrom kfp.components import InputPath, OutputPath, create_component_from_func @create_component_from_func def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.",content:"Component Environment # 앞서 8. Component - InputPath/OutputPath에서 작성한 파이프라인을 실행하면 실패하게 됩니다. 왜 실패하는지 알아보고 정상적으로 실행될 수 있도록 수정합니다.\nConvert to Kubeflow Format # 앞에서 작성한 컴포넌트를 yaml파일로 변환하도록 하겠습니다.\nfrom kfp.components import InputPath, OutputPath, create_component_from_func @create_component_from_func def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) if __name__ == \u0026#34;__main__\u0026#34;: train_from_csv.component_spec.save(\u0026#34;train_from_csv.yaml\u0026#34;) 위의 스크립트를 실행하면 다음과 같은 train_from_csv.yaml 파일을 얻을 수 있습니다.\nname: Train from csv inputs: - {name: train_data, type: csv} - {name: train_target, type: csv} - {name: model, type: dill} - {name: kernel, type: String} implementation: container: image: python:3.7 command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def train_from_csv( train_data_path, train_target_path, model_path, kernel, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Train from csv\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--train-data\u0026#34;, dest=\u0026#34;train_data_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--train-target\u0026#34;, dest=\u0026#34;train_target_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--model\u0026#34;, dest=\u0026#34;model_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--kernel\u0026#34;, dest=\u0026#34;kernel\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = train_from_csv(**_parsed_args) args: - --train-data - {inputPath: train_data} - --train-target - {inputPath: train_target} - --model - {inputPath: model} - --kernel - {inputValue: kernel} 앞서 Basic Usage Component에서 설명한 내용에 따르면 이 컴포넌트는 다음과 같이 실행됩니다.\n docker pull python:3.7 run command  하지만 위에서 생성된 컴포넌트를 실행하면 오류가 발생하게 됩니다.\n그 이유는 컴포넌트 래퍼가 실행되는 방식에 있습니다.\nKubeflow는 쿠버네티스를 이용하기 때문에 컴포넌트 래퍼는 각각 독립된 컨테이너 위에서 컴포넌트 콘텐츠를 실행합니다.\n자세히 보면 생성된 만든 train_from_csv.yaml 에서 정해진 이미지는 image: python:3.7 입니다.\n이제 어떤 이유 때문에 실행이 안 되는지 눈치채신 분들도 있을 것입니다.\npython:3.7 이미지에는 우리가 사용하고자 하는 dill, pandas, sklearn 이 설치되어 있지 않습니다.\n그러므로 실행할 때 해당 패키지가 존재하지 않는다는 에러와 함께 실행이 안 됩니다.\n그럼 어떻게 패키지를 추가할 수 있을까요?\n패키지 추가 방법 # Kubeflow를 변환하는 과정에서 두 가지 방법을 통해 패키지를 추가할 수 있습니다.\n base_image 사용 package_to_install 사용  컴포넌트를 컴파일할 때 사용했던 함수 create_component_from_func 가 어떤 argument들을 받을 수 있는지 확인해 보겠습니다.\ndef create_component_from_func( func: Callable, output_component_file: Optional[str] = None, base_image: Optional[str] = None, packages_to_install: List[str] = None, annotations: Optional[Mapping[str, str]] = None, ):  func: 컴포넌트로 만들 컴포넌트 래퍼 함수 base_image: 컴포넌트 래퍼가 실행할 이미지 packages_to_install: 컴포넌트에서 사용해서 추가로 설치해야 하는 패키지  1. base_image # 컴포넌트가 실행되는 순서를 좀 더 자세히 들여다보면 다음과 같습니다.\n docker pull base_image pip install packages_to_install run command  만약 컴포넌트가 사용하는 base_image에 패키지들이 전부 설치되어 있다면 추가적인 패키지 설치 없이 바로 사용할 수 있습니다.\n예를 들어, 이번 페이지에서는 다음과 같은 Dockerfile을 작성하겠습니다.\nFROMpython:3.7RUN pip install dill pandas scikit-learn위의 Dockerfile을 이용해 이미지를 빌드해 보겠습니다. 실습에서 사용해볼 도커 허브는 ghcr입니다.\n각자 환경에 맞추어서 도커 허브를 선택 후 업로드하면 됩니다.\ndocker build . -f Dockerfile -t ghcr.io/mlops-for-all/base-image docker push ghcr.io/mlops-for-all/base-image 이제 base_image를 입력해 보겠습니다.\nfrom functools import partial from kfp.components import InputPath, OutputPath, create_component_from_func @partial( create_component_from_func, base_image=\u0026#34;ghcr.io/mlops-for-all/base-image:latest\u0026#34;, ) def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) if __name__ == \u0026#34;__main__\u0026#34;: train_from_csv.component_spec.save(\u0026#34;train_from_csv.yaml\u0026#34;) 이제 생성된 컴포넌트를 컴파일하면 다음과 같이 나옵니다.\nname: Train from csv inputs: - {name: train_data, type: csv} - {name: train_target, type: csv} - {name: kernel, type: String} outputs: - {name: model, type: dill} implementation: container: image: ghcr.io/mlops-for-all/base-image:latest command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def _make_parent_dirs_and_return_path(file_path: str): import os os.makedirs(os.path.dirname(file_path), exist_ok=True) return file_path def train_from_csv( train_data_path, train_target_path, model_path, kernel, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Train from csv\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--train-data\u0026#34;, dest=\u0026#34;train_data_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--train-target\u0026#34;, dest=\u0026#34;train_target_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--kernel\u0026#34;, dest=\u0026#34;kernel\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--model\u0026#34;, dest=\u0026#34;model_path\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = train_from_csv(**_parsed_args) args: - --train-data - {inputPath: train_data} - --train-target - {inputPath: train_target} - --kernel - {inputValue: kernel} - --model - {outputPath: model} base_image가 우리가 설정한 값으로 바뀐 것을 확인할 수 있습니다.\n2. packages_to_install # 하지만 패키지가 추가될 때마다 docker 이미지를 계속해서 새로 생성하는 작업은 많은 시간이 소요됩니다. 이 때, packages_to_install argument 를 사용하면 패키지를 컨테이너에 쉽게 추가할 수 있습니다.\nfrom functools import partial from kfp.components import InputPath, OutputPath, create_component_from_func @partial( create_component_from_func, packages_to_install=[\u0026#34;dill==0.3.4\u0026#34;, \u0026#34;pandas==1.3.4\u0026#34;, \u0026#34;scikit-learn==1.0.1\u0026#34;], ) def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) if __name__ == \u0026#34;__main__\u0026#34;: train_from_csv.component_spec.save(\u0026#34;train_from_csv.yaml\u0026#34;) 스크립트를 실행하면 다음과 같은 train_from_csv.yaml 파일이 생성됩니다.\nname: Train from csv inputs: - {name: train_data, type: csv} - {name: train_target, type: csv} - {name: kernel, type: String} outputs: - {name: model, type: dill} implementation: container: image: python:3.7 command: - sh - -c - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;dill==0.3.4\u0026#39; \u0026#39;pandas==1.3.4\u0026#39; \u0026#39;scikit-learn==1.0.1\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;dill==0.3.4\u0026#39; \u0026#39;pandas==1.3.4\u0026#39; \u0026#39;scikit-learn==1.0.1\u0026#39; --user) \u0026amp;\u0026amp; \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def _make_parent_dirs_and_return_path(file_path: str): import os os.makedirs(os.path.dirname(file_path), exist_ok=True) return file_path def train_from_csv( train_data_path, train_target_path, model_path, kernel, ): import dill import pandas as pd from sklearn.svm import SVC train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Train from csv\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--train-data\u0026#34;, dest=\u0026#34;train_data_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--train-target\u0026#34;, dest=\u0026#34;train_target_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--kernel\u0026#34;, dest=\u0026#34;kernel\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--model\u0026#34;, dest=\u0026#34;model_path\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = train_from_csv(**_parsed_args) args: - --train-data - {inputPath: train_data} - --train-target - {inputPath: train_target} - --kernel - {inputValue: kernel} - --model - {outputPath: model} 위에 작성한 컴포넌트가 실행되는 순서를 좀 더 자세히 들여다보면 다음과 같습니다.\n docker pull python:3.7 pip install dill==0.3.4 pandas==1.3.4 scikit-learn==1.0.1 run command  생성된 yaml 파일을 자세히 보면, 다음과 같은 줄이 자동으로 추가되어 필요한 패키지가 설치되기 때문에 오류 없이 정상적으로 실행됩니다.\ncommand: - sh - -c - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;dill==0.3.4\u0026#39; \u0026#39;pandas==1.3.4\u0026#39; \u0026#39;scikit-learn==1.0.1\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;dill==0.3.4\u0026#39; \u0026#39;pandas==1.3.4\u0026#39; \u0026#39;scikit-learn==1.0.1\u0026#39; --user) \u0026amp;\u0026amp; \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; "}),e.add({id:37,href:"/docs/kubeflow/advanced-pipeline/",title:"10. Pipeline - Setting",description:"Pipeline Setting # 이번 페이지에서는 파이프라인에서 설정할 수 있는 값들에 대해 알아보겠습니다.\nDisplay Name # 생성된 파이프라인 내에서 컴포넌트는 두 개의 이름을 갖습니다.\n task_name: 컴포넌트를 작성할 때 작성한 함수 이름 display_name: kubeflow UI상에 보이는 이름  예를 들어서 다음과 같은 경우 두 컴포넌트 모두 Print and return number로 설정되어 있어서 어떤 컴포넌트가 1번인지 2번인지 확인하기 어렵습니다.\nset_display_name # 이를 위한 것이 바로 display_name 입니다.\n설정하는 방법은 파이프라인에서 컴포넌트에 다음과 같이 set_display_name attribute를 이용하면 됩니다.",content:"Pipeline Setting # 이번 페이지에서는 파이프라인에서 설정할 수 있는 값들에 대해 알아보겠습니다.\nDisplay Name # 생성된 파이프라인 내에서 컴포넌트는 두 개의 이름을 갖습니다.\n task_name: 컴포넌트를 작성할 때 작성한 함수 이름 display_name: kubeflow UI상에 보이는 이름  예를 들어서 다음과 같은 경우 두 컴포넌트 모두 Print and return number로 설정되어 있어서 어떤 컴포넌트가 1번인지 2번인지 확인하기 어렵습니다.\nset_display_name # 이를 위한 것이 바로 display_name 입니다.\n설정하는 방법은 파이프라인에서 컴포넌트에 다음과 같이 set_display_name attribute를 이용하면 됩니다.\nimport kfp from kfp.components import create_component_from_func from kfp.dsl import pipeline @create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number @create_component_from_func def sum_and_print_numbers(number_1: int, number_2: int): print(number_1 + number_2) @pipeline(name=\u0026#34;example_pipeline\u0026#34;) def example_pipeline(number_1: int, number_2: int): number_1_result = print_and_return_number(number_1).set_display_name(\u0026#34;This is number 1\u0026#34;) number_2_result = print_and_return_number(number_2).set_display_name(\u0026#34;This is number 2\u0026#34;) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ).set_display_name(\u0026#34;This is sum of number 1 and number 2\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(example_pipeline, \u0026#34;example_pipeline.yaml\u0026#34;) 이 스크립트를 실행해서 나온 example_pipeline.yaml을 확인하면 다음과 같습니다.\n  example_pipeline.yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: example-pipeline- annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.9, pipelines.kubeflow.org/pipeline_compilation_time: \u0026#39;2021-12-09T18:11:43.193190\u0026#39;, pipelines.kubeflow.org/pipeline_spec: \u0026#39;{\u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;number_1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;number_2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;example_pipeline\u0026#34;}\u0026#39;} labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.9} spec: entrypoint: example-pipeline templates: - name: example-pipeline inputs: parameters: - {name: number_1} - {name: number_2} dag: tasks: - name: print-and-return-number template: print-and-return-number arguments: parameters: - {name: number_1, value: \u0026#39;{{inputs.parameters.number_1}}\u0026#39;} - name: print-and-return-number-2 template: print-and-return-number-2 arguments: parameters: - {name: number_2, value: \u0026#39;{{inputs.parameters.number_2}}\u0026#39;} - name: sum-and-print-numbers template: sum-and-print-numbers dependencies: [print-and-return-number, print-and-return-number-2] arguments: parameters: - {name: print-and-return-number-2-Output, value: \u0026#39;{{tasks.print-and-return-number-2.outputs.parameters.print-and-return-number-2-Output}}\u0026#39;} - {name: print-and-return-number-Output, value: \u0026#39;{{tasks.print-and-return-number.outputs.parameters.print-and-return-number-Output}}\u0026#39;} - name: print-and-return-number container: args: [--number, \u0026#39;{{inputs.parameters.number_1}}\u0026#39;, \u0026#39;----output-paths\u0026#39;, /tmp/outputs/Output/data] command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def print_and_return_number(number): print(number) return number def _serialize_int(int_value: int) -\u0026gt; str: if isinstance(int_value, str): return int_value if not isinstance(int_value, int): raise TypeError(\u0026#39;Value \u0026#34;{}\u0026#34; has type \u0026#34;{}\u0026#34; instead of int.\u0026#39;.format( str(int_value), str(type(int_value)))) return str(int_value) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Print and return number\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--number\u0026#34;, dest=\u0026#34;number\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;----output-paths\u0026#34;, dest=\u0026#34;_output_paths\u0026#34;, type=str, nargs=1) _parsed_args = vars(_parser.parse_args()) _output_files = _parsed_args.pop(\u0026#34;_output_paths\u0026#34;, []) _outputs = print_and_return_number(**_parsed_args) _outputs = [_outputs] _output_serializers = [ _serialize_int, ] import os for idx, output_file in enumerate(_output_files): try: os.makedirs(os.path.dirname(output_file)) except OSError: pass with open(output_file, \u0026#39;w\u0026#39;) as f: f.write(_output_serializers[idx](_outputs[idx])) image: python:3.7 inputs: parameters: - {name: number_1} outputs: parameters: - name: print-and-return-number-Output valueFrom: {path: /tmp/outputs/Output/data} artifacts: - {name: print-and-return-number-Output, path: /tmp/outputs/Output/data} metadata: annotations: {pipelines.kubeflow.org/task_display_name: This is number 1, pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--number\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;number\u0026#34;}, \u0026#34;----output-paths\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;Output\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def print_and_return_number(number):\\n print(number)\\n return number\\n\\ndef _serialize_int(int_value: int) -\u0026gt; str:\\n if isinstance(int_value, str):\\n return int_value\\n if not isinstance(int_value, int):\\n raise TypeError(\u0026#39;\u0026#39;Value \\\u0026#34;{}\\\u0026#34; has type \\\u0026#34;{}\\\u0026#34; instead of int.\u0026#39;\u0026#39;.format(\\n str(int_value), str(type(int_value))))\\n return str(int_value)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Print and return number\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--number\\\u0026#34;, dest=\\\u0026#34;number\\\u0026#34;, type=int, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;----output-paths\\\u0026#34;, dest=\\\u0026#34;_output_paths\\\u0026#34;, type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\\\u0026#34;_output_paths\\\u0026#34;, [])\\n\\n_outputs = print_and_return_number(**_parsed_args)\\n\\n_outputs = [_outputs]\\n\\n_output_serializers = [\\n _serialize_int,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n try:\\n os.makedirs(os.path.dirname(output_file))\\n except OSError:\\n pass\\n with open(output_file, \u0026#39;\u0026#39;w\u0026#39;\u0026#39;) as f:\\n f.write(_output_serializers[idx](_outputs[idx]))\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;Print and return number\u0026#34;, \u0026#34;outputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;Output\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}]}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;, pipelines.kubeflow.org/arguments.parameters: \u0026#39;{\u0026#34;number\u0026#34;: \u0026#34;{{inputs.parameters.number_1}}\u0026#34;}\u0026#39;} labels: pipelines.kubeflow.org/kfp_sdk_version: 1.8.9 pipelines.kubeflow.org/pipeline-sdk-type: kfp pipelines.kubeflow.org/enable_caching: \u0026#34;true\u0026#34; - name: print-and-return-number-2 container: args: [--number, \u0026#39;{{inputs.parameters.number_2}}\u0026#39;, \u0026#39;----output-paths\u0026#39;, /tmp/outputs/Output/data] command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def print_and_return_number(number): print(number) return number def _serialize_int(int_value: int) -\u0026gt; str: if isinstance(int_value, str): return int_value if not isinstance(int_value, int): raise TypeError(\u0026#39;Value \u0026#34;{}\u0026#34; has type \u0026#34;{}\u0026#34; instead of int.\u0026#39;.format( str(int_value), str(type(int_value)))) return str(int_value) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Print and return number\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--number\u0026#34;, dest=\u0026#34;number\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;----output-paths\u0026#34;, dest=\u0026#34;_output_paths\u0026#34;, type=str, nargs=1) _parsed_args = vars(_parser.parse_args()) _output_files = _parsed_args.pop(\u0026#34;_output_paths\u0026#34;, []) _outputs = print_and_return_number(**_parsed_args) _outputs = [_outputs] _output_serializers = [ _serialize_int, ] import os for idx, output_file in enumerate(_output_files): try: os.makedirs(os.path.dirname(output_file)) except OSError: pass with open(output_file, \u0026#39;w\u0026#39;) as f: f.write(_output_serializers[idx](_outputs[idx])) image: python:3.7 inputs: parameters: - {name: number_2} outputs: parameters: - name: print-and-return-number-2-Output valueFrom: {path: /tmp/outputs/Output/data} artifacts: - {name: print-and-return-number-2-Output, path: /tmp/outputs/Output/data} metadata: annotations: {pipelines.kubeflow.org/task_display_name: This is number 2, pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--number\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;number\u0026#34;}, \u0026#34;----output-paths\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;Output\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def print_and_return_number(number):\\n print(number)\\n return number\\n\\ndef _serialize_int(int_value: int) -\u0026gt; str:\\n if isinstance(int_value, str):\\n return int_value\\n if not isinstance(int_value, int):\\n raise TypeError(\u0026#39;\u0026#39;Value \\\u0026#34;{}\\\u0026#34; has type \\\u0026#34;{}\\\u0026#34; instead of int.\u0026#39;\u0026#39;.format(\\n str(int_value), str(type(int_value))))\\n return str(int_value)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Print and return number\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--number\\\u0026#34;, dest=\\\u0026#34;number\\\u0026#34;, type=int, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;----output-paths\\\u0026#34;, dest=\\\u0026#34;_output_paths\\\u0026#34;, type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\\\u0026#34;_output_paths\\\u0026#34;, [])\\n\\n_outputs = print_and_return_number(**_parsed_args)\\n\\n_outputs = [_outputs]\\n\\n_output_serializers = [\\n _serialize_int,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n try:\\n os.makedirs(os.path.dirname(output_file))\\n except OSError:\\n pass\\n with open(output_file, \u0026#39;\u0026#39;w\u0026#39;\u0026#39;) as f:\\n f.write(_output_serializers[idx](_outputs[idx]))\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;Print and return number\u0026#34;, \u0026#34;outputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;Output\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}]}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;, pipelines.kubeflow.org/arguments.parameters: \u0026#39;{\u0026#34;number\u0026#34;: \u0026#34;{{inputs.parameters.number_2}}\u0026#34;}\u0026#39;} labels: pipelines.kubeflow.org/kfp_sdk_version: 1.8.9 pipelines.kubeflow.org/pipeline-sdk-type: kfp pipelines.kubeflow.org/enable_caching: \u0026#34;true\u0026#34; - name: sum-and-print-numbers container: args: [--number-1, \u0026#39;{{inputs.parameters.print-and-return-number-Output}}\u0026#39;, --number-2, \u0026#39;{{inputs.parameters.print-and-return-number-2-Output}}\u0026#39;] command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def sum_and_print_numbers(number_1, number_2): print(number_1 + number_2) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Sum and print numbers\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--number-1\u0026#34;, dest=\u0026#34;number_1\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--number-2\u0026#34;, dest=\u0026#34;number_2\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = sum_and_print_numbers(**_parsed_args) image: python:3.7 inputs: parameters: - {name: print-and-return-number-2-Output} - {name: print-and-return-number-Output} metadata: annotations: {pipelines.kubeflow.org/task_display_name: This is sum of number 1 and number 2, pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--number-1\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;number_1\u0026#34;}, \u0026#34;--number-2\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;number_2\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def sum_and_print_numbers(number_1, number_2):\\n print(number_1 + number_2)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Sum and print numbers\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--number-1\\\u0026#34;, dest=\\\u0026#34;number_1\\\u0026#34;, type=int, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--number-2\\\u0026#34;, dest=\\\u0026#34;number_2\\\u0026#34;, type=int, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = sum_and_print_numbers(**_parsed_args)\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;number_1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;number_2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Integer\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;Sum and print numbers\u0026#34;}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;, pipelines.kubeflow.org/arguments.parameters: \u0026#39;{\u0026#34;number_1\u0026#34;: \u0026#34;{{inputs.parameters.print-and-return-number-Output}}\u0026#34;, \u0026#34;number_2\u0026#34;: \u0026#34;{{inputs.parameters.print-and-return-number-2-Output}}\u0026#34;}\u0026#39;} labels: pipelines.kubeflow.org/kfp_sdk_version: 1.8.9 pipelines.kubeflow.org/pipeline-sdk-type: kfp pipelines.kubeflow.org/enable_caching: \u0026#34;true\u0026#34; arguments: parameters: - {name: number_1} - {name: number_2} serviceAccountName: pipeline-runner   이 전의 파일과 비교하면 pipelines.kubeflow.org/task_display_name key가 새로 생성되었습니다.\nUI in Kubeflow # 위에서 만든 파일을 이용해 이전에 생성한 파이프라인의 버전을 올리겠습니다.\n그러면 위와 같이 설정한 이름이 노출되는 것을 확인할 수 있습니다.\nResources # GPU # 특별한 설정이 없다면 파이프라인은 컴포넌트를 쿠버네티스 파드(pod)로 실행할 때, 기본 리소스 스펙으로 실행하게 됩니다.\n만약 GPU를 사용해 모델을 학습해야 할 때 쿠버네티스상에서 GPU를 할당받지 못해 제대로 학습이 이루어지지 않습니다.\n이를 위해 set_gpu_limit() attribute을 이용해 설정할 수 있습니다.\nimport kfp from kfp.components import create_component_from_func from kfp.dsl import pipeline @create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number @create_component_from_func def sum_and_print_numbers(number_1: int, number_2: int): print(number_1 + number_2) @pipeline(name=\u0026#34;example_pipeline\u0026#34;) def example_pipeline(number_1: int, number_2: int): number_1_result = print_and_return_number(number_1).set_display_name(\u0026#34;This is number 1\u0026#34;) number_2_result = print_and_return_number(number_2).set_display_name(\u0026#34;This is number 2\u0026#34;) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ).set_display_name(\u0026#34;This is sum of number 1 and number 2\u0026#34;).set_gpu_limit(1) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(example_pipeline, \u0026#34;example_pipeline.yaml\u0026#34;) 위의 스크립트를 실행하면 생성된 파일에서 sum-and-print-numbers를 자세히 보면 resources에 {nvidia.com/gpu: 1} 도 추가된 것을 볼 수 있습니다. 이를 통해 GPU를 할당받을 수 있습니다.\n- name: sum-and-print-numbers container: args: [--number-1, \u0026#39;{{inputs.parameters.print-and-return-number-Output}}\u0026#39;, --number-2, \u0026#39;{{inputs.parameters.print-and-return-number-2-Output}}\u0026#39;] command: - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def sum_and_print_numbers(number_1, number_2): print(number_1 + number_2) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Sum and print numbers\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--number-1\u0026#34;, dest=\u0026#34;number_1\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--number-2\u0026#34;, dest=\u0026#34;number_2\u0026#34;, type=int, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = sum_and_print_numbers(**_parsed_args) image: python:3.7 resources: limits: {nvidia.com/gpu: 1} CPU # cpu의 개수를 정하기 위해서 이용하는 함수는 .set_cpu_limit() attribute을 이용해 설정할 수 있습니다.\ngpu와는 다른 점은 int가 아닌 string으로 입력해야 한다는 점입니다.\nimport kfp from kfp.components import create_component_from_func from kfp.dsl import pipeline @create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number @create_component_from_func def sum_and_print_numbers(number_1: int, number_2: int): print(number_1 + number_2) @pipeline(name=\u0026#34;example_pipeline\u0026#34;) def example_pipeline(number_1: int, number_2: int): number_1_result = print_and_return_number(number_1).set_display_name(\u0026#34;This is number 1\u0026#34;) number_2_result = print_and_return_number(number_2).set_display_name(\u0026#34;This is number 2\u0026#34;) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ).set_display_name(\u0026#34;This is sum of number 1 and number 2\u0026#34;).set_gpu_limit(1).set_cpu_limit(\u0026#34;16\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(example_pipeline, \u0026#34;example_pipeline.yaml\u0026#34;) 바뀐 부분만 확인하면 다음과 같습니다.\nresources: limits: {nvidia.com/gpu: 1, cpu: \u0026#39;16\u0026#39;} Memory # 메모리는 .set_memory_limit() attribute을 이용해 설정할 수 있습니다.\nimport kfp from kfp.components import create_component_from_func from kfp.dsl import pipeline @create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number @create_component_from_func def sum_and_print_numbers(number_1: int, number_2: int): print(number_1 + number_2) @pipeline(name=\u0026#34;example_pipeline\u0026#34;) def example_pipeline(number_1: int, number_2: int): number_1_result = print_and_return_number(number_1).set_display_name(\u0026#34;This is number 1\u0026#34;) number_2_result = print_and_return_number(number_2).set_display_name(\u0026#34;This is number 2\u0026#34;) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ).set_display_name(\u0026#34;This is sum of number 1 and number 2\u0026#34;).set_gpu_limit(1).set_memory_limit(\u0026#34;1G\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(example_pipeline, \u0026#34;example_pipeline.yaml\u0026#34;) 바뀐 부분만 확인하면 다음과 같습니다.\nresources: limits: {nvidia.com/gpu: 1, memory: 1G} "}),e.add({id:38,href:"/docs/kubeflow/advanced-run/",title:"11. Pipeline - Run Result",description:"Run Result # Run 실행 결과를 눌러보면 3개의 탭이 존재합니다. 각각 Graph, Run output, Config 입니다.\nGraph # 그래프에서는 실행된 컴포넌트를 누르면 컴포넌트의 실행 정보를 확인할 수 있습니다.\nInput/Output # Input/Output 탭은 컴포넌트에서 사용한 Config들과 Input, Output Artifacts를 확인하고 다운로드 받을 수 있습니다.\nLogs # Logs에서는 파이썬 코드 실행 중 나오는 모든 stdout을 확인할 수 있습니다. 다만 pod은 일정 시간이 지난 후 지워지기 때문에 일정 시간이 지나면 이 탭에서는 확인할 수 없습니다.",content:"Run Result # Run 실행 결과를 눌러보면 3개의 탭이 존재합니다. 각각 Graph, Run output, Config 입니다.\nGraph # 그래프에서는 실행된 컴포넌트를 누르면 컴포넌트의 실행 정보를 확인할 수 있습니다.\nInput/Output # Input/Output 탭은 컴포넌트에서 사용한 Config들과 Input, Output Artifacts를 확인하고 다운로드 받을 수 있습니다.\nLogs # Logs에서는 파이썬 코드 실행 중 나오는 모든 stdout을 확인할 수 있습니다. 다만 pod은 일정 시간이 지난 후 지워지기 때문에 일정 시간이 지나면 이 탭에서는 확인할 수 없습니다. 이때는 Output artifacts의 main-logs에서 확인할 수 있습니다.\nVisualizations # Visualizations에서는 컴포넌트에서 생성된 플랏을 보여줍니다.\n플랏을 생성하기 위해서는 mlpipeline_ui_metadata: OutputPath(\u0026quot;UI_Metadata\u0026quot;) argument로 보여주고 싶은 값을 저장하면 됩니다. 이 때 플랏의 형태는 html 포맷이어야 합니다. 변환하는 과정은 다음과 같습니다.\n@partial( create_component_from_func, packages_to_install=[\u0026#34;matplotlib\u0026#34;], ) def plot_linear( mlpipeline_ui_metadata: OutputPath(\u0026#34;UI_Metadata\u0026#34;) ): import base64 import json from io import BytesIO import matplotlib.pyplot as plt plt.plot(x=[1, 2, 3], y=[1, 2,3]) tmpfile = BytesIO() plt.savefig(tmpfile, format=\u0026#34;png\u0026#34;) encoded = base64.b64encode(tmpfile.getvalue()).decode(\u0026#34;utf-8\u0026#34;) html = f\u0026#34;\u0026lt;img src=\u0026#39;data:image/png;base64,{encoded}\u0026#39;\u0026gt;\u0026#34; metadata = { \u0026#34;outputs\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;web-app\u0026#34;, \u0026#34;storage\u0026#34;: \u0026#34;inline\u0026#34;, \u0026#34;source\u0026#34;: html, }, ], } with open(mlpipeline_ui_metadata, \u0026#34;w\u0026#34;) as html_writer: json.dump(metadata, html_writer) 파이프라인으로 작성하면 다음과 같이 됩니다.\nfrom functools import partial import kfp from kfp.components import create_component_from_func, OutputPath from kfp.dsl import pipeline @partial( create_component_from_func, packages_to_install=[\u0026#34;matplotlib\u0026#34;], ) def plot_linear(mlpipeline_ui_metadata: OutputPath(\u0026#34;UI_Metadata\u0026#34;)): import base64 import json from io import BytesIO import matplotlib.pyplot as plt plt.plot([1, 2, 3], [1, 2, 3]) tmpfile = BytesIO() plt.savefig(tmpfile, format=\u0026#34;png\u0026#34;) encoded = base64.b64encode(tmpfile.getvalue()).decode(\u0026#34;utf-8\u0026#34;) html = f\u0026#34;\u0026lt;img src=\u0026#39;data:image/png;base64,{encoded}\u0026#39;\u0026gt;\u0026#34; metadata = { \u0026#34;outputs\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;web-app\u0026#34;, \u0026#34;storage\u0026#34;: \u0026#34;inline\u0026#34;, \u0026#34;source\u0026#34;: html, }, ], } with open(mlpipeline_ui_metadata, \u0026#34;w\u0026#34;) as html_writer: json.dump(metadata, html_writer) @pipeline(name=\u0026#34;plot_pipeline\u0026#34;) def plot_pipeline(): plot_linear() if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(plot_pipeline, \u0026#34;plot_pipeline.yaml\u0026#34;) 이 스크립트를 실행해서 나온 plot_pipeline.yaml을 확인하면 다음과 같습니다.\n  plot_pipeline.yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: plot-pipeline- annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.9, pipelines.kubeflow.org/pipeline_compilation_time: \u0026#39;2 022-01-17T13:31:32.963214\u0026#39;, pipelines.kubeflow.org/pipeline_spec: \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;plot_pipeline\u0026#34;}\u0026#39;} labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.9} spec: entrypoint: plot-pipeline templates: - name: plot-linear container: args: [--mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data] command: - sh - -c - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;matplotlib\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;matplotlib\u0026#39; --user) \u0026amp;\u0026amp; \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def _make_parent_dirs_and_return_path(file_path: str): import os os.makedirs(os.path.dirname(file_path), exist_ok=True) return file_path def plot_linear(mlpipeline_ui_metadata): import base64 import json from io import BytesIO import matplotlib.pyplot as plt plt.plot([1, 2, 3], [1, 2, 3]) tmpfile = BytesIO() plt.savefig(tmpfile, format=\u0026#34;png\u0026#34;) encoded = base64.b64encode(tmpfile.getvalue()).decode(\u0026#34;utf-8\u0026#34;) html = f\u0026#34;\u0026lt;img src=\u0026#39;data:image/png;base64,{encoded}\u0026#39;\u0026gt;\u0026#34; metadata = { \u0026#34;outputs\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;web-app\u0026#34;, \u0026#34;storage\u0026#34;: \u0026#34;inline\u0026#34;, \u0026#34;source\u0026#34;: html, }, ], } with open(mlpipeline_ui_metadata, \u0026#34;w\u0026#34;) as html_writer: json.dump(metadata, html_writer) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Plot linear\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--mlpipeline-ui-metadata\u0026#34;, dest=\u0026#34;mlpipeline_ui_metadata\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = plot_linear(**_parsed_args) image: python:3.7 outputs: artifacts: - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data} metadata: labels: pipelines.kubeflow.org/kfp_sdk_version: 1.8.9 pipelines.kubeflow.org/pipeline-sdk-type: kfp pipelines.kubeflow.org/enable_caching: \u0026#34;true\u0026#34; annotations: {pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--mlpipeline-ui-metadata\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;mlpipeline_ui_metadata\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;\u0026#39;matplotlib\u0026#39;\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;\u0026#39;matplotlib\u0026#39;\u0026#39; --user) \u0026amp;\u0026amp; \\\u0026#34;$0\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\u0026#34;, \u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def _make_parent_dirs_and_return_path(file_path: str):\\n import os\\n os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n return file_path\\n\\ndef plot_linear(mlpipeline_ui_metadata):\\n import base64\\n import json\\n from io import BytesIO\\n\\n import matplotlib.pyplot as plt\\n\\n plt.plot([1, 2, 3], [1, 2, 3])\\n\\n tmpfile = BytesIO()\\n plt.savefig(tmpfile, format=\\\u0026#34;png\\\u0026#34;)\\n encoded = base64.b64encode(tmpfile.getvalue()).decode(\\\u0026#34;utf-8\\\u0026#34;)\\n\\n html = f\\\u0026#34;\u0026lt;img src=\u0026#39;\u0026#39;data:image/png;base64,{encoded}\u0026#39;\u0026#39;\u0026gt;\\\u0026#34;\\n metadata = {\\n \\\u0026#34;outputs\\\u0026#34;: [\\n {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;web-app\\\u0026#34;,\\n \\\u0026#34;storage\\\u0026#34;: \\\u0026#34;inline\\\u0026#34;,\\n \\\u0026#34;source\\\u0026#34;: html,\\n },\\n ],\\n }\\n with open(mlpipeline_ui_metadata, \\\u0026#34;w\\\u0026#34;) as html_writer:\\n json.dump(metadata, html_writer)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Plot linear\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--mlpipeline-ui-metadata\\\u0026#34;, dest=\\\u0026#34;mlpipeline_ui_metadata\\\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = plot_linear(**_parsed_args)\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;name\u0026#34;: \u0026#34;Plot linear\u0026#34;, \u0026#34;outputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;mlpipeline_ui_metadata\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;UI_Metadata\u0026#34;}]}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;} - name: plot-pipeline dag: tasks: - {name: plot-linear, template: plot-linear} arguments: parameters: [] serviceAccountName: pipeline-runner   실행 후 Visualization을 클릭합니다.\nRun output # Run output은 kubeflow에서 지정한 형태로 생긴 Artifacts를 모아서 보여주는 곳이며 평가 지표(Metric)를 보여줍니다.\n평가 지표(Metric)을 보여주기 위해서는 mlpipeline_metrics_path: OutputPath(\u0026quot;Metrics\u0026quot;) argument에 보여주고 싶은 이름과 값을 json 형태로 저장하면 됩니다. 예를 들어서 다음과 같이 작성할 수 있습니다.\n@create_component_from_func def show_metric_of_sum( number: int, mlpipeline_metrics_path: OutputPath(\u0026#34;Metrics\u0026#34;), ): import json metrics = { \u0026#34;metrics\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;sum_value\u0026#34;, \u0026#34;numberValue\u0026#34;: number, }, ], } with open(mlpipeline_metrics_path, \u0026#34;w\u0026#34;) as f: json.dump(metrics, f) 평가 지표를 생성하는 컴포넌트를 파이프라인에서 생성한 파이프라인에 추가 후 실행해 보겠습니다. 전체 파이프라인은 다음과 같습니다.\nimport kfp from kfp.components import create_component_from_func, OutputPath from kfp.dsl import pipeline @create_component_from_func def print_and_return_number(number: int) -\u0026gt; int: print(number) return number @create_component_from_func def sum_and_print_numbers(number_1: int, number_2: int) -\u0026gt; int: sum_number = number_1 + number_2 print(sum_number) return sum_number @create_component_from_func def show_metric_of_sum( number: int, mlpipeline_metrics_path: OutputPath(\u0026#34;Metrics\u0026#34;), ): import json metrics = { \u0026#34;metrics\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;sum_value\u0026#34;, \u0026#34;numberValue\u0026#34;: number, }, ], } with open(mlpipeline_metrics_path, \u0026#34;w\u0026#34;) as f: json.dump(metrics, f) @pipeline(name=\u0026#34;example_pipeline\u0026#34;) def example_pipeline(number_1: int, number_2: int): number_1_result = print_and_return_number(number_1) number_2_result = print_and_return_number(number_2) sum_result = sum_and_print_numbers( number_1=number_1_result.output, number_2=number_2_result.output ) show_metric_of_sum(sum_result.output) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(example_pipeline, \u0026#34;example_pipeline.yaml\u0026#34;) 실행 후 Run Output을 클릭하면 다음과 같이 나옵니다.\nConfig # Config에서는 파이프라인 Config로 입력받은 모든 값을 확인할 수 있습니다.\n"}),e.add({id:39,href:"/docs/kubeflow/advanced-mlflow/",title:"12. Component - MLFlow",description:"MLFlow Component # Advanced Usage Component 에서 학습한 모델이 API Deployment까지 이어지기 위해서는 MLFlow에 모델을 저장해야 합니다.\n이번 페이지에서는 MLFlow에 모델을 저장할 수 있는 컴포넌트를 작성하는 과정을 설명합니다.\nMLFlow in Local # MLFlow에서 모델을 저장하고 서빙에서 사용하기 위해서는 다음의 항목들이 필요합니다.\n model signature input_example conda_env  파이썬 코드를 통해서 MLFLow에 모델을 저장하는 과정에 대해서 알아보겠습니다.\n1. 모델 학습 # 아래 과정은 iris 데이터를 이용해 SVC 모델을 학습하는 과정입니다.\nimport pandas as pd from sklearn.",content:"MLFlow Component # Advanced Usage Component 에서 학습한 모델이 API Deployment까지 이어지기 위해서는 MLFlow에 모델을 저장해야 합니다.\n이번 페이지에서는 MLFlow에 모델을 저장할 수 있는 컴포넌트를 작성하는 과정을 설명합니다.\nMLFlow in Local # MLFlow에서 모델을 저장하고 서빙에서 사용하기 위해서는 다음의 항목들이 필요합니다.\n model signature input_example conda_env  파이썬 코드를 통해서 MLFLow에 모델을 저장하는 과정에 대해서 알아보겠습니다.\n1. 모델 학습 # 아래 과정은 iris 데이터를 이용해 SVC 모델을 학습하는 과정입니다.\nimport pandas as pd from sklearn.datasets import load_iris from sklearn.svm import SVC iris = load_iris() data = pd.DataFrame(iris[\u0026#34;data\u0026#34;], columns=iris[\u0026#34;feature_names\u0026#34;]) target = pd.DataFrame(iris[\u0026#34;target\u0026#34;], columns=[\u0026#34;target\u0026#34;]) clf = SVC(kernel=\u0026#34;rbf\u0026#34;) clf.fit(data, target) 2. MLFLow Infos # mlflow에 필요한 정보들을 만드는 과정입니다.\nfrom mlflow.models.signature import infer_signature from mlflow.utils.environment import _mlflow_conda_env input_example = data.sample(1) signature = infer_signature(data, clf.predict(data)) conda_env = _mlflow_conda_env(additional_pip_deps=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;]) 각 변수의 내용을 확인하면 다음과 같습니다.\n  input_example\n   sepal length (cm) sepal width (cm) petal length (cm) petal width (cm)     6.5 6.7 3.1 4.4      signature\ninputs: [\u0026#39;sepal length (cm)\u0026#39;: double, \u0026#39;sepal width (cm)\u0026#39;: double, \u0026#39;petal length (cm)\u0026#39;: double, \u0026#39;petal width (cm)\u0026#39;: double] outputs: [Tensor(\u0026#39;int64\u0026#39;, (-1,))]   conda_env\n{\u0026#39;name\u0026#39;: \u0026#39;mlflow-env\u0026#39;, \u0026#39;channels\u0026#39;: [\u0026#39;conda-forge\u0026#39;], \u0026#39;dependencies\u0026#39;: [\u0026#39;python=3.8.10\u0026#39;, \u0026#39;pip\u0026#39;, {\u0026#39;pip\u0026#39;: [\u0026#39;mlflow\u0026#39;, \u0026#39;dill\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;scikit-learn\u0026#39;]}]}   3. Save MLFLow Infos # 다음으로 학습한 정보들과 모델을 저장합니다. 학습한 모델이 sklearn 패키지를 이용하기 때문에 mlflow.sklearn 을 이용하면 쉽게 모델을 저장할 수 있습니다.\nfrom mlflow.sklearn import save_model save_model( sk_model=clf, path=\u0026#34;svc\u0026#34;, serialization_format=\u0026#34;cloudpickle\u0026#34;, conda_env=conda_env, signature=signature, input_example=input_example, ) 로컬에서 작업하면 다음과 같은 svc 폴더가 생기며 아래와 같은 파일들이 생성됩니다.\nls svc 위의 명령어를 실행하면 다음의 출력값을 확인할 수 있습니다.\nMLmodel conda.yaml input_example.json model.pkl requirements.txt 각 파일을 확인하면 다음과 같습니다.\n  MLmodel\nflavors: python_function: env: conda.yaml loader_module: mlflow.sklearn model_path: model.pkl python_version: 3.8.10 sklearn: pickled_model: model.pkl serialization_format: cloudpickle sklearn_version: 1.0.1 saved_input_example_info: artifact_path: input_example.json pandas_orient: split type: dataframe signature: inputs: \u0026#39;[{\u0026#34;name\u0026#34;: \u0026#34;sepal length (cm)\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;sepal width (cm)\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;petal length (cm)\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;petal width (cm)\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34;}]\u0026#39; outputs: \u0026#39;[{\u0026#34;type\u0026#34;: \u0026#34;tensor\u0026#34;, \u0026#34;tensor-spec\u0026#34;: {\u0026#34;dtype\u0026#34;: \u0026#34;int64\u0026#34;, \u0026#34;shape\u0026#34;: [-1]}}]\u0026#39; utc_time_created: \u0026#39;2021-12-06 06:52:30.612810\u0026#39;   conda.yaml\nchannels: - conda-forge dependencies: - python=3.8.10 - pip - pip: - mlflow - dill - pandas - scikit-learn name: mlflow-env   input_example.json\n{ \u0026#34;columns\u0026#34;: [ \u0026#34;sepal length (cm)\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;, \u0026#34;petal length (cm)\u0026#34;, \u0026#34;petal width (cm)\u0026#34; ], \u0026#34;data\u0026#34;: [ [6.7, 3.1, 4.4, 1.4] ] }   requirements.txt\nmlflow dill pandas scikit-learn   model.pkl\n  MLFlow on Server # 이제 저장된 모델을 mlflow 서버에 올리는 작업을 해보겠습니다.\nimport mlflow with mlflow.start_run(): mlflow.log_artifact(\u0026#34;svc/\u0026#34;) 저장하고 mlruns 가 생성된 경로에서 mlflow ui 명령어를 이용해 mlflow 서버와 대시보드를 띄웁니다. mlflow 대시보드에 접속하여 생성된 run을 클릭하면 다음과 같이 보입니다.\n(해당 화면은 mlflow 버전에 따라 다를 수 있습니다.) MLFlow Component # 이제 Kubeflow에서 재사용할 수 있는 컴포넌트를 작성해 보겠습니다.\n재사용할 수 있는 컴포넌트를 작성하는 방법은 크게 3가지가 있습니다.\n  모델을 학습하는 컴포넌트에서 필요한 환경을 저장 후 MLFlow 컴포넌트는 업로드만 담당\n  학습된 모델과 데이터를 MLFlow 컴포넌트에 전달 후 컴포넌트에서 저장과 업로드 담당\n  모델을 학습하는 컴포넌트에서 저장과 업로드를 담당\n  저희는 이 중 1번의 접근 방법을 통해 모델을 관리하려고 합니다. 이유는 MLFlow 모델을 업로드하는 코드는 바뀌지 않기 때문에 매번 3번처럼 컴포넌트 작성마다 작성할 필요는 없기 때문입니다.\n컴포넌트를 재활용하는 방법은 1번과 2번의 방법으로 가능합니다. 다만 2번의 경우 모델이 학습된 이미지와 패키지들을 전달해야 하므로 결국 컴포넌트에 대한 추가 정보를 전달해야 합니다.\n1번의 방법으로 진행하기 위해서는 학습하는 컴포넌트 또한 변경되어야 합니다. 모델을 저장하는데 필요한 환경들을 저장해주는 코드가 추가되어야 합니다.\nfrom functools import partial from kfp.components import InputPath, OutputPath, create_component_from_func @partial( create_component_from_func, packages_to_install=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;, \u0026#34;mlflow\u0026#34;], ) def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), input_example_path: OutputPath(\u0026#34;dill\u0026#34;), signature_path: OutputPath(\u0026#34;dill\u0026#34;), conda_env_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC from mlflow.models.signature import infer_signature from mlflow.utils.environment import _mlflow_conda_env train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) input_example = train_data.sample(1) with open(input_example_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(input_example, file_writer) signature = infer_signature(train_data, clf.predict(train_data)) with open(signature_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(signature, file_writer) conda_env = _mlflow_conda_env( additional_pip_deps=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;] ) with open(conda_env_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(conda_env, file_writer) 그리고 MLFlow에 업로드하는 컴포넌트를 작성합니다. 이 때 업로드되는 MLflow의 endpoint를 우리가 설치한 mlflow service 로 이어지게 설정해주어야 합니다.\n이 때 S3 Endpoint의 주소는 MLflow Server 설치 당시 설치한 minio의 쿠버네티스 서비스 DNS 네임을 활용합니다. 해당 service 는 kubeflow namespace에서 minio-service라는 이름으로 생성되었으므로, http://minio-service.kubeflow.svc:9000 로 설정합니다..\n이와 비슷하게 tracking_uri의 주소는 mlflow server의 쿠버네티스 서비스 DNS 네임을 활용하여, http://mlflow-server-service.mlflow-system.svc:5000 로 설정합니다.\nfrom functools import partial from kfp.components import InputPath, create_component_from_func @partial( create_component_from_func, packages_to_install=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;, \u0026#34;mlflow\u0026#34;, \u0026#34;boto3\u0026#34;], ) def upload_sklearn_model_to_mlflow( model_name: str, model_path: InputPath(\u0026#34;dill\u0026#34;), input_example_path: InputPath(\u0026#34;dill\u0026#34;), signature_path: InputPath(\u0026#34;dill\u0026#34;), conda_env_path: InputPath(\u0026#34;dill\u0026#34;), ): import os import dill from mlflow.sklearn import save_model from mlflow.tracking.client import MlflowClient os.environ[\u0026#34;MLFLOW_S3_ENDPOINT_URL\u0026#34;] = \u0026#34;http://minio-service.kubeflow.svc:9000\u0026#34; os.environ[\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;] = \u0026#34;minio\u0026#34; os.environ[\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;] = \u0026#34;minio123\u0026#34; client = MlflowClient(\u0026#34;http://mlflow-server-service.mlflow-system.svc:5000\u0026#34;) with open(model_path, mode=\u0026#34;rb\u0026#34;) as file_reader: clf = dill.load(file_reader) with open(input_example_path, \u0026#34;rb\u0026#34;) as file_reader: input_example = dill.load(file_reader) with open(signature_path, \u0026#34;rb\u0026#34;) as file_reader: signature = dill.load(file_reader) with open(conda_env_path, \u0026#34;rb\u0026#34;) as file_reader: conda_env = dill.load(file_reader) save_model( sk_model=clf, path=model_name, serialization_format=\u0026#34;cloudpickle\u0026#34;, conda_env=conda_env, signature=signature, input_example=input_example, ) run = client.create_run(experiment_id=\u0026#34;0\u0026#34;) client.log_artifact(run.info.run_id, model_name) MLFlow Pipeline # 이제 작성한 컴포넌트들을 연결해서 파이프라인으로 만들어 보겠습니다.\nData Component # 모델을 학습할 때 쓸 데이터는 sklearn의 iris 입니다. 데이터를 생성하는 컴포넌트를 작성합니다.\nfrom functools import partial from kfp.components import InputPath, OutputPath, create_component_from_func @partial( create_component_from_func, packages_to_install=[\u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;], ) def load_iris_data( data_path: OutputPath(\u0026#34;csv\u0026#34;), target_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.DataFrame(iris[\u0026#34;data\u0026#34;], columns=iris[\u0026#34;feature_names\u0026#34;]) target = pd.DataFrame(iris[\u0026#34;target\u0026#34;], columns=[\u0026#34;target\u0026#34;]) data.to_csv(data_path, index=False) target.to_csv(target_path, index=False) Pipeline # 파이프라인 코드는 다음과 같이 작성할 수 있습니다.\nfrom kfp.dsl import pipeline @pipeline(name=\u0026#34;mlflow_pipeline\u0026#34;) def mlflow_pipeline(kernel: str, model_name: str): iris_data = load_iris_data() model = train_from_csv( train_data=iris_data.outputs[\u0026#34;data\u0026#34;], train_target=iris_data.outputs[\u0026#34;target\u0026#34;], kernel=kernel, ) _ = upload_sklearn_model_to_mlflow( model_name=model_name, model=model.outputs[\u0026#34;model\u0026#34;], input_example=model.outputs[\u0026#34;input_example\u0026#34;], signature=model.outputs[\u0026#34;signature\u0026#34;], conda_env=model.outputs[\u0026#34;conda_env\u0026#34;], ) Run # 위에서 작성된 컴포넌트와 파이프라인을 하나의 파이썬 파일에 정리하면 다음과 같습니다.\nfrom functools import partial import kfp from kfp.components import InputPath, OutputPath, create_component_from_func from kfp.dsl import pipeline @partial( create_component_from_func, packages_to_install=[\u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;], ) def load_iris_data( data_path: OutputPath(\u0026#34;csv\u0026#34;), target_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.DataFrame(iris[\u0026#34;data\u0026#34;], columns=iris[\u0026#34;feature_names\u0026#34;]) target = pd.DataFrame(iris[\u0026#34;target\u0026#34;], columns=[\u0026#34;target\u0026#34;]) data.to_csv(data_path, index=False) target.to_csv(target_path, index=False) @partial( create_component_from_func, packages_to_install=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;, \u0026#34;mlflow\u0026#34;], ) def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), input_example_path: OutputPath(\u0026#34;dill\u0026#34;), signature_path: OutputPath(\u0026#34;dill\u0026#34;), conda_env_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC from mlflow.models.signature import infer_signature from mlflow.utils.environment import _mlflow_conda_env train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) input_example = train_data.sample(1) with open(input_example_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(input_example, file_writer) signature = infer_signature(train_data, clf.predict(train_data)) with open(signature_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(signature, file_writer) conda_env = _mlflow_conda_env( additional_pip_deps=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;] ) with open(conda_env_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(conda_env, file_writer) @partial( create_component_from_func, packages_to_install=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;, \u0026#34;mlflow\u0026#34;, \u0026#34;boto3\u0026#34;], ) def upload_sklearn_model_to_mlflow( model_name: str, model_path: InputPath(\u0026#34;dill\u0026#34;), input_example_path: InputPath(\u0026#34;dill\u0026#34;), signature_path: InputPath(\u0026#34;dill\u0026#34;), conda_env_path: InputPath(\u0026#34;dill\u0026#34;), ): import os import dill from mlflow.sklearn import save_model from mlflow.tracking.client import MlflowClient os.environ[\u0026#34;MLFLOW_S3_ENDPOINT_URL\u0026#34;] = \u0026#34;http://minio-service.kubeflow.svc:9000\u0026#34; os.environ[\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;] = \u0026#34;minio\u0026#34; os.environ[\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;] = \u0026#34;minio123\u0026#34; client = MlflowClient(\u0026#34;http://mlflow-server-service.mlflow-system.svc:5000\u0026#34;) with open(model_path, mode=\u0026#34;rb\u0026#34;) as file_reader: clf = dill.load(file_reader) with open(input_example_path, \u0026#34;rb\u0026#34;) as file_reader: input_example = dill.load(file_reader) with open(signature_path, \u0026#34;rb\u0026#34;) as file_reader: signature = dill.load(file_reader) with open(conda_env_path, \u0026#34;rb\u0026#34;) as file_reader: conda_env = dill.load(file_reader) save_model( sk_model=clf, path=model_name, serialization_format=\u0026#34;cloudpickle\u0026#34;, conda_env=conda_env, signature=signature, input_example=input_example, ) run = client.create_run(experiment_id=\u0026#34;0\u0026#34;) client.log_artifact(run.info.run_id, model_name) @pipeline(name=\u0026#34;mlflow_pipeline\u0026#34;) def mlflow_pipeline(kernel: str, model_name: str): iris_data = load_iris_data() model = train_from_csv( train_data=iris_data.outputs[\u0026#34;data\u0026#34;], train_target=iris_data.outputs[\u0026#34;target\u0026#34;], kernel=kernel, ) _ = upload_sklearn_model_to_mlflow( model_name=model_name, model=model.outputs[\u0026#34;model\u0026#34;], input_example=model.outputs[\u0026#34;input_example\u0026#34;], signature=model.outputs[\u0026#34;signature\u0026#34;], conda_env=model.outputs[\u0026#34;conda_env\u0026#34;], ) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(mlflow_pipeline, \u0026#34;mlflow_pipeline.yaml\u0026#34;)   mlflow_pipeline.yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: mlflow-pipeline- annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.10, pipelines.kubeflow.org/pipeline_compilation_time: \u0026#39;2022-01-19T14:14:11.999807\u0026#39;, pipelines.kubeflow.org/pipeline_spec: \u0026#39;{\u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;model_name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;mlflow_pipeline\u0026#34;}\u0026#39;} labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.10} spec: entrypoint: mlflow-pipeline templates: - name: load-iris-data container: args: [--data, /tmp/outputs/data/data, --target, /tmp/outputs/target/data] command: - sh - -c - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;pandas\u0026#39; \u0026#39;scikit-learn\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;pandas\u0026#39; \u0026#39;scikit-learn\u0026#39; --user) \u0026amp;\u0026amp; \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def _make_parent_dirs_and_return_path(file_path: str): import os os.makedirs(os.path.dirname(file_path), exist_ok=True) return file_path def load_iris_data( data_path, target_path, ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.DataFrame(iris[\u0026#34;data\u0026#34;], columns=iris[\u0026#34;feature_names\u0026#34;]) target = pd.DataFrame(iris[\u0026#34;target\u0026#34;], columns=[\u0026#34;target\u0026#34;]) data.to_csv(data_path, index=False) target.to_csv(target_path, index=False) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Load iris data\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--data\u0026#34;, dest=\u0026#34;data_path\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--target\u0026#34;, dest=\u0026#34;target_path\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = load_iris_data(**_parsed_args) image: python:3.7 outputs: artifacts: - {name: load-iris-data-data, path: /tmp/outputs/data/data} - {name: load-iris-data-target, path: /tmp/outputs/target/data} metadata: labels: pipelines.kubeflow.org/kfp_sdk_version: 1.8.10 pipelines.kubeflow.org/pipeline-sdk-type: kfp pipelines.kubeflow.org/enable_caching: \u0026#34;true\u0026#34; annotations: {pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--data\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;data\u0026#34;}, \u0026#34;--target\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;target\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;\u0026#39;pandas\u0026#39;\u0026#39; \u0026#39;\u0026#39;scikit-learn\u0026#39;\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;\u0026#39;pandas\u0026#39;\u0026#39; \u0026#39;\u0026#39;scikit-learn\u0026#39;\u0026#39; --user) \u0026amp;\u0026amp; \\\u0026#34;$0\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\u0026#34;, \u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def _make_parent_dirs_and_return_path(file_path: str):\\n import os\\n os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n return file_path\\n\\ndef load_iris_data(\\n data_path,\\n target_path,\\n):\\n import pandas as pd\\n from sklearn.datasets import load_iris\\n\\n iris = load_iris()\\n\\n data = pd.DataFrame(iris[\\\u0026#34;data\\\u0026#34;], columns=iris[\\\u0026#34;feature_names\\\u0026#34;])\\n target = pd.DataFrame(iris[\\\u0026#34;target\\\u0026#34;], columns=[\\\u0026#34;target\\\u0026#34;])\\n\\n data.to_csv(data_path, index=False)\\n target.to_csv(target_path, index=False)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Load iris data\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--data\\\u0026#34;, dest=\\\u0026#34;data_path\\\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--target\\\u0026#34;, dest=\\\u0026#34;target_path\\\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = load_iris_data(**_parsed_args)\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;name\u0026#34;: \u0026#34;Load iris data\u0026#34;, \u0026#34;outputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;csv\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;target\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;csv\u0026#34;}]}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;} - name: mlflow-pipeline inputs: parameters: - {name: kernel} - {name: model_name} dag: tasks: - {name: load-iris-data, template: load-iris-data} - name: train-from-csv template: train-from-csv dependencies: [load-iris-data] arguments: parameters: - {name: kernel, value: \u0026#39;{{inputs.parameters.kernel}}\u0026#39;} artifacts: - {name: load-iris-data-data, from: \u0026#39;{{tasks.load-iris-data.outputs.artifacts.load-iris-data-data}}\u0026#39;} - {name: load-iris-data-target, from: \u0026#39;{{tasks.load-iris-data.outputs.artifacts.load-iris-data-target}}\u0026#39;} - name: upload-sklearn-model-to-mlflow template: upload-sklearn-model-to-mlflow dependencies: [train-from-csv] arguments: parameters: - {name: model_name, value: \u0026#39;{{inputs.parameters.model_name}}\u0026#39;} artifacts: - {name: train-from-csv-conda_env, from: \u0026#39;{{tasks.train-from-csv.outputs.artifacts.train-from-csv-conda_env}}\u0026#39;} - {name: train-from-csv-input_example, from: \u0026#39;{{tasks.train-from-csv.outputs.artifacts.train-from-csv-input_example}}\u0026#39;} - {name: train-from-csv-model, from: \u0026#39;{{tasks.train-from-csv.outputs.artifacts.train-from-csv-model}}\u0026#39;} - {name: train-from-csv-signature, from: \u0026#39;{{tasks.train-from-csv.outputs.artifacts.train-from-csv-signature}}\u0026#39;} - name: train-from-csv container: args: [--train-data, /tmp/inputs/train_data/data, --train-target, /tmp/inputs/train_target/data, --kernel, \u0026#39;{{inputs.parameters.kernel}}\u0026#39;, --model, /tmp/outputs/model/data, --input-example, /tmp/outputs/input_example/data, --signature, /tmp/outputs/signature/data, --conda-env, /tmp/outputs/conda_env/data] command: - sh - -c - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;dill\u0026#39; \u0026#39;pandas\u0026#39; \u0026#39;scikit-learn\u0026#39; \u0026#39;mlflow\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;dill\u0026#39; \u0026#39;pandas\u0026#39; \u0026#39;scikit-learn\u0026#39; \u0026#39;mlflow\u0026#39; --user) \u0026amp;\u0026amp; \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def _make_parent_dirs_and_return_path(file_path: str): import os os.makedirs(os.path.dirname(file_path), exist_ok=True) return file_path def train_from_csv( train_data_path, train_target_path, model_path, input_example_path, signature_path, conda_env_path, kernel, ): import dill import pandas as pd from sklearn.svm import SVC from mlflow.models.signature import infer_signature from mlflow.utils.environment import _mlflow_conda_env train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) input_example = train_data.sample(1) with open(input_example_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(input_example, file_writer) signature = infer_signature(train_data, clf.predict(train_data)) with open(signature_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(signature, file_writer) conda_env = _mlflow_conda_env( additional_pip_deps=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;] ) with open(conda_env_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(conda_env, file_writer) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Train from csv\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--train-data\u0026#34;, dest=\u0026#34;train_data_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--train-target\u0026#34;, dest=\u0026#34;train_target_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--kernel\u0026#34;, dest=\u0026#34;kernel\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--model\u0026#34;, dest=\u0026#34;model_path\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--input-example\u0026#34;, dest=\u0026#34;input_example_path\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--signature\u0026#34;, dest=\u0026#34;signature_path\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--conda-env\u0026#34;, dest=\u0026#34;conda_env_path\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = train_from_csv(**_parsed_args) image: python:3.7 inputs: parameters: - {name: kernel} artifacts: - {name: load-iris-data-data, path: /tmp/inputs/train_data/data} - {name: load-iris-data-target, path: /tmp/inputs/train_target/data} outputs: artifacts: - {name: train-from-csv-conda_env, path: /tmp/outputs/conda_env/data} - {name: train-from-csv-input_example, path: /tmp/outputs/input_example/data} - {name: train-from-csv-model, path: /tmp/outputs/model/data} - {name: train-from-csv-signature, path: /tmp/outputs/signature/data} metadata: labels: pipelines.kubeflow.org/kfp_sdk_version: 1.8.10 pipelines.kubeflow.org/pipeline-sdk-type: kfp pipelines.kubeflow.org/enable_caching: \u0026#34;true\u0026#34; annotations: {pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--train-data\u0026#34;, {\u0026#34;inputPath\u0026#34;: \u0026#34;train_data\u0026#34;}, \u0026#34;--train-target\u0026#34;, {\u0026#34;inputPath\u0026#34;: \u0026#34;train_target\u0026#34;}, \u0026#34;--kernel\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;kernel\u0026#34;}, \u0026#34;--model\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;model\u0026#34;}, \u0026#34;--input-example\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;input_example\u0026#34;}, \u0026#34;--signature\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;signature\u0026#34;}, \u0026#34;--conda-env\u0026#34;, {\u0026#34;outputPath\u0026#34;: \u0026#34;conda_env\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;\u0026#39;dill\u0026#39;\u0026#39; \u0026#39;\u0026#39;pandas\u0026#39;\u0026#39; \u0026#39;\u0026#39;scikit-learn\u0026#39;\u0026#39; \u0026#39;\u0026#39;mlflow\u0026#39;\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;\u0026#39;dill\u0026#39;\u0026#39; \u0026#39;\u0026#39;pandas\u0026#39;\u0026#39; \u0026#39;\u0026#39;scikit-learn\u0026#39;\u0026#39; \u0026#39;\u0026#39;mlflow\u0026#39;\u0026#39; --user) \u0026amp;\u0026amp; \\\u0026#34;$0\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\u0026#34;, \u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def _make_parent_dirs_and_return_path(file_path: str):\\n import os\\n os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n return file_path\\n\\ndef train_from_csv(\\n train_data_path,\\n train_target_path,\\n model_path,\\n input_example_path,\\n signature_path,\\n conda_env_path,\\n kernel,\\n):\\n import dill\\n import pandas as pd\\n from sklearn.svm import SVC\\n\\n from mlflow.models.signature import infer_signature\\n from mlflow.utils.environment import _mlflow_conda_env\\n\\n train_data = pd.read_csv(train_data_path)\\n train_target = pd.read_csv(train_target_path)\\n\\n clf = SVC(kernel=kernel)\\n clf.fit(train_data, train_target)\\n\\n with open(model_path, mode=\\\u0026#34;wb\\\u0026#34;) as file_writer:\\n dill.dump(clf, file_writer)\\n\\n input_example = train_data.sample(1)\\n with open(input_example_path, \\\u0026#34;wb\\\u0026#34;) as file_writer:\\n dill.dump(input_example, file_writer)\\n\\n signature = infer_signature(train_data, clf.predict(train_data))\\n with open(signature_path, \\\u0026#34;wb\\\u0026#34;) as file_writer:\\n dill.dump(signature, file_writer)\\n\\n conda_env = _mlflow_conda_env(\\n additional_pip_deps=[\\\u0026#34;dill\\\u0026#34;, \\\u0026#34;pandas\\\u0026#34;, \\\u0026#34;scikit-learn\\\u0026#34;]\\n )\\n with open(conda_env_path, \\\u0026#34;wb\\\u0026#34;) as file_writer:\\n dill.dump(conda_env, file_writer)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Train from csv\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--train-data\\\u0026#34;, dest=\\\u0026#34;train_data_path\\\u0026#34;, type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--train-target\\\u0026#34;, dest=\\\u0026#34;train_target_path\\\u0026#34;, type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--kernel\\\u0026#34;, dest=\\\u0026#34;kernel\\\u0026#34;, type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--model\\\u0026#34;, dest=\\\u0026#34;model_path\\\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--input-example\\\u0026#34;, dest=\\\u0026#34;input_example_path\\\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--signature\\\u0026#34;, dest=\\\u0026#34;signature_path\\\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--conda-env\\\u0026#34;, dest=\\\u0026#34;conda_env_path\\\u0026#34;, type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = train_from_csv(**_parsed_args)\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;train_data\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;csv\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;train_target\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;csv\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;kernel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;Train from csv\u0026#34;, \u0026#34;outputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;model\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dill\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;input_example\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dill\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;signature\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dill\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;conda_env\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dill\u0026#34;}]}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;, pipelines.kubeflow.org/arguments.parameters: \u0026#39;{\u0026#34;kernel\u0026#34;: \u0026#34;{{inputs.parameters.kernel}}\u0026#34;}\u0026#39;} - name: upload-sklearn-model-to-mlflow container: args: [--model-name, \u0026#39;{{inputs.parameters.model_name}}\u0026#39;, --model, /tmp/inputs/model/data, --input-example, /tmp/inputs/input_example/data, --signature, /tmp/inputs/signature/data, --conda-env, /tmp/inputs/conda_env/data] command: - sh - -c - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;dill\u0026#39; \u0026#39;pandas\u0026#39; \u0026#39;scikit-learn\u0026#39; \u0026#39;mlflow\u0026#39; \u0026#39;boto3\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;dill\u0026#39; \u0026#39;pandas\u0026#39; \u0026#39;scikit-learn\u0026#39; \u0026#39;mlflow\u0026#39; \u0026#39;boto3\u0026#39; --user) \u0026amp;\u0026amp; \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; - sh - -ec - | program_path=$(mktemp) printf \u0026#34;%s\u0026#34; \u0026#34;$0\u0026#34; \u0026gt; \u0026#34;$program_path\u0026#34; python3 -u \u0026#34;$program_path\u0026#34; \u0026#34;$@\u0026#34; - | def upload_sklearn_model_to_mlflow( model_name, model_path, input_example_path, signature_path, conda_env_path, ): import os import dill from mlflow.sklearn import save_model from mlflow.tracking.client import MlflowClient os.environ[\u0026#34;MLFLOW_S3_ENDPOINT_URL\u0026#34;] = \u0026#34;http://minio-service.kubeflow.svc:9000\u0026#34; os.environ[\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;] = \u0026#34;minio\u0026#34; os.environ[\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;] = \u0026#34;minio123\u0026#34; client = MlflowClient(\u0026#34;http://mlflow-server-service.mlflow-system.svc:5000\u0026#34;) with open(model_path, mode=\u0026#34;rb\u0026#34;) as file_reader: clf = dill.load(file_reader) with open(input_example_path, \u0026#34;rb\u0026#34;) as file_reader: input_example = dill.load(file_reader) with open(signature_path, \u0026#34;rb\u0026#34;) as file_reader: signature = dill.load(file_reader) with open(conda_env_path, \u0026#34;rb\u0026#34;) as file_reader: conda_env = dill.load(file_reader) save_model( sk_model=clf, path=model_name, serialization_format=\u0026#34;cloudpickle\u0026#34;, conda_env=conda_env, signature=signature, input_example=input_example, ) run = client.create_run(experiment_id=\u0026#34;0\u0026#34;) client.log_artifact(run.info.run_id, model_name) import argparse _parser = argparse.ArgumentParser(prog=\u0026#39;Upload sklearn model to mlflow\u0026#39;, description=\u0026#39;\u0026#39;) _parser.add_argument(\u0026#34;--model-name\u0026#34;, dest=\u0026#34;model_name\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--model\u0026#34;, dest=\u0026#34;model_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--input-example\u0026#34;, dest=\u0026#34;input_example_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--signature\u0026#34;, dest=\u0026#34;signature_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parser.add_argument(\u0026#34;--conda-env\u0026#34;, dest=\u0026#34;conda_env_path\u0026#34;, type=str, required=True, default=argparse.SUPPRESS) _parsed_args = vars(_parser.parse_args()) _outputs = upload_sklearn_model_to_mlflow(**_parsed_args) image: python:3.7 inputs: parameters: - {name: model_name} artifacts: - {name: train-from-csv-conda_env, path: /tmp/inputs/conda_env/data} - {name: train-from-csv-input_example, path: /tmp/inputs/input_example/data} - {name: train-from-csv-model, path: /tmp/inputs/model/data} - {name: train-from-csv-signature, path: /tmp/inputs/signature/data} metadata: labels: pipelines.kubeflow.org/kfp_sdk_version: 1.8.10 pipelines.kubeflow.org/pipeline-sdk-type: kfp pipelines.kubeflow.org/enable_caching: \u0026#34;true\u0026#34; annotations: {pipelines.kubeflow.org/component_spec: \u0026#39;{\u0026#34;implementation\u0026#34;: {\u0026#34;container\u0026#34;: {\u0026#34;args\u0026#34;: [\u0026#34;--model-name\u0026#34;, {\u0026#34;inputValue\u0026#34;: \u0026#34;model_name\u0026#34;}, \u0026#34;--model\u0026#34;, {\u0026#34;inputPath\u0026#34;: \u0026#34;model\u0026#34;}, \u0026#34;--input-example\u0026#34;, {\u0026#34;inputPath\u0026#34;: \u0026#34;input_example\u0026#34;}, \u0026#34;--signature\u0026#34;, {\u0026#34;inputPath\u0026#34;: \u0026#34;signature\u0026#34;}, \u0026#34;--conda-env\u0026#34;, {\u0026#34;inputPath\u0026#34;: \u0026#34;conda_env\u0026#34;}], \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;\u0026#39;dill\u0026#39;\u0026#39; \u0026#39;\u0026#39;pandas\u0026#39;\u0026#39; \u0026#39;\u0026#39;scikit-learn\u0026#39;\u0026#39; \u0026#39;\u0026#39;mlflow\u0026#39;\u0026#39; \u0026#39;\u0026#39;boto3\u0026#39;\u0026#39; || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \u0026#39;\u0026#39;dill\u0026#39;\u0026#39; \u0026#39;\u0026#39;pandas\u0026#39;\u0026#39; \u0026#39;\u0026#39;scikit-learn\u0026#39;\u0026#39; \u0026#39;\u0026#39;mlflow\u0026#39;\u0026#39; \u0026#39;\u0026#39;boto3\u0026#39;\u0026#39; --user) \u0026amp;\u0026amp; \\\u0026#34;$0\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\u0026#34;, \u0026#34;sh\u0026#34;, \u0026#34;-ec\u0026#34;, \u0026#34;program_path=$(mktemp)\\nprintf \\\u0026#34;%s\\\u0026#34; \\\u0026#34;$0\\\u0026#34; \u0026gt; \\\u0026#34;$program_path\\\u0026#34;\\npython3 -u \\\u0026#34;$program_path\\\u0026#34; \\\u0026#34;$@\\\u0026#34;\\n\u0026#34;, \u0026#34;def upload_sklearn_model_to_mlflow(\\n model_name,\\n model_path,\\n input_example_path,\\n signature_path,\\n conda_env_path,\\n):\\n import os\\n import dill\\n from mlflow.sklearn import save_model\\n\\n from mlflow.tracking.client import MlflowClient\\n\\n os.environ[\\\u0026#34;MLFLOW_S3_ENDPOINT_URL\\\u0026#34;] = \\\u0026#34;http://minio-service.kubeflow.svc:9000\\\u0026#34;\\n os.environ[\\\u0026#34;AWS_ACCESS_KEY_ID\\\u0026#34;] = \\\u0026#34;minio\\\u0026#34;\\n os.environ[\\\u0026#34;AWS_SECRET_ACCESS_KEY\\\u0026#34;] = \\\u0026#34;minio123\\\u0026#34;\\n\\n client = MlflowClient(\\\u0026#34;http://mlflow-server-service.mlflow-system.svc:5000\\\u0026#34;)\\n\\n with open(model_path, mode=\\\u0026#34;rb\\\u0026#34;) as file_reader:\\n clf = dill.load(file_reader)\\n\\n with open(input_example_path, \\\u0026#34;rb\\\u0026#34;) as file_reader:\\n input_example = dill.load(file_reader)\\n\\n with open(signature_path, \\\u0026#34;rb\\\u0026#34;) as file_reader:\\n signature = dill.load(file_reader)\\n\\n with open(conda_env_path, \\\u0026#34;rb\\\u0026#34;) as file_reader:\\n conda_env = dill.load(file_reader)\\n\\n save_model(\\n sk_model=clf,\\n path=model_name,\\n serialization_format=\\\u0026#34;cloudpickle\\\u0026#34;,\\n conda_env=conda_env,\\n signature=signature,\\n input_example=input_example,\\n )\\n run = client.create_run(experiment_id=\\\u0026#34;0\\\u0026#34;)\\n client.log_artifact(run.info.run_id, model_name)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\u0026#39;\u0026#39;Upload sklearn model to mlflow\u0026#39;\u0026#39;, description=\u0026#39;\u0026#39;\u0026#39;\u0026#39;)\\n_parser.add_argument(\\\u0026#34;--model-name\\\u0026#34;, dest=\\\u0026#34;model_name\\\u0026#34;, type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--model\\\u0026#34;, dest=\\\u0026#34;model_path\\\u0026#34;, type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--input-example\\\u0026#34;, dest=\\\u0026#34;input_example_path\\\u0026#34;, type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--signature\\\u0026#34;, dest=\\\u0026#34;signature_path\\\u0026#34;, type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\u0026#34;--conda-env\\\u0026#34;, dest=\\\u0026#34;conda_env_path\\\u0026#34;, type=str, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = upload_sklearn_model_to_mlflow(**_parsed_args)\\n\u0026#34;], \u0026#34;image\u0026#34;: \u0026#34;python:3.7\u0026#34;}}, \u0026#34;inputs\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;model_name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;model\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dill\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;input_example\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dill\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;signature\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dill\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;conda_env\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dill\u0026#34;}], \u0026#34;name\u0026#34;: \u0026#34;Upload sklearn model to mlflow\u0026#34;}\u0026#39;, pipelines.kubeflow.org/component_ref: \u0026#39;{}\u0026#39;, pipelines.kubeflow.org/arguments.parameters: \u0026#39;{\u0026#34;model_name\u0026#34;: \u0026#34;{{inputs.parameters.model_name}}\u0026#34;}\u0026#39;} arguments: parameters: - {name: kernel} - {name: model_name} serviceAccountName: pipeline-runner   실행후 생성된 mlflow_pipeline.yaml 파일을 파이프라인 업로드한 후, 실행하여 run 의 결과를 확인합니다.\nmlflow service를 포트포워딩해서 MLflow ui에 접속합니다.\nkubectl port-forward svc/mlflow-server-service -n mlflow-system 5000:5000 웹 브라우저를 열어 localhost:5000으로 접속하면, 다음과 같이 run이 생성된 것을 확인할 수 있습니다.\nrun 을 클릭해서 확인하면 학습한 모델 파일이 있는 것을 확인할 수 있습니다.\n"}),e.add({id:40,href:"/docs/kubeflow/how-to-debug/",title:"13. Component - Debugging",description:"Debugging Pipeline # 이번 페이지에서는 Kubeflow 컴포넌트를 디버깅하는 방법에 대해서 알아봅니다.\nFailed Component # 이번 페이지에서는 Component - MLFlow 에서 이용한 파이프라인을 조금 수정해서 사용합니다.\n우선 컴포넌트가 실패하도록 파이프라인을 변경하도록 하겠습니다.\nfrom functools import partial import kfp from kfp.components import InputPath, OutputPath, create_component_from_func from kfp.dsl import pipeline @partial( create_component_from_func, packages_to_install=[\u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;], ) def load_iris_data( data_path: OutputPath(\u0026#34;csv\u0026#34;), target_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.",content:"Debugging Pipeline # 이번 페이지에서는 Kubeflow 컴포넌트를 디버깅하는 방법에 대해서 알아봅니다.\nFailed Component # 이번 페이지에서는 Component - MLFlow 에서 이용한 파이프라인을 조금 수정해서 사용합니다.\n우선 컴포넌트가 실패하도록 파이프라인을 변경하도록 하겠습니다.\nfrom functools import partial import kfp from kfp.components import InputPath, OutputPath, create_component_from_func from kfp.dsl import pipeline @partial( create_component_from_func, packages_to_install=[\u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;], ) def load_iris_data( data_path: OutputPath(\u0026#34;csv\u0026#34;), target_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.DataFrame(iris[\u0026#34;data\u0026#34;], columns=iris[\u0026#34;feature_names\u0026#34;]) target = pd.DataFrame(iris[\u0026#34;target\u0026#34;], columns=[\u0026#34;target\u0026#34;]) data[\u0026#34;sepal length (cm)\u0026#34;] = None data.to_csv(data_path, index=False) target.to_csv(target_path, index=False) @partial( create_component_from_func, packages_to_install=[\u0026#34;pandas\u0026#34;], ) def drop_na_from_csv( data_path: InputPath(\u0026#34;csv\u0026#34;), output_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd data = pd.read_csv(data_path) data = data.dropna() data.to_csv(output_path, index=False) @partial( create_component_from_func, packages_to_install=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;, \u0026#34;mlflow\u0026#34;], ) def train_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), input_example_path: OutputPath(\u0026#34;dill\u0026#34;), signature_path: OutputPath(\u0026#34;dill\u0026#34;), conda_env_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC from mlflow.models.signature import infer_signature from mlflow.utils.environment import _mlflow_conda_env train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) input_example = train_data.sample(1) with open(input_example_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(input_example, file_writer) signature = infer_signature(train_data, clf.predict(train_data)) with open(signature_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(signature, file_writer) conda_env = _mlflow_conda_env( additional_pip_deps=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;] ) with open(conda_env_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(conda_env, file_writer) @pipeline(name=\u0026#34;debugging_pipeline\u0026#34;) def debugging_pipeline(kernel: str): iris_data = load_iris_data() drop_data = drop_na_from_csv(data=iris_data.outputs[\u0026#34;data\u0026#34;]) model = train_from_csv( train_data=drop_data.outputs[\u0026#34;output\u0026#34;], train_target=iris_data.outputs[\u0026#34;target\u0026#34;], kernel=kernel, ) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(debugging_pipeline, \u0026#34;debugging_pipeline.yaml\u0026#34;) 수정한 점은 다음과 같습니다.\n 데이터를 불러오는 load_iris_data 컴포넌트에서 sepal length (cm) 피처에 None 값을 주입 drop_na_from_csv 컴포넌트에서 drop_na() 함수를 이용해 na 값이 포함된 row를 제거  이제 파이프라인을 업로드하고 실행해 보겠습니다.\n실행 후 Run을 눌러서 확인해보면 Train from csv 컴포넌트에서 실패했다고 나옵니다.\n실패한 컴포넌트를 클릭하고 로그를 확인해서 실패한 이유를 확인해 보겠습니다.\n로그를 확인하면 데이터의 개수가 0이여서 실행되지 않았다고 나옵니다.\n분명 정상적으로 데이터를 전달했는데 왜 데이터의 개수가 0개일까요?\n이제 입력받은 데이터에 어떤 문제가 있었는지 확인해 보겠습니다.\n우선 컴포넌트를 클릭하고 Input/Ouput 탭에서 입력값으로 들어간 데이터들을 다운로드 받습니다.\n다운로드는 빨간색 네모로 표시된 곳의 링크를 클릭하면 됩니다.\n두 개의 파일을 같은 경로에 다운로드합니다.\n그리고 해당 경로로 이동해서 파일을 확인합니다.\nls 다음과 같이 두 개의 파일이 있습니다.\ndrop-na-from-csv-output.tgz load-iris-data-target.tgz 압축을 풀어보겠습니다.\ntar -xzvf load-iris-data-target.tgz ; mv data target.csv tar -xzvf drop-na-from-csv-output.tgz ; mv data data.csv 그리고 이를 주피터 노트북을 이용해 컴포넌트 코드를 실행합니다.\n디버깅을 해본 결과 dropna 할 때 column을 기준으로 drop을 해야 하는데 row를 기준으로 drop을 해서 데이터가 모두 사라졌습니다. 이제 문제의 원인을 알아냈으니 column을 기준으로 drop이 되게 컴포넌트를 수정합니다.\n@partial( create_component_from_func, packages_to_install=[\u0026#34;pandas\u0026#34;], ) def drop_na_from_csv( data_path: InputPath(\u0026#34;csv\u0026#34;), output_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd data = pd.read_csv(data_path) data = data.dropna(axis=\u0026#34;columns\u0026#34;) data.to_csv(output_path, index=False) 수정 후 파이프라인을 다시 업로드하고 실행하면 다음과 같이 정상적으로 수행하는 것을 확인할 수 있습니다.\n"}),e.add({id:41,href:"/docs/api-deployment/",title:"API Deployment",description:"API deployment with seldon-core",content:""}),e.add({id:42,href:"/docs/api-deployment/what-is-api-deployment/",title:"1. What is API Deployment?",description:"API Deployment란? # 머신러닝 모델을 학습한 뒤에는 어떻게 사용해야 할까요?\n머신러닝을 학습할 때는 더 높은 성능의 모델이 나오기를 기대하지만, 학습된 모델을 사용하여 추론을 할 때는 빠르고 쉽게 추론 결과를 받아보고 싶을 것입니다.\n모델의 추론 결과를 확인하고자 할 때 주피터 노트북이나 파이썬 스크립트를 통해 학습된 모델을 로드한 뒤 추론할 수 있습니다.\n그렇지만 이런 방법은 모델이 클수록 모델을 불러오는 데 많은 시간을 소요하게 되어서 비효율적입니다. 또한 이렇게 이용하면 많은 사람이 모델을 이용할 수 없고 학습된 모델이 있는 환경에서밖에 사용할 수 없습니다.",content:"API Deployment란? # 머신러닝 모델을 학습한 뒤에는 어떻게 사용해야 할까요?\n머신러닝을 학습할 때는 더 높은 성능의 모델이 나오기를 기대하지만, 학습된 모델을 사용하여 추론을 할 때는 빠르고 쉽게 추론 결과를 받아보고 싶을 것입니다.\n모델의 추론 결과를 확인하고자 할 때 주피터 노트북이나 파이썬 스크립트를 통해 학습된 모델을 로드한 뒤 추론할 수 있습니다.\n그렇지만 이런 방법은 모델이 클수록 모델을 불러오는 데 많은 시간을 소요하게 되어서 비효율적입니다. 또한 이렇게 이용하면 많은 사람이 모델을 이용할 수 없고 학습된 모델이 있는 환경에서밖에 사용할 수 없습니다.\n그래서 실제 서비스에서 머신러닝이 사용될 때는 API를 이용해서 학습된 모델을 사용합니다. 모델은 API 서버가 구동되는 환경에서 한 번만 로드가 되며, DNS를 활용하여 외부에서도 쉽게 추론 결과를 받을 수 있고 다른 서비스와 연동할 수 있습니다.\n하지만 모델을 API로 만드는 작업에는 생각보다 많은 부수적인 작업이 필요합니다.\n그래서 API로 만드는 작업을 더 쉽게 하기 위해서 Tensorflow와 같은 머신러닝 프레임워크 진영에서는 추론 엔진(Inference engine)을 개발하였습니다.\n추론 엔진들을 이용하면 해당 머신러닝 프레임워크로 개발되고 학습된 모델을 불러와 추론이 가능한 API(REST 또는 gRPC)를 생성합니다.\n이러한 추론 엔진을 활용하여 구축한 API 서버로 추론하고자 하는 데이터를 담아 요청을 보내면, 추론 엔진이 추론 결과를 응답에 담아 전송하는 것입니다.\n대표적으로 다음과 같은 오픈소스 추론 엔진들이 개발되었습니다.\n Tensorflow : Tensorflow Serving PyTorch : Torchserve Onnx : Onnx Runtime  오프소스에서 공식적으로 지원하지는 않지만, 많이 쓰이는 sklearn, xgboost 프레임워크를 위한 추론 엔진도 개발되어 있습니다.\n이처럼 모델의 추론 결과를 API의 형태로 받아볼 수 있도록 배포하는 것을 API Deployment라고 합니다.\nServing Framework # 위에서 다양한 추론 엔진들이 개발되었다는 사실을 소개해 드렸습니다. 쿠버네티스 환경에서 이러한 추론 엔진들을 사용하여 API Deployment를 한다면 어떤 작업이 필요할까요? 추론 엔진을 배포하기 위한 Deployment, 추론 요청을 보낼 Endpoint를 생성하기 위한 Service, 외부에서의 추론 요청을 추론 엔진으로 보내기 위한 Ingress 등 많은 쿠버네티스 리소스를 배포해 주어야 합니다. 이것 이외에도, 많은 추론 요청이 들어왔을 경우의 스케일 아웃(scale-out), 추론 엔진 상태에 대한 모니터링, 개선된 모델이 나왔을 경우 버전 업데이트 등 추론 엔진을 운영할 때의 요구사항은 한두 가지가 아닙니다.\n이러한 많은 요구사항을 처리하기 위해 추론 엔진들을 쿠버네티스 환경 위에서 한 번 더 추상화한 Serving Framework들이 개발되었습니다.\n개발된 Serving Framework들은 다음과 같은 오픈소스들이 있습니다.\n Seldon Core Kserve BentoML  모두의 MLOps에서는 Seldon Core를 사용하여 API Deployment를 하는 과정을 다루어 보도록 하겠습니다.\n"}),e.add({id:43,href:"/docs/api-deployment/seldon-iris/",title:"2. Deploy SeldonDeployment",description:"SeldonDeployment를 통해 배포하기 # 이번에는 학습된 모델이 있을 때 SeldonDeployment를 통해 API Deployment를 해보겠습니다. SeldonDeployment는 쿠버네티스(Kubernetes)에 모델을 REST/gRPC 서버의 형태로 배포하기 위해 정의된 CRD(CustomResourceDefinition)입니다.\n1. Prerequisites # SeldonDeployment 관련된 실습은 seldon-deploy라는 새로운 네임스페이스(namespace)에서 진행하도록 하겠습니다. 네임스페이스를 생성한 뒤, seldon-deploy를 현재 네임스페이스로 설정합니다.\nkubectl create namespace seldon-deploy kubectl config set-context --current --namespace=seldon-deploy 2. 스펙 정의 # SeldonDeployment를 배포하기 위한 yaml 파일을 생성합니다. 이번 페이지에서는 공개된 iris model을 사용하도록 하겠습니다. 이 iris model은 sklearn 프레임워크를 통해 학습되었기 때문에 SKLEARN_SERVER를 사용합니다.",content:"SeldonDeployment를 통해 배포하기 # 이번에는 학습된 모델이 있을 때 SeldonDeployment를 통해 API Deployment를 해보겠습니다. SeldonDeployment는 쿠버네티스(Kubernetes)에 모델을 REST/gRPC 서버의 형태로 배포하기 위해 정의된 CRD(CustomResourceDefinition)입니다.\n1. Prerequisites # SeldonDeployment 관련된 실습은 seldon-deploy라는 새로운 네임스페이스(namespace)에서 진행하도록 하겠습니다. 네임스페이스를 생성한 뒤, seldon-deploy를 현재 네임스페이스로 설정합니다.\nkubectl create namespace seldon-deploy kubectl config set-context --current --namespace=seldon-deploy 2. 스펙 정의 # SeldonDeployment를 배포하기 위한 yaml 파일을 생성합니다. 이번 페이지에서는 공개된 iris model을 사용하도록 하겠습니다. 이 iris model은 sklearn 프레임워크를 통해 학습되었기 때문에 SKLEARN_SERVER를 사용합니다.\ncat \u0026lt;\u0026lt;EOF \u0026gt; iris-sdep.yaml apiVersion: machinelearning.seldon.io/v1alpha2 kind: SeldonDeployment metadata: name: sklearn namespace: seldon-deploy spec: name: iris predictors: - graph: children: [] implementation: SKLEARN_SERVER modelUri: gs://seldon-models/v1.12.0-dev/sklearn/iris name: classifier name: default replicas: 1 EOF yaml 파일을 배포합니다.\nkubectl apply -f iris-sdep.yaml 다음 명령어를 통해 정상적으로 배포가 되었는지 확인합니다.\nkubectl get pods --selector seldon-app=sklearn-default -n seldon-deploy 모두 Running 이 되면 다음과 비슷한 결과가 출력됩니다.\nNAME READY STATUS RESTARTS AGE sklearn-default-0-classifier-5fdfd7bb77-ls9tr 2/2 Running 0 5m Ingress URL # 이제 배포된 모델에 추론 요청(predict request)를 보내서 추론 결괏값을 받아옵니다. 배포된 API는 다음과 같은 규칙으로 생성됩니다. http://{NODE_IP}:{NODE_PORT}/seldon/{namespace}/{seldon-deployment-name}/api/v1.0/{method-name}/\nNODE_IP / NODE_PORT # Seldon Core 설치 시, Ambassador를 Ingress Controller로 설정하였으므로, SeldonDeployment로 생성된 API 서버는 모두 Ambassador의 Ingress gateway를 통해 요청할 수 있습니다.\n따라서 우선 Ambassador Ingress Gateway의 url을 환경 변수로 설정합니다.\nexport NODE_IP=$(kubectl get nodes -o jsonpath=\u0026#39;{ $.items[*].status.addresses[?(@.type==\u0026#34;InternalIP\u0026#34;)].address }\u0026#39;) export NODE_PORT=$(kubectl get service ambassador -n seldon-system -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34;) 설정된 url을 확인합니다.\necho \u0026#34;NODE_IP\u0026#34;=$NODE_IP echo \u0026#34;NODE_PORT\u0026#34;=$NODE_PORT 다음과 비슷하게 출력되어야 하며, 클라우드 등을 통해 설정할 경우, internal ip 주소가 설정되는 것을 확인할 수 있습니다.\nNODE_IP=192.168.0.19 NODE_PORT=30486 namespace / seldon-deployment-name # SeldonDeployment가 배포된 namespace와 seldon-deployment-name를 의미합니다. 이는 스펙을 정의할 때 metadata에 정의된 값을 사용합니다.\nmetadata: name: sklearn namespace: seldon-deploy 위의 예시에서는 namespace는 seldon-deploy, seldon-deployment-name은 sklearn 입니다.\nmethod-name # SeldonDeployment에서 주로 사용하는 method-name은 두 가지가 있습니다.\n doc predictions  각각의 method의 자세한 사용 방법은 아래에서 설명합니다.\nUse Swagger # 우선 doc method를 사용하는 방법입니다. doc method를 이용하면 seldon에서 생성한 swagger에 접속할 수 있습니다.\n1. Swagger 접속 # 위에서 설명한 ingress url 규칙에 따라 아래 주소를 통해 swagger에 접근할 수 있습니다.\nhttp://192.168.0.19:30486/seldon/seldon-deploy/sklearn/api/v1.0/doc/\n 2. Swagger Predictions 메뉴 선택 # UI에서 /seldon/seldon-deploy/sklearn/api/v1.0/predictions 메뉴를 선택합니다.\n 3. Try it out 선택 #  4. Request body에 data 입력 #  다음 데이터를 입력합니다.\n{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;:[[1.0, 2.0, 5.0, 6.0]] } } 5. 추론 결과 확인 # Execute 버튼을 눌러서 추론 결과를 확인할 수 있습니다.\n 정상적으로 수행되면 다음과 같은 추론 결과를 얻습니다.\n{ \u0026#34;data\u0026#34;: { \u0026#34;names\u0026#34;: [ \u0026#34;t:0\u0026#34;, \u0026#34;t:1\u0026#34;, \u0026#34;t:2\u0026#34; ], \u0026#34;ndarray\u0026#34;: [ [ 9.912315378486697e-7, 0.0007015931307746079, 0.9992974156376876 ] ] }, \u0026#34;meta\u0026#34;: { \u0026#34;requestPath\u0026#34;: { \u0026#34;classifier\u0026#34;: \u0026#34;seldonio/sklearnserver:1.11.2\u0026#34; } } } Using CLI # 또한, curl과 같은 http client CLI 도구를 활용해서도 API 요청을 수행할 수 있습니다.\n예를 들어, 다음과 같이 /predictions를 요청하면\ncurl -X POST http://$NODE_IP:$NODE_PORT/seldon/seldon-deploy/sklearn/api/v1.0/predictions \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;: [[1,2,3,4]] } }\u0026#39; 아래와 같은 응답이 정상적으로 출력되는 것을 확인할 수 있습니다.\n{\u0026#34;data\u0026#34;:{\u0026#34;names\u0026#34;:[\u0026#34;t:0\u0026#34;,\u0026#34;t:1\u0026#34;,\u0026#34;t:2\u0026#34;],\u0026#34;ndarray\u0026#34;:[[0.0006985194531162835,0.00366803903943666,0.995633441507447]]},\u0026#34;meta\u0026#34;:{\u0026#34;requestPath\u0026#34;:{\u0026#34;classifier\u0026#34;:\u0026#34;seldonio/sklearnserver:1.11.2\u0026#34;}}} "}),e.add({id:44,href:"/docs/api-deployment/seldon-pg/",title:"3. Seldon Monitoring",description:"Prometheus \u0026 Grafana 확인하기",content:"Grafana \u0026amp; Prometheus # 이제, 지난 페이지에서 생성했던 SeldonDeployment 로 API Request 를 반복적으로 수행해보고, 대시보드에 변화가 일어나는지 확인해봅니다.\n대시보드 # 앞서 생성한 대시보드를 포트 포워딩합니다.\nkubectl port-forward svc/seldon-core-analytics-grafana -n seldon-system 8090:80 API 요청 # 앞서 생성한 Seldon Deployment에 요청을 반복해서 보냅니다.\ncurl -X POST http://$NODE_IP:$NODE_PORT/seldon/seldon-deploy/sklearn/api/v1.0/predictions \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;: [[1,2,3,4]] } }\u0026#39; 그리고 그라파나 대시보드를 확인하면 다음과 같이 Global Request Rate 이 0 ops 에서 순간적으로 상승하는 것을 확인할 수 있습니다.\n이렇게 프로메테우스와 그라파나가 정상적으로 설치된 것을 확인할 수 있습니다.\n"}),e.add({id:45,href:"/docs/api-deployment/seldon-fields/",title:"4. Seldon Fields",description:"How Seldon Core works? # Seldon Core가 API 서버를 생성하는 과정을 요약하면 다음과 같습니다.\n initContainer는 모델 저장소에서 필요한 모델을 다운로드 받습니다. 다운로드받은 모델을 container로 전달합니다. container는 전달받은 모델을 감싼 API 서버를 실행합니다. 생성된 API 서버 주소로 API를 요청하여 모델의 추론 값을 받을 수 있습니다.  SeldonDeployment Spec # Seldon Core를 사용할 때, 주로 사용하게 되는 커스텀 리소스인 SeldonDeployment를 정의하는 yaml 파일은 다음과 같습니다.\napiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: seldon-example namespace: kubeflow-user-example-com spec: name: model predictors: - name: model componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} initContainers: - name: model-initializer image: gcr.",content:"How Seldon Core works? # Seldon Core가 API 서버를 생성하는 과정을 요약하면 다음과 같습니다.\n initContainer는 모델 저장소에서 필요한 모델을 다운로드 받습니다. 다운로드받은 모델을 container로 전달합니다. container는 전달받은 모델을 감싼 API 서버를 실행합니다. 생성된 API 서버 주소로 API를 요청하여 모델의 추론 값을 받을 수 있습니다.  SeldonDeployment Spec # Seldon Core를 사용할 때, 주로 사용하게 되는 커스텀 리소스인 SeldonDeployment를 정의하는 yaml 파일은 다음과 같습니다.\napiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: seldon-example namespace: kubeflow-user-example-com spec: name: model predictors: - name: model componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} initContainers: - name: model-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 args: - \u0026#34;gs://seldon-models/v1.12.0-dev/sklearn/iris\u0026#34; - \u0026#34;/mnt/models\u0026#34; volumeMounts: - mountPath: /mnt/models name: model-provision-location containers: - name: model image: seldonio/sklearnserver:1.8.0-dev volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true securityContext: privileged: true runAsUser: 0 runAsGroup: 0 graph: name: model type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; children: [] SeldonDeployment spe 중 name 과 predictors 필드는 required 필드입니다.\nname은 쿠버네티스 상에서 pod의 구분을 위한 이름으로 크게 영향을 미치지 않습니다.\npredictors는 한 개로 구성된 array로 name, componentSpecs 와 graph 가 정의되어야 합니다.\n여기서도 name은 pod의 구분을 위한 이름으로 크게 영향을 미치지 않습니다.\n이제 componentSpecs 와 graph에서 정의해야 할 필드들에 대해서 알아보겠습니다.\ncomponentSpecs # componentSpecs 는 하나로 구성된 array로 spec 키값이 정의되어야 합니다.\nspec 에는 volumes, initContainers, containers 의 필드가 정의되어야 합니다.\nvolumes # volumes: - name: model-provision-location emptyDir: {} volumes은 initContainer에서 다운로드받는 모델을 저장하기 위한 공간을 의미합니다.\narray로 입력을 받으며 array의 구성 요소는 name과 emptyDir 입니다.\n이 값들은 모델을 다운로드받고 옮길 때 한번 사용되므로 크게 수정하지 않아도 됩니다.\ninitContainer # - name: model-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 args: - \u0026#34;gs://seldon-models/v1.12.0-dev/sklearn/iris\u0026#34; - \u0026#34;/mnt/models\u0026#34; volumeMounts: - mountPath: /mnt/models name: model-provision-location initContainer는 API에서 사용할 모델을 다운로드받는 역할을 합니다.\n그래서 사용되는 필드들은 모델 저장소(Model Registry)로부터 데이터를 다운로드받을 때 필요한 정보들을 정해줍니다.\ninitContainer의 값은 n개의 array로 구성되어 있으며 사용하는 모델마다 각각 지정해주어야 합니다.\nname # name은 쿠버네티스 상의 pod의 이름입니다.\n디버깅을 위해 {model_name}-initializer 로 사용하길 권장합니다.\nimage # image 는 모델을 다운로드 받기 위해 사용할 이미지 이름입니다.\nseldon core에서 권장하는 이미지는 크게 두 가지입니다.\n gcr.io/kfserving/storage-initializer:v0.4.0 seldonio/rclone-storage-initializer:1.13.0-dev  각각의 자세한 내용은 다음을 참고 바랍니다.\n kfserving rlone  모두의 MLOps 에서는 kfserving을 사용합니다.\nargs # args: - \u0026#34;gs://seldon-models/v1.12.0-dev/sklearn/iris\u0026#34; - \u0026#34;/mnt/models\u0026#34; gcr.io/kfserving/storage-initializer:v0.4.0 도커 이미지가 실행(run)될 때 입력받는 argument를 입력합니다.\narray로 구성되며 첫 번째 array의 값은 다운로드받을 모델의 주소를 적습니다.\n두 번째 array의 값은 다운로드받은 모델을 저장할 주소를 적습니다. (seldon core에서는 주로 /mnt/models에 저장합니다.)\nvolumeMounts # volumeMounts: - mountPath: /mnt/models name: model-provision-location volumneMounts는 volumes에서 설명한 것과 같이 /mnt/models를 쿠버네티스 상에서 공유할 수 있도록 볼륨을 붙여주는 필드입니다.\n자세한 내용은 쿠버네티스 Volume을 참조 바랍니다.\ncontainer # containers: - name: model image: seldonio/sklearnserver:1.8.0-dev volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true securityContext: privileged: true runAsUser: 0 runAsGroup: 0 container는 실제로 모델이 API 형식으로 실행될 때의 설정을 정의하는 필드입니다.\nname # name은 쿠버네티스 상의 pod의 이름입니다. 사용하는 모델의 이름을 적습니다.\nimage # image 는 모델을 API로 만드는 데 사용할 이미지입니다.\n이미지에는 모델이 로드될 때 필요한 패키지들이 모두 설치되어 있어야 합니다.\nSeldon Core에서 지원하는 공식 이미지는 다음과 같습니다.\n seldonio/sklearnserver seldonio/mlflowserver seldonio/xgboostserver seldonio/tfserving  volumeMounts # volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true initContainer에서 다운로드받은 데이터가 있는 경로를 알려주는 필드입니다.\n이때 모델이 수정되는 것을 방지하기 위해 readOnly: true도 같이 주겠습니다.\nsecurityContext # securityContext: privileged: true runAsUser: 0 runAsGroup: 0 필요한 패키지를 설치할 때 pod이 권한이 없어서 패키지 설치를 수행하지 못할 수 있습니다.\n이를 위해서 root 권한을 부여합니다. (다만 이 작업은 실제 서빙 시 보안 문제가 생길 수 있습니다.)\ngraph # graph: name: model type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; children: [] 모델이 동작하는 순서를 정의한 필드입니다.\nname # 모델 그래프의 이름입니다. container에서 정의된 이름을 사용합니다.\ntype # type은 크게 4가지가 있습니다.\n TRANSFROMER MODEL OUTPUT_TRANSFORMER ROUTER  각 type에 대한 자세한 설명은 Seldon Core Complex Graphs Metadata Example을 참조 바랍니다.\nparameters # class init 에서 사용되는 값들입니다.\nsklearnserver에서 필요한 값은 다음 파일에서 확인할 수 있습니다.\nclass SKLearnServer(SeldonComponent): def __init__(self, model_uri: str = None, method: str = \u0026#34;predict_proba\u0026#34;): 코드를 보면 model_uri와 method를 정의할 수 있습니다.\nchildren # 순서도를 작성할 때 사용됩니다. 자세한 내용은 다음 페이지에서 설명합니다.\n"}),e.add({id:46,href:"/docs/api-deployment/seldon-mlflow/",title:"5. Model from MLflow",description:"Model from MLflow # 이번 페이지에서는 MLflow Component에서 저장된 모델을 이용해 API를 생성하는 방법에 대해서 알아보겠습니다.\nSecret # initContainer가 minio에 접근해서 모델을 다운로드받으려면 credentials가 필요합니다. minio에 접근하기 위한 credentials는 다음과 같습니다.\napiVersion: v1 type: Opaque kind: Secret metadata: name: seldon-init-container-secret namespace: kubeflow-user-example-com data: AWS_ACCESS_KEY_ID: bWluaW8K= AWS_SECRET_ACCESS_KEY: bWluaW8xMjM= AWS_ENDPOINT_URL: aHR0cDovL21pbmlvLm1ha2luYXJvY2tzLmFp USE_SSL: ZmFsc2U= AWS_ACCESS_KEY_ID 의 입력값은 minio입니다. 다만 secret의 입력값은 인코딩된 값이여야 되기 때문에 실제로 입력되는 값은 다음을 수행후 나오는 값이어야 합니다.",content:"Model from MLflow # 이번 페이지에서는 MLflow Component에서 저장된 모델을 이용해 API를 생성하는 방법에 대해서 알아보겠습니다.\nSecret # initContainer가 minio에 접근해서 모델을 다운로드받으려면 credentials가 필요합니다. minio에 접근하기 위한 credentials는 다음과 같습니다.\napiVersion: v1 type: Opaque kind: Secret metadata: name: seldon-init-container-secret namespace: kubeflow-user-example-com data: AWS_ACCESS_KEY_ID: bWluaW8K= AWS_SECRET_ACCESS_KEY: bWluaW8xMjM= AWS_ENDPOINT_URL: aHR0cDovL21pbmlvLm1ha2luYXJvY2tzLmFp USE_SSL: ZmFsc2U= AWS_ACCESS_KEY_ID 의 입력값은 minio입니다. 다만 secret의 입력값은 인코딩된 값이여야 되기 때문에 실제로 입력되는 값은 다음을 수행후 나오는 값이어야 합니다.\ndata에 입력되어야 하는 값들은 다음과 같습니다.\n AWS_ACCESS_KEY_ID: minio AWS_SECRET_ACCESS_KEY: minio123 AWS_ENDPOINT_URL: http://minio-service.kubeflow.svc:9000 USE_SSL: false  인코딩은 다음 명령어를 통해서 할 수 있습니다.\necho -n minio | base64 그러면 다음과 같은 값이 출력됩니다.\nbWluaW8= 인코딩을 전체 값에 대해서 진행하면 다음과 같이 됩니다.\n AWS_ACCESS_KEY_ID: bWluaW8= AWS_SECRET_ACCESS_KEY: bWluaW8xMjM= AWS_ENDPOINT_URL: aHR0cDovL21pbmlvLXNlcnZpY2Uua3ViZWZsb3cuc3ZjOjkwMDA= USE_SSL: ZmFsc2U=  다음 명령어를 통해 secret을 생성할 수 있는 yaml파일을 생성합니다.\ncat \u0026lt;\u0026lt;EOF \u0026gt; seldon-init-container-secret.yaml apiVersion: v1 kind: Secret metadata: name: seldon-init-container-secret namespace: kubeflow-user-example-com type: Opaque data: AWS_ACCESS_KEY_ID: bWluaW8= AWS_SECRET_ACCESS_KEY: bWluaW8xMjM= AWS_ENDPOINT_URL: aHR0cDovL21pbmlvLXNlcnZpY2Uua3ViZWZsb3cuc3ZjOjkwMDA= USE_SSL: ZmFsc2U= EOF 다음 명령어를 통해 secret을 생성합니다.\nkubectl apply -f seldon-init-container-secret.yaml 정상적으로 수행되면 다음과 같이 출력됩니다.\nsecret/seldon-init-container-secret created Seldon Core yaml # 이제 Seldon Core를 생성하는 yaml파일을 작성합니다.\napiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: seldon-example namespace: kubeflow-user-example-com spec: name: model predictors: - name: model componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} initContainers: - name: model-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 args: - \u0026#34;s3://mlflow/mlflow/artifacts/0/74ba8e33994144f599e50b3be176cdb0/artifacts/svc\u0026#34; - \u0026#34;/mnt/models\u0026#34; volumeMounts: - mountPath: /mnt/models name: model-provision-location envFrom: - secretRef: name: seldon-init-container-secret containers: - name: model image: ghcr.io/mlops-for-all/mlflowserver volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true securityContext: privileged: true runAsUser: 0 runAsGroup: 0 graph: name: model type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; children: [] 이 전에 작성한 Seldon Fields와 달라진 점은 크게 두 부분입니다. initContainer에 envFrom 필드가 추가되었으며 args의 주소가 s3://mlflow/mlflow/artifacts/0/74ba8e33994144f599e50b3be176cdb0/artifacts/svc 로 바뀌었습니다.\nargs # 앞서 args의 첫번째 array는 우리가 다운로드받을 모델의 경로라고 했습니다.\n그럼 mlflow에 저장된 모델의 경로는 어떻게 알 수 있을까요?\n다시 mlflow에 들어가서 run을 클릭하고 모델을 누르면 다음과 같이 확인할 수 있습니다.\n이렇게 확인된 경로를 입력하면 됩니다.\nenvFrom # minio에 접근해서 모델을 다운로드 받는 데 필요한 환경변수를 입력해주는 과정입니다. 앞서 만든 seldon-init-container-secret를 이용합니다.\nAPI 생성 # 우선 위에서 정의한 스펙을 yaml 파일로 생성하겠습니다.\napiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: seldon-example namespace: kubeflow-user-example-com spec: name: model predictors: - name: model componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} initContainers: - name: model-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 args: - \u0026#34;s3://mlflow/mlflow/artifacts/0/74ba8e33994144f599e50b3be176cdb0/artifacts/svc\u0026#34; - \u0026#34;/mnt/models\u0026#34; volumeMounts: - mountPath: /mnt/models name: model-provision-location envFrom: - secretRef: name: seldon-init-container-secret containers: - name: model image: ghcr.io/mlops-for-all/mlflowserver volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true securityContext: privileged: true runAsUser: 0 runAsGroup: 0 graph: name: model type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; - name: xtype type: STRING value: \u0026#34;dataframe\u0026#34; children: [] EOF seldon pod을 생성합니다.\nkubectl apply -f seldon-mlflow.yaml 정상적으로 수행되면 다음과 같이 출력됩니다.\nseldondeployment.machinelearning.seldon.io/seldon-example created 이제 pod이 정상적으로 뜰 때까지 기다립니다.\nkubectl get po -n kubeflow-user-example-com | grep seldon 다음과 비슷하게 출력되면 정상적으로 API를 생성했습니다.\nseldon-example-model-0-model-5c949bd894-c5f28 3/3 Running 0 69s CLI를 이용해 생성된 API에는 다음 request를 통해 실행을 확인할 수 있습니다.\ncurl -X POST http://$NODE_IP:$NODE_PORT/seldon/seldon-deploy/sklearn/api/v1.0/predictions \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;: [ [ 143.0, 0.0, 30.0, 30.0 ] ], \u0026#34;names\u0026#34;: [ \u0026#34;sepal length (cm)\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;, \u0026#34;petal length (cm)\u0026#34;, \u0026#34;petal width (cm)\u0026#34; ] } }\u0026#39; 정상적으로 실행될 경우 다음과 같은 결과를 받을 수 있습니다.\n{\u0026#34;data\u0026#34;:{\u0026#34;names\u0026#34;:[],\u0026#34;ndarray\u0026#34;:[\u0026#34;Virginica\u0026#34;]},\u0026#34;meta\u0026#34;:{\u0026#34;requestPath\u0026#34;:{\u0026#34;model\u0026#34;:\u0026#34;ghcr.io/mlops-for-all/mlflowserver:e141f57\u0026#34;}}} "}),e.add({id:47,href:"/docs/api-deployment/seldon-children/",title:"6. Multi Models",description:"Multi Models # 앞서 설명했던 방법들은 모두 단일 모델을 대상으로 했습니다.\n이번 페이지에서는 여러 개의 모델을 연결하는 방법에 대해서 알아봅니다.\nPipeline # 우선 모델을 2개를 생성하는 파이프라인을 작성하겠습니다.\n모델은 앞서 사용한 SVC 모델에 StandardScaler를 추가하고 저장하도록 하겠습니다.\nfrom functools import partial import kfp from kfp.components import InputPath, OutputPath, create_component_from_func @partial( create_component_from_func, packages_to_install=[\u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;], ) def load_iris_data( data_path: OutputPath(\u0026#34;csv\u0026#34;), target_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.",content:"Multi Models # 앞서 설명했던 방법들은 모두 단일 모델을 대상으로 했습니다.\n이번 페이지에서는 여러 개의 모델을 연결하는 방법에 대해서 알아봅니다.\nPipeline # 우선 모델을 2개를 생성하는 파이프라인을 작성하겠습니다.\n모델은 앞서 사용한 SVC 모델에 StandardScaler를 추가하고 저장하도록 하겠습니다.\nfrom functools import partial import kfp from kfp.components import InputPath, OutputPath, create_component_from_func @partial( create_component_from_func, packages_to_install=[\u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;], ) def load_iris_data( data_path: OutputPath(\u0026#34;csv\u0026#34;), target_path: OutputPath(\u0026#34;csv\u0026#34;), ): import pandas as pd from sklearn.datasets import load_iris iris = load_iris() data = pd.DataFrame(iris[\u0026#34;data\u0026#34;], columns=iris[\u0026#34;feature_names\u0026#34;]) target = pd.DataFrame(iris[\u0026#34;target\u0026#34;], columns=[\u0026#34;target\u0026#34;]) data.to_csv(data_path, index=False) target.to_csv(target_path, index=False) @partial( create_component_from_func, packages_to_install=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;, \u0026#34;mlflow\u0026#34;], ) def train_scaler_from_csv( data_path: InputPath(\u0026#34;csv\u0026#34;), scaled_data_path: OutputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), input_example_path: OutputPath(\u0026#34;dill\u0026#34;), signature_path: OutputPath(\u0026#34;dill\u0026#34;), conda_env_path: OutputPath(\u0026#34;dill\u0026#34;), ): import dill import pandas as pd from sklearn.preprocessing import StandardScaler from mlflow.models.signature import infer_signature from mlflow.utils.environment import _mlflow_conda_env data = pd.read_csv(data_path) scaler = StandardScaler() scaled_data = scaler.fit_transform(data) scaled_data = pd.DataFrame(scaled_data, columns=data.columns, index=data.index) scaled_data.to_csv(scaled_data_path, index=False) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(scaler, file_writer) input_example = data.sample(1) with open(input_example_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(input_example, file_writer) signature = infer_signature(data, scaler.transform(data)) with open(signature_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(signature, file_writer) conda_env = _mlflow_conda_env( additional_pip_deps=[\u0026#34;scikit-learn\u0026#34;], install_mlflow=False ) with open(conda_env_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(conda_env, file_writer) @partial( create_component_from_func, packages_to_install=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;, \u0026#34;mlflow\u0026#34;], ) def train_svc_from_csv( train_data_path: InputPath(\u0026#34;csv\u0026#34;), train_target_path: InputPath(\u0026#34;csv\u0026#34;), model_path: OutputPath(\u0026#34;dill\u0026#34;), input_example_path: OutputPath(\u0026#34;dill\u0026#34;), signature_path: OutputPath(\u0026#34;dill\u0026#34;), conda_env_path: OutputPath(\u0026#34;dill\u0026#34;), kernel: str, ): import dill import pandas as pd from sklearn.svm import SVC from mlflow.models.signature import infer_signature from mlflow.utils.environment import _mlflow_conda_env train_data = pd.read_csv(train_data_path) train_target = pd.read_csv(train_target_path) clf = SVC(kernel=kernel) clf.fit(train_data, train_target) with open(model_path, mode=\u0026#34;wb\u0026#34;) as file_writer: dill.dump(clf, file_writer) input_example = train_data.sample(1) with open(input_example_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(input_example, file_writer) signature = infer_signature(train_data, clf.predict(train_data)) with open(signature_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(signature, file_writer) conda_env = _mlflow_conda_env( additional_pip_deps=[\u0026#34;scikit-learn\u0026#34;], install_mlflow=False ) with open(conda_env_path, \u0026#34;wb\u0026#34;) as file_writer: dill.dump(conda_env, file_writer) @partial( create_component_from_func, packages_to_install=[\u0026#34;dill\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;scikit-learn\u0026#34;, \u0026#34;mlflow\u0026#34;, \u0026#34;boto3\u0026#34;], ) def upload_sklearn_model_to_mlflow( model_name: str, model_path: InputPath(\u0026#34;dill\u0026#34;), input_example_path: InputPath(\u0026#34;dill\u0026#34;), signature_path: InputPath(\u0026#34;dill\u0026#34;), conda_env_path: InputPath(\u0026#34;dill\u0026#34;), ): import os import dill from mlflow.sklearn import save_model from mlflow.tracking.client import MlflowClient os.environ[\u0026#34;MLFLOW_S3_ENDPOINT_URL\u0026#34;] = \u0026#34;http://minio-service.kubeflow.svc:9000\u0026#34; os.environ[\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;] = \u0026#34;minio\u0026#34; os.environ[\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;] = \u0026#34;minio123\u0026#34; client = MlflowClient(\u0026#34;http://mlflow-server-service.mlflow-system.svc:5000\u0026#34;) with open(model_path, mode=\u0026#34;rb\u0026#34;) as file_reader: clf = dill.load(file_reader) with open(input_example_path, \u0026#34;rb\u0026#34;) as file_reader: input_example = dill.load(file_reader) with open(signature_path, \u0026#34;rb\u0026#34;) as file_reader: signature = dill.load(file_reader) with open(conda_env_path, \u0026#34;rb\u0026#34;) as file_reader: conda_env = dill.load(file_reader) save_model( sk_model=clf, path=model_name, serialization_format=\u0026#34;cloudpickle\u0026#34;, conda_env=conda_env, signature=signature, input_example=input_example, ) run = client.create_run(experiment_id=\u0026#34;0\u0026#34;) client.log_artifact(run.info.run_id, model_name) from kfp.dsl import pipeline @pipeline(name=\u0026#34;multi_model_pipeline\u0026#34;) def multi_model_pipeline(kernel: str = \u0026#34;rbf\u0026#34;): iris_data = load_iris_data() scaled_data = train_scaler_from_csv(data=iris_data.outputs[\u0026#34;data\u0026#34;]) _ = upload_sklearn_model_to_mlflow( model_name=\u0026#34;scaler\u0026#34;, model=scaled_data.outputs[\u0026#34;model\u0026#34;], input_example=scaled_data.outputs[\u0026#34;input_example\u0026#34;], signature=scaled_data.outputs[\u0026#34;signature\u0026#34;], conda_env=scaled_data.outputs[\u0026#34;conda_env\u0026#34;], ) model = train_svc_from_csv( train_data=scaled_data.outputs[\u0026#34;scaled_data\u0026#34;], train_target=iris_data.outputs[\u0026#34;target\u0026#34;], kernel=kernel, ) _ = upload_sklearn_model_to_mlflow( model_name=\u0026#34;svc\u0026#34;, model=model.outputs[\u0026#34;model\u0026#34;], input_example=model.outputs[\u0026#34;input_example\u0026#34;], signature=model.outputs[\u0026#34;signature\u0026#34;], conda_env=model.outputs[\u0026#34;conda_env\u0026#34;], ) if __name__ == \u0026#34;__main__\u0026#34;: kfp.compiler.Compiler().compile(multi_model_pipeline, \u0026#34;multi_model_pipeline.yaml\u0026#34;) 파이프라인을 업로드하면 다음과 같이 나옵니다.\nMLflow 대시보드를 확인하면 다음과 같이 두 개의 모델이 생성됩니다.\n각각의 run_id를 확인 후 다음과 같이 SeldonDeployment 스펙을 정의합니다.\napiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: multi-model-example namespace: kubeflow-user-example-com spec: name: model predictors: - name: model componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} initContainers: - name: scaler-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 args: - \u0026#34;s3://mlflow/mlflow/artifacts/0/7f445015a0e94519b003d316478766ef/artifacts/scaler\u0026#34; - \u0026#34;/mnt/models\u0026#34; volumeMounts: - mountPath: /mnt/models name: model-provision-location envFrom: - secretRef: name: seldon-init-container-secret - name: svc-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 args: - \u0026#34;s3://mlflow/mlflow/artifacts/0/87eb168e76264b39a24b0e5ca0fe922b/artifacts/svc\u0026#34; - \u0026#34;/mnt/models\u0026#34; volumeMounts: - mountPath: /mnt/models name: model-provision-location envFrom: - secretRef: name: seldon-init-container-secret containers: - name: scaler image: seldonio/mlflowserver:1.8.0-dev volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true securityContext: privileged: true runAsUser: 0 runAsGroup: 0 - name: svc image: seldonio/mlflowserver:1.8.0-dev volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true securityContext: privileged: true runAsUser: 0 runAsGroup: 0 graph: name: scaler type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; - name: predict_method type: STRING value: \u0026#34;transform\u0026#34; children: - name: svc type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; 모델이 두 개가 되었으므로 각 모델의 initContainer와 container를 정의해주어야 합니다. 이 필드는 입력값을 array로 받으며 순서는 관계없습니다.\n모델이 실행하는 순서는 graph에서 정의됩니다.\ngraph: name: scaler type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; - name: predict_method type: STRING value: \u0026#34;transform\u0026#34; children: - name: svc type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; graph의 동작 방식은 처음 받은 값을 정해진 predict_method로 변환한 뒤 children으로 정의된 모델에 전달하는 방식입니다. 이 경우 scaler -\u0026gt; svc 로 데이터가 전달됩니다.\n이제 위의 스펙을 yaml파일로 생성해 보겠습니다.\ncat \u0026lt;\u0026lt;EOF \u0026gt; multi-model.yaml apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: multi-model-example namespace: kubeflow-user-example-com spec: name: model predictors: - name: model componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} initContainers: - name: scaler-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 args: - \u0026#34;s3://mlflow/mlflow/artifacts/0/7f445015a0e94519b003d316478766ef/artifacts/scaler\u0026#34; - \u0026#34;/mnt/models\u0026#34; volumeMounts: - mountPath: /mnt/models name: model-provision-location envFrom: - secretRef: name: seldon-init-container-secret - name: svc-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 args: - \u0026#34;s3://mlflow/mlflow/artifacts/0/87eb168e76264b39a24b0e5ca0fe922b/artifacts/svc\u0026#34; - \u0026#34;/mnt/models\u0026#34; volumeMounts: - mountPath: /mnt/models name: model-provision-location envFrom: - secretRef: name: seldon-init-container-secret containers: - name: scaler image: ghcr.io/mlops-for-all/mlflowserver volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true securityContext: privileged: true runAsUser: 0 runAsGroup: 0 - name: svc image: ghcr.io/mlops-for-all/mlflowserver volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true securityContext: privileged: true runAsUser: 0 runAsGroup: 0 graph: name: scaler type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; - name: predict_method type: STRING value: \u0026#34;transform\u0026#34; children: - name: svc type: MODEL parameters: - name: model_uri type: STRING value: \u0026#34;/mnt/models\u0026#34; EOF 다음 명령어를 통해 API를 생성합니다.\nkubectl apply -f multi-model.yaml 정상적으로 수행되면 다음과 같이 출력됩니다.\nseldondeployment.machinelearning.seldon.io/multi-model-example created 정상적으로 생성됐는지 확인합니다.\nkubectl get po -n kubeflow-user-example-com | grep multi-model-example 정상적으로 생성되면 다음과 비슷한 pod이 생성됩니다.\nmulti-model-example-model-0-scaler-svc-9955fb795-n9ffw 4/4 Running 0 2m30s "}),e.add({id:48,href:"/docs/further-readings/",title:"Further Readings",description:"To learn more about MLOps",content:""}),e.add({id:49,href:"/docs/further-readings/info/",title:"다루지 못한 것들",description:"MLOps Component # MLOps Concepts에서 다루었던 컴포넌트를 도식화하면 다음과 같습니다.\n 이 중 모두의 MLOps 에서 다룬 기술 스택들은 다음과 같습니다.\n 보시는 것처럼 아직 우리가 다루지 못한 많은 MLOps 컴포넌트들이 있습니다.\n시간 관계상 이번에 모두 다루지는 못했지만, 만약 필요하다면 다음과 같은 오픈소스들을 먼저 참고해보면 좋을 것 같습니다.\n 세부 내용은 다음과 같습니다.\n   Mgmt. Component Open Soruce     Data Mgmt. Collection Kafka    Validation Beam    Feature Store Flink   ML Model Dev.",content:"MLOps Component # MLOps Concepts에서 다루었던 컴포넌트를 도식화하면 다음과 같습니다.\n 이 중 모두의 MLOps 에서 다룬 기술 스택들은 다음과 같습니다.\n 보시는 것처럼 아직 우리가 다루지 못한 많은 MLOps 컴포넌트들이 있습니다.\n시간 관계상 이번에 모두 다루지는 못했지만, 만약 필요하다면 다음과 같은 오픈소스들을 먼저 참고해보면 좋을 것 같습니다.\n 세부 내용은 다음과 같습니다.\n   Mgmt. Component Open Soruce     Data Mgmt. Collection Kafka    Validation Beam    Feature Store Flink   ML Model Dev. \u0026amp; Experiment Modeling Jupyter    Analysis \u0026amp; Experiment Mgmt. MLflow    HPO Tuning \u0026amp; AutoML Katib   Deploy Mgmt. Serving Framework Seldon Core    A/B Test Iter8    Monitoring Grafana, Prometheus   Process Mgmt. pipeline Kubeflow    CI/CD Github Action    Continuous Training Argo Events   Platform Mgmt. Configuration Mgmt. Consul    Code Version Mgmt. Github, Minio    Logging (EFK) Elastic Search, Fluentd, Kibana    Resource Mgmt. Kubernetes    "}),e.add({id:50,href:"/docs/appendix/",title:"Appendix",description:"Appendix",content:""}),e.add({id:51,href:"/docs/appendix/pyenv/",title:"1. Python 가상환경 설치",description:"파이썬 가상환경 # Python 환경을 사용하다 보면 여러 버전의 Python 환경을 사용하고 싶은 경우나, 여러 프로젝트별 패키지 버전을 따로 관리하고 싶은 경우가 발생합니다.\n이처럼 Python 환경 혹은 Python Package 환경을 가상화하여 관리하는 것을 쉽게 도와주는 도구로는 pyenv, conda, virtualenv, venv 등이 존재합니다.\n이 중 모두의 MLOps에서는 pyenv와 pyenv-virtualenv를 설치하는 방법을 다룹니다.\npyenv는 Python 버전을 관리하는 것을 도와주며, pyenv-virtualenv는 pyenv의 plugin으로써 파이썬 패키지 환경을 관리하는 것을 도와줍니다.\npyenv 설치 # Prerequisites # 운영 체제별로 Prerequisites가 다릅니다.",content:"파이썬 가상환경 # Python 환경을 사용하다 보면 여러 버전의 Python 환경을 사용하고 싶은 경우나, 여러 프로젝트별 패키지 버전을 따로 관리하고 싶은 경우가 발생합니다.\n이처럼 Python 환경 혹은 Python Package 환경을 가상화하여 관리하는 것을 쉽게 도와주는 도구로는 pyenv, conda, virtualenv, venv 등이 존재합니다.\n이 중 모두의 MLOps에서는 pyenv와 pyenv-virtualenv를 설치하는 방법을 다룹니다.\npyenv는 Python 버전을 관리하는 것을 도와주며, pyenv-virtualenv는 pyenv의 plugin으로써 파이썬 패키지 환경을 관리하는 것을 도와줍니다.\npyenv 설치 # Prerequisites # 운영 체제별로 Prerequisites가 다릅니다. 다음 페이지를 참고하여 필수 패키지들을 설치해주시기 바랍니다.\n설치 - macOS #  pyenv, pyenv-virtualenv 설치  brew update brew install pyenv brew install pyenv-virtualenv pyenv 설정  macOS의 경우 카탈리나 버전 이후 기본 shell이 zsh로 변경되었기 때문에 zsh을 사용하는 경우를 가정하였습니다.\necho \u0026#39;eval \u0026#34;$(pyenv init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc pyenv 명령이 정상적으로 수행되는지 확인합니다.\npyenv --help $ pyenv --help Usage: pyenv \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;] Some useful pyenv commands are: --version Display the version of pyenv activate Activate virtual environment commands List all available pyenv commands deactivate Deactivate virtual environment exec Run an executable with the selected Python version global Set or show the global Python version(s) help Display help for a command hooks List hook scripts for a given pyenv command init Configure the shell environment for pyenv install Install a Python version using python-build local Set or show the local application-specific Python version(s) prefix Display prefix for a Python version rehash Rehash pyenv shims (run this after installing executables) root Display the root directory where versions and shims are kept shell Set or show the shell-specific Python version shims List existing pyenv shims uninstall Uninstall a specific Python version version Show the current Python version(s) and its origin version-file Detect the file that sets the current pyenv version version-name Show the current Python version version-origin Explain how the current Python version is set versions List all Python versions available to pyenv virtualenv Create a Python virtualenv using the pyenv-virtualenv plugin virtualenv-delete Uninstall a specific Python virtualenv virtualenv-init Configure the shell environment for pyenv-virtualenv virtualenv-prefix Display real_prefix for a Python virtualenv version virtualenvs List all Python virtualenvs found in `$PYENV_ROOT/versions/*\u0026#39;. whence List all Python versions that contain the given executable which Display the full path to an executable See `pyenv help \u0026lt;command\u0026gt;\u0026#39; for information on a specific command. For full documentation, see: https://github.com/pyenv/pyenv#readme 설치 - Ubuntu #  pyenv, pyenv-virtualenv 설치  curl https://pyenv.run | bash 다음과 같은 내용이 출력되면 정상적으로 설치된 것을 의미합니다.\n% Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- 0 0 0 0 0 0 0 0 --:--:-- --:--:-- 100 270 100 270 0 0 239 0 0:00:01 0:00:01 --:--:-- 239 Cloning into \u0026#39;/home/mlops/.pyenv\u0026#39;... r ... 중략... ... remote: Enumerating objects: 10, done. remote: Counting objects: 100% (10/10), done. remote: Compressing objects: 100% (6/6), done. remote: Total 10 (delta 1), reused 6 (delta 0), pack-reused 0 Unpacking objects: 100% (10/10), 2.92 KiB | 2.92 MiB/s, done. WARNING: seems you still have not added \u0026#39;pyenv\u0026#39; to the load path. # See the README for instructions on how to set up # your shell environment for Pyenv. # Load pyenv-virtualenv automatically by adding # the following to ~/.bashrc: eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; pyenv 설정  기본 shell로 bash shell을 사용하는 경우를 가정하였습니다. bash에서 pyenv와 pyenv-virtualenv 를 사용할 수 있도록 설정합니다.\nsudo vi ~/.bashrc 다음 문자열을 입력한 후 저장합니다.\nexport PATH=\u0026#34;$HOME/.pyenv/bin:$PATH\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; shell을 restart 합니다.\nexec $SHELL pyenv 명령이 정상적으로 수행되는지 확인합니다.\npyenv --help 다음과 같은 메시지가 출력되면 정상적으로 설정된 것을 의미합니다.\n$ pyenv pyenv 2.2.2 Usage: pyenv \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;] Some useful pyenv commands are: --version Display the version of pyenv activate Activate virtual environment commands List all available pyenv commands deactivate Deactivate virtual environment doctor Verify pyenv installation and development tools to build pythons. exec Run an executable with the selected Python version global Set or show the global Python version(s) help Display help for a command hooks List hook scripts for a given pyenv command init Configure the shell environment for pyenv install Install a Python version using python-build local Set or show the local application-specific Python version(s) prefix Display prefix for a Python version rehash Rehash pyenv shims (run this after installing executables) root Display the root directory where versions and shims are kept shell Set or show the shell-specific Python version shims List existing pyenv shims uninstall Uninstall a specific Python version version Show the current Python version(s) and its origin version-file Detect the file that sets the current pyenv version version-name Show the current Python version version-origin Explain how the current Python version is set versions List all Python versions available to pyenv virtualenv Create a Python virtualenv using the pyenv-virtualenv plugin virtualenv-delete Uninstall a specific Python virtualenv virtualenv-init Configure the shell environment for pyenv-virtualenv virtualenv-prefix Display real_prefix for a Python virtualenv version virtualenvs List all Python virtualenvs found in `$PYENV_ROOT/versions/*\u0026#39;. whence List all Python versions that contain the given executable which Display the full path to an executable See `pyenv help \u0026lt;command\u0026gt;\u0026#39; for information on a specific command. For full documentation, see: https://github.com/pyenv/pyenv#readme pyenv 사용 # Python 버전 설치 # pyenv install \u0026lt;Python-Version\u0026gt; 명령을 통해 원하는 파이썬 버전을 설치할 수 있습니다. 이번 페이지에서는 예시로 kubeflow에서 기본으로 사용하는 파이썬 3.7.12 버전을 설치하겠습니다.\npyenv install 3.7.12 정상적으로 설치되면 다음과 같은 메시지가 출력됩니다.\n$ pyenv install 3.7.12 Downloading Python-3.7.12.tar.xz... -\u0026gt; https://www.python.org/ftp/python/3.7.12/Python-3.7.12.tar.xz Installing Python-3.7.12... patching file Doc/library/ctypes.rst patching file Lib/test/test_unicode.py patching file Modules/_ctypes/_ctypes.c patching file Modules/_ctypes/callproc.c patching file Modules/_ctypes/ctypes.h patching file setup.py patching file \u0026#39;Misc/NEWS.d/next/Core and Builtins/2020-06-30-04-44-29.bpo-41100.PJwA6F.rst\u0026#39; patching file Modules/_decimal/libmpdec/mpdecimal.h Installed Python-3.7.12 to /home/mlops/.pyenv/versions/3.7.12 Python 가상환경 생성 # pyenv virtualenv \u0026lt;Installed-Python-Version\u0026gt; \u0026lt;가상환경-이름\u0026gt; 명령을 통해 원하는 파이썬 버전의 파이썬 가상환경을 생성할 수 있습니다.\n예시로 Python 3.7.12 버전의 demo라는 이름의 Python 가상환경을 생성하겠습니다.\npyenv virtualenv 3.7.12 demo $ pyenv virtualenv 3.7.12 demo Looking in links: /tmp/tmpffqys0gv Requirement already satisfied: setuptools in /home/mlops/.pyenv/versions/3.7.12/envs/demo/lib/python3.7/site-packages (47.1.0) Requirement already satisfied: pip in /home/mlops/.pyenv/versions/3.7.12/envs/demo/lib/python3.7/site-packages (20.1.1) Python 가상환경 사용 # pyenv activate \u0026lt;가상환경 이름\u0026gt; 명령을 통해 위와 같은 방식으로 생성한 가상환경을 사용할 수 있습니다.\n예시로는 demo라는 이름의 Python 가상환경을 사용하겠습니다.\npyenv activate demo 다음과 같이 현재 가상환경의 정보가 shell의 맨 앞에 출력되는 것을 확인할 수 있습니다.\nBefore\nmlops@ubuntu:~$ pyenv activate demo After\npyenv-virtualenv: prompt changing will be removed from future release. configure `export PYENV_VIRTUALENV_DISABLE_PROMPT=1\u0026#39; to simulate the behavior. (demo) mlops@ubuntu:~$ Python 가상환경 비활성화 # source deactivate 명령을 통해 현재 사용 중인 가상환경을 비활성화할 수 있습니다.\nsource deactivate Before\n(demo) mlops@ubuntu:~$ source deactivate After\nmlops@ubuntu:~$ "}),e.add({id:52,href:"/docs/appendix/metallb/",title:"2. bare-metal 클러스터용 load balancer metallb 설치",description:"MetalLB란? # Kubernetes 사용 시 AWS, GCP, Azure 와 같은 클라우드 플랫폼에서는 자체적으로 로드 벨런서(Load Balancer)를 제공해 주지만, 온프레미스 클러스터에서는 로드 벨런싱 기능을 제공하는 모듈을 추가적으로 설치해야 합니다.\nMetalLB는 베어메탈 환경에서 사용할 수 있는 로드 벨런서를 제공하는 오픈소스 프로젝트 입니다.\n요구사항 #    요구 사항 버전 및 내용     Kubernetes 로드 벨런싱 기능이 없는 \u0026gt;= v1.13.0   호환가능한 네트워크 CNI Calico, Canal, Cilium, Flannel, Kube-ovn, Kube-router, Weave Net   IPv4 주소 MetalLB 배포에 사용   BGP 모드를 사용할 경우 BGP 기능을 지원하는 하나 이상의 라우터   노드 간 포트 TCP/UDP 7946 오픈 memberlist 요구 사항    MetalLB 설치 # Preparation # IPVS 모드에서 kube-proxy를 사용하는 경우 Kubernetes v1.",content:"MetalLB란? # Kubernetes 사용 시 AWS, GCP, Azure 와 같은 클라우드 플랫폼에서는 자체적으로 로드 벨런서(Load Balancer)를 제공해 주지만, 온프레미스 클러스터에서는 로드 벨런싱 기능을 제공하는 모듈을 추가적으로 설치해야 합니다.\nMetalLB는 베어메탈 환경에서 사용할 수 있는 로드 벨런서를 제공하는 오픈소스 프로젝트 입니다.\n요구사항 #    요구 사항 버전 및 내용     Kubernetes 로드 벨런싱 기능이 없는 \u0026gt;= v1.13.0   호환가능한 네트워크 CNI Calico, Canal, Cilium, Flannel, Kube-ovn, Kube-router, Weave Net   IPv4 주소 MetalLB 배포에 사용   BGP 모드를 사용할 경우 BGP 기능을 지원하는 하나 이상의 라우터   노드 간 포트 TCP/UDP 7946 오픈 memberlist 요구 사항    MetalLB 설치 # Preparation # IPVS 모드에서 kube-proxy를 사용하는 경우 Kubernetes v1.14.2 이후부터는 엄격한 ARP(strictARP) 모드를 사용하도록 설정해야 합니다.\nKube-router는 기본적으로 엄격한 ARP를 활성화하므로 서비스 프록시로 사용할 경우에는 이 기능이 필요하지 않습니다.\n엄격한 ARP 모드를 적용하기에 앞서, 현재 모드를 확인합니다.\n# see what changes would be made, returns nonzero returncode if different kubectl get configmap kube-proxy -n kube-system -o yaml | \\ grep strictARP strictARP: false strictARP: false 가 출력되는 경우 다음을 실행하여 strictARP: true로 변경합니다. (strictARP: true가 이미 출력된다면 다음 커맨드를 수행하지 않으셔도 됩니다.)\n# actually apply the changes, returns nonzero returncode on errors only kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl apply -f - -n kube-system 정상적으로 수행되면 다음과 같이 출력됩니다.\nWarning: resource configmaps/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. configmap/kube-proxy configured 설치 - Manifest # 1. MetalLB 를 설치합니다. # kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/metallb.yaml 2. 정상 설치 확인 # metallb-system namespace 의 2 개의 pod 이 모두 Running 이 될 때까지 기다립니다.\nkubectl get pod -n metallb-system 모두 Running 이 되면 다음과 비슷한 결과가 출력됩니다.\nNAME READY STATUS RESTARTS AGE controller-7dcc8764f4-8n92q 1/1 Running 1 1m speaker-fnf8l 1/1 Running 1 1m 매니페스트의 구성 요소는 다음과 같습니다.\n metallb-system/controller  deployment 로 배포되며, 로드 벨런싱을 수행할 external IP 주소의 할당을 처리하는 역할을 담당합니다.   metallb-system/speaker  daemonset 형태로 배포되며, 외부 트래픽과 서비스를 연결해 네트워크 통신이 가능하도록 구성하는 역할을 담당합니다.    서비스에는 컨트롤러 및 스피커와 구성 요소가 작동하는 데 필요한 RBAC 사용 권한이 포함됩니다.\nConfiguration # MetalLB 의 로드 벨런싱 정책 설정은 관련 설정 정보를 담은 configmap 을 배포하여 설정할 수 있습니다.\nMetalLB 에서 구성할 수 있는 모드로는 다음과 같이 2가지가 있습니다.\n Layer 2 모드 BGP 모드  여기에서는 Layer 2 모드로 진행하겠습니다.\nLayer 2 Configuration # Layer 2 모드는 간단하게 사용할 IP 주소의 대역만 설정하면 됩니다.\nLayer 2 모드를 사용할 경우 워커 노드의 네트워크 인터페이스에 IP를 바인딩 하지 않아도 되는데 로컬 네트워크의 ARP 요청에 직접 응답하여 컴퓨터의 MAC주소를 클라이언트에 제공하는 방식으로 작동하기 때문입니다.\n다음 metallb_config.yaml 파일은 MetalLB 가 192.168.35.100 ~ 192.168.35.110의 IP에 대한 제어 권한을 제공하고 Layer 2 모드를 구성하는 설정입니다.\n클러스터 노드와 클라이언트 노드가 분리된 경우, 192.168.35.100 ~ 192.168.35.110 대역이 클라이언트 노드와 클러스터 노드 모두 접근 가능한 대역이어야 합니다.\nmetallb_config.yaml # apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.35.100-192.168.35.110 # IP 대역폭 위의 설정을 적용합니다.\nkubectl apply -f metallb_config.yaml 정상적으로 배포하면 다음과 같이 출력됩니다.\nconfigmap/config created MetalLB 사용 # Kubeflow Dashboard # 먼저 kubeflow의 Dashboard 를 제공하는 istio-system 네임스페이스의 istio-ingressgateway 서비스의 타입을 LoadBalancer로 변경하여 MetalLB로부터 로드 벨런싱 기능을 제공받기 전에, 현재 상태를 확인합니다.\nkubectl get svc/istio-ingressgateway -n istio-system 해당 서비스의 타입은 ClusterIP이며, External-IP 값은 none 인 것을 확인할 수 있습니다.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway ClusterIP 10.103.72.5 \u0026lt;none\u0026gt; 15021/TCP,80/TCP,443/TCP,31400/TCP,15443/TCP 4h21m type 을 LoadBalancer 로 변경하고 원하는 IP 주소를 입력하고 싶은 경우 loadBalancerIP 항목을 추가합니다.\n추가 하지 않을 경우에는 위에서 설정한 IP 주소풀에서 순차적으로 IP 주소가 배정됩니다.\nkubectl edit svc/istio-ingressgateway -n istio-system spec: clusterIP: 10.103.72.5 clusterIPs: - 10.103.72.5 ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: status-port port: 15021 protocol: TCP targetPort: 15021 - name: http2 port: 80 protocol: TCP targetPort: 8080 - name: https port: 443 protocol: TCP targetPort: 8443 - name: tcp port: 31400 protocol: TCP targetPort: 31400 - name: tls port: 15443 protocol: TCP targetPort: 15443 selector: app: istio-ingressgateway istio: ingressgateway sessionAffinity: None type: LoadBalancer # Change ClusterIP to LoadBalancer loadBalancerIP: 192.168.35.100 # Add IP status: loadBalancer: {} 다시 확인을 해보면 External-IP 값이 192.168.35.100 인 것을 확인합니다.\nkubectl get svc/istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.103.72.5 192.168.35.100 15021:31054/TCP,80:30853/TCP,443:30443/TCP,31400:30012/TCP,15443:31650/TCP 5h1m Web Browser 를 열어 http://192.168.35.100 으로 접속하여, 다음과 같은 화면이 출력되는 것을 확인합니다.\nminio Dashboard # 먼저 minio 의 Dashboard 를 제공하는 kubeflow 네임스페이스의 minio-service 서비스의 타입을 LoadBalancer로 변경하여 MetalLB로부터 로드 벨런싱 기능을 제공받기 전에, 현재 상태를 확인합니다.\nkubectl get svc/minio-service -n kubeflow 해당 서비스의 타입은 ClusterIP이며, External-IP 값은 none 인 것을 확인할 수 있습니다.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE minio-service ClusterIP 10.109.209.87 \u0026lt;none\u0026gt; 9000/TCP 5h14m type 을 LoadBalancer 로 변경하고 원하는 IP 주소를 입력하고 싶은 경우 loadBalancerIP 항목을 추가합니다.\n추가 하지 않을 경우에는 위에서 설정한 IP 주소풀에서 순차적으로 IP 주소가 배정됩니다.\nkubectl edit svc/minio-service -n kubeflow apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;application-crd-id\u0026#34;:\u0026#34;kubeflow-pipelines\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;minio-ser\u0026gt; creationTimestamp: \u0026#34;2022-01-05T08:44:23Z\u0026#34; labels: application-crd-id: kubeflow-pipelines name: minio-service namespace: kubeflow resourceVersion: \u0026#34;21120\u0026#34; uid: 0053ee28-4f87-47bb-ad6b-7ad68aa29a48 spec: clusterIP: 10.109.209.87 clusterIPs: - 10.109.209.87 ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: http port: 9000 protocol: TCP targetPort: 9000 selector: app: minio application-crd-id: kubeflow-pipelines sessionAffinity: None type: LoadBalancer # Change ClusterIP to LoadBalancer loadBalancerIP: 192.168.35.101 # Add IP status: loadBalancer: {} 다시 확인을 해보면 External-IP 값이 192.168.35.101 인 것을 확인할 수 있습니다.\nkubectl get svc/minio-service -n kubeflow NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE minio-service LoadBalancer 10.109.209.87 192.168.35.101 9000:31371/TCP 5h21m Web Browser 를 열어 http://192.168.35.101:9000 으로 접속하여, 다음과 같은 화면이 출력되는 것을 확인합니다.\nmlflow Dashboard # 먼저 mlflow 의 Dashboard 를 제공하는 mlflow-system 네임스페이스의 mlflow-server-service 서비스의 타입을 LoadBalancer로 변경하여 MetalLB로부터 로드 벨런싱 기능을 제공받기 전에, 현재 상태를 확인합니다.\nkubectl get svc/mlflow-server-service -n mlflow-system 해당 서비스의 타입은 ClusterIP이며, External-IP 값은 none 인 것을 확인할 수 있습니다.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mlflow-server-service ClusterIP 10.111.173.209 \u0026lt;none\u0026gt; 5000/TCP 4m50s type 을 LoadBalancer 로 변경하고 원하는 IP 주소를 입력하고 싶은 경우 loadBalancerIP 항목을 추가합니다.\n추가 하지 않을 경우에는 위에서 설정한 IP 주소풀에서 순차적으로 IP 주소가 배정됩니다.\nkubectl edit svc/mlflow-server-service -n mlflow-system apiVersion: v1 kind: Service metadata: annotations: meta.helm.sh/release-name: mlflow-server meta.helm.sh/release-namespace: mlflow-system creationTimestamp: \u0026#34;2022-01-07T04:00:19Z\u0026#34; labels: app.kubernetes.io/managed-by: Helm name: mlflow-server-service namespace: mlflow-system resourceVersion: \u0026#34;276246\u0026#34; uid: e5d39fb7-ad98-47e7-b512-f9c673055356 spec: clusterIP: 10.111.173.209 clusterIPs: - 10.111.173.209 ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - port: 5000 protocol: TCP targetPort: 5000 selector: app.kubernetes.io/name: mlflow-server sessionAffinity: None type: LoadBalancer # Change ClusterIP to LoadBalancer loadBalancerIP: 192.168.35.102 # Add IP status: loadBalancer: {} 다시 확인을 해보면 External-IP 값이 192.168.35.102 인 것을 확인할 수 있습니다.\nkubectl get svc/mlflow-server-service -n mlflow-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mlflow-server-service LoadBalancer 10.111.173.209 192.168.35.102 5000:32287/TCP 6m11s Web Browser 를 열어 http://192.168.35.102:5000 으로 접속하여, 다음과 같은 화면이 출력되는 것을 확인합니다.\nGrafana Dashboard # 먼저 Grafana 의 Dashboard 를 제공하는 seldon-system 네임스페이스의 seldon-core-analytics-grafana 서비스의 타입을 LoadBalancer로 변경하여 MetalLB로부터 로드 벨런싱 기능을 제공받기 전에, 현재 상태를 확인합니다.\nkubectl get svc/seldon-core-analytics-grafana -n seldon-system 해당 서비스의 타입은 ClusterIP이며, External-IP 값은 none 인 것을 확인할 수 있습니다.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE seldon-core-analytics-grafana ClusterIP 10.109.20.161 \u0026lt;none\u0026gt; 80/TCP 94s type 을 LoadBalancer 로 변경하고 원하는 IP 주소를 입력하고 싶은 경우 loadBalancerIP 항목을 추가합니다.\n추가 하지 않을 경우에는 위에서 설정한 IP 주소풀에서 순차적으로 IP 주소가 배정됩니다.\nkubectl edit svc/seldon-core-analytics-grafana -n seldon-system apiVersion: v1 kind: Service metadata: annotations: meta.helm.sh/release-name: seldon-core-analytics meta.helm.sh/release-namespace: seldon-system creationTimestamp: \u0026#34;2022-01-07T04:16:47Z\u0026#34; labels: app.kubernetes.io/instance: seldon-core-analytics app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: grafana app.kubernetes.io/version: 7.0.3 helm.sh/chart: grafana-5.1.4 name: seldon-core-analytics-grafana namespace: seldon-system resourceVersion: \u0026#34;280605\u0026#34; uid: 75073b78-92ec-472c-b0d5-240038ea8fa5 spec: clusterIP: 10.109.20.161 clusterIPs: - 10.109.20.161 ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: service port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: seldon-core-analytics app.kubernetes.io/name: grafana sessionAffinity: None type: LoadBalancer # Change ClusterIP to LoadBalancer loadBalancerIP: 192.168.35.103 # Add IP status: loadBalancer: {} 다시 확인을 해보면 External-IP 값이 192.168.35.103 인 것을 확인할 수 있습니다.\nkubectl get svc/seldon-core-analytics-grafana -n seldon-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE seldon-core-analytics-grafana LoadBalancer 10.109.20.161 192.168.35.103 80:31191/TCP 5m14s Web Browser 를 열어 http://192.168.35.103:80 으로 접속하여, 다음과 같은 화면이 출력되는 것을 확인합니다.\n"}),e.add({id:53,href:"/docs/help/",title:"Help",description:"커뮤니티에 대한 설명과 기여하는 법에 대한 안내 페이지입니다.",content:""}),e.add({id:54,href:"/docs/help/community/",title:"Community",description:"모두의 MLOps 릴리즈 소식 # 새로운 포스트나 수정사항은 Announcements에서 확인할 수 있습니다.\nQuestion # 프로젝트 내용과 관련된 궁금점은 Q\u0026amp;A를 통해 질문할 수 있습니다.\nSuggestion # 제안점은 Ideas를 통해 제안해 주시면 됩니다.\nv2.0 Release # v2.0 릴리즈 계획 및 목차는 추후 Announcements에 공개될 예정입니다.",content:"모두의 MLOps 릴리즈 소식 # 새로운 포스트나 수정사항은 Announcements에서 확인할 수 있습니다.\nQuestion # 프로젝트 내용과 관련된 궁금점은 Q\u0026amp;A를 통해 질문할 수 있습니다.\nSuggestion # 제안점은 Ideas를 통해 제안해 주시면 됩니다.\nv2.0 Release # v2.0 릴리즈 계획 및 목차는 추후 Announcements에 공개될 예정입니다.\n"}),e.add({id:55,href:"/docs/help/how-to-contribute/",title:"How to Contribute",description:"How to Start # Git Repo 준비 #   모두의 MLOps GitHub Repository에 접속합니다.\n  여러분의 개인 Repository로 Fork합니다.\n  Forked Repository를 여러분의 작업 환경으로 git clone합니다.\n  환경 설정 #  모두의 MLOps는 Hugo 와 Node를 이용하고 있습니다.\n다음 명령어를 통해 필요한 패키지가 설치되어 있는지 확인합니다.    node \u0026amp; npm\nnpm --version   hugo\nhugo version     필요한 node module을 설치합니다.",content:"How to Start # Git Repo 준비 #   모두의 MLOps GitHub Repository에 접속합니다.\n  여러분의 개인 Repository로 Fork합니다.\n  Forked Repository를 여러분의 작업 환경으로 git clone합니다.\n  환경 설정 #  모두의 MLOps는 Hugo 와 Node를 이용하고 있습니다.\n다음 명령어를 통해 필요한 패키지가 설치되어 있는지 확인합니다.    node \u0026amp; npm\nnpm --version   hugo\nhugo version     필요한 node module을 설치합니다.\nnpm install   프로젝트에서는 각 글의 일관성을 위해서 여러 markdown lint를 적용하고 있습니다.\n다음 명령어를 실행해 test를 진행한 후 커밋합니다.내용 수정 및 추가 후 lint가 맞는지 확인합니다.\nnpm test   lint 확인 완료 후 ci 를 실행합니다.\nnpm ci   로컬에서 실행 후 수정한 글이 정상적으로 나오는지 확인합니다.\nnpm run start   How to Contribute # 1. 새로운 포스트를 작성할 때 # 새로운 포스트는 각 챕터와 포스트의 위치에 맞는 weight를 설정합니다.\n Introduction: 1xx Setup: 2xx Kubeflow: 3xx API Deployment: 4xx Help: 10xx  2. 기존의 포스트를 수정할 때 # 기존의 포스트를 수정할 때 Contributor에 본인의 이름을 입력합니다.\ncontributors: [\u0026#34;John Doe\u0026#34;, \u0026#34;Adam Smith\u0026#34;] 3. 프로젝트에 처음 기여할 때 # 만약 프로젝트에 처음 기여 할 때 content/kor/contributors에 본인의 이름으로 폴더를 생성한 후, _index.md라는 파일을 작성합니다.\n예를 들어, minsoo kim이 본인의 영어 이름이라면, 폴더명은 minsoo-kim으로 하여 해당 폴더 내부의 _index.md파일에 다음의 내용을 작성합니다. 폴더명은 하이픈(-)으로 연결한 소문자로, title은 띄어쓰기를 포함한 CamelCase로 작성합니다.\n--- title: \u0026#34;John Doe\u0026#34; draft: false --- After Pull Request # Pull Request를 생성하면 프로젝트에서는 자동으로 모두의 MLOps 운영진에게 리뷰 요청이 전해집니다. 최대 일주일 이내로 확인 후 Comment를 드릴 예정입니다.\n"}),e.add({id:56,href:"/docs/",title:"Docs",description:"Docs Doks.",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()